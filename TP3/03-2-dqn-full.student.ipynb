{"cells":[{"cell_type":"markdown","id":"e92ec4b2","metadata":{"id":"dYfGJCe52lP4"},"source":["# Outlook"]},{"cell_type":"markdown","id":"bfdd7055","metadata":{"id":"aZUSf0n_2otG"},"source":["In this notebook, using BBRL, we code a version of the DQN algorithm with a replay buffer and a target network. To understand this code, you need to know more about BBRL. You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing), then [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) and, most importantly, [details about the data collection implementation](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing)."]},{"cell_type":"markdown","id":"03418d0f","metadata":{"id":"HYW4-1lsSwwS"},"source":["The DQN algorithm is explained in [this video](https://www.youtube.com/watch?v=CXwvOMJujZk) and you can also read [the corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/dqn.pdf)."]},{"cell_type":"markdown","id":"dcf454f0","metadata":{},"source":["In this notebook, you will learn how to modify the previous notebook:\n","\n","- to use a replay buffer and an environment that resets\n","- to use a target network for $Q$\n","- to use a better estimation for the maximum (Double-DQN)"]},{"cell_type":"markdown","id":"2f384cc2","metadata":{"id":"zJZDcDafp7Uf"},"source":["## Installation and Imports"]},{"cell_type":"markdown","id":"3e3d67df","metadata":{"id":"aHO1nIdM21Lq"},"source":["### Installation"]},{"cell_type":"markdown","id":"ba9c7921","metadata":{"id":"Ymc-lbXi9vDE"},"source":["The BBRL library is [here](https://github.com/osigaud/bbrl)."]},{"cell_type":"markdown","id":"fa800634","metadata":{"id":"pDy9yuQH73tJ"},"source":["This is OmegaConf that makes it possible that by just defining the `def run_dqn(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n","\n","More precisely, the code is run by calling\n","\n","`config=OmegaConf.create(params)`\n","\n","`run_dqn(config)`\n","\n","at the very bottom of the colab, after starting tensorboard."]},{"cell_type":"code","execution_count":55,"id":"4b43fce7","metadata":{"executionInfo":{"elapsed":181164,"status":"ok","timestamp":1662009761066,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"j0MaggiOl4KU","outputId":"d550b877-7166-4d2b-d78f-18dfa9030c92"},"outputs":[],"source":["try:\n","    from easypip import easyimport\n","except:\n","    !pip install easypip\n","    from easypip import easyimport\n","\n","\n","import os\n","import functools\n","import time\n","from typing import Tuple\n","\n","OmegaConf = easyimport(\"omegaconf\").OmegaConf\n","torch = easyimport(\"torch\")\n","bbrl_gym = easyimport(\"bbrl_gym\")\n","import gym\n","import bbrl\n","import copy\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import copy\n","import numpy as np\n"]},{"cell_type":"markdown","id":"a9a2c7ca","metadata":{"id":"fE1c7ZLf60X_"},"source":["### BBRL imports"]},{"cell_type":"code","execution_count":2,"id":"ba9e815e","metadata":{"executionInfo":{"elapsed":604,"status":"ok","timestamp":1662009762891,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"RcuqoAvG3zMZ"},"outputs":[],"source":["from bbrl.agents.agent import Agent\n","from bbrl import get_arguments, get_class, instantiate_class\n","\n","# The workspace is the main class in BBRL, this is where all data is collected and stored\n","from bbrl.workspace import Workspace\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n","\n","# AutoResetGymAgent is an agent able to execute a batch of gym environments\n","# with auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n","# Not present in the A2C version...\n","from bbrl.utils.logger import TFLogger\n","from bbrl.utils.replay_buffer import ReplayBuffer\n","\n","def make_env(env_name):\n","    return gym.make(env_name)"]},{"cell_type":"code","execution_count":3,"id":"8926178b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","id":"0f0ed202","metadata":{},"source":["## The replay buffer\n","\n","Differently from the previous case, we use a replace buffer that stores the a set of transitions $(s_t, a_t, r_t, s_{t+1})$"]},{"cell_type":"code","execution_count":4,"id":"9b7813bb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Environment: observation space in R^4 and action space R^2\n"]}],"source":["# We deal with 3 running episodes at a time (random seed 2139)\n","env_agent = NoAutoResetGymAgent(make_env, {'env_name': 'CartPole-v1'}, 1, 2139)\n","obs_size, action_dim = env_agent.get_obs_and_actions_sizes()\n","print(f\"Environment: observation space in R^{obs_size} and action space R^{action_dim}\")\n","\n","class RandomAgent(Agent):\n","    def __init__(self, action_dim):\n","        super().__init__()\n","        self.action_dim = action_dim\n","\n","    def forward(self, t: int, choose_action=True, **kwargs):\n","        \"\"\"An Agent can use self.workspace\"\"\"\n","        obs = self.get((\"env/env_obs\", t))\n","        action = torch.randint(0, self.action_dim, (len(obs), ))\n","        self.set((\"action\", t), action)\n","\n","# Each agent will be run (in the order given when constructing Agents)\n","agents = Agents(env_agent, RandomAgent(action_dim))\n","t_agents = TemporalAgent(agents)"]},{"cell_type":"code","execution_count":5,"id":"e23715ce","metadata":{},"outputs":[{"data":{"text/plain":["'Observations (first 3)'"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["tensor([[ 0.0269,  0.0284,  0.0051, -0.0418],\n","        [ 0.0275,  0.2235,  0.0042, -0.3329],\n","        [ 0.0319,  0.4185, -0.0024, -0.6242]])"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'Transitions of actions (first 3)'"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'(s_0, s_1)'"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["tensor([[ 0.0269,  0.0284,  0.0051, -0.0418],\n","        [ 0.0275,  0.2235,  0.0042, -0.3329]])"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'(s_1, s_2)'"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["tensor([[ 0.0275,  0.2235,  0.0042, -0.3329],\n","        [ 0.0319,  0.4185, -0.0024, -0.6242]])"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'(s_2, s_3)'"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["tensor([[ 0.0319,  0.4185, -0.0024, -0.6242],\n","        [ 0.0403,  0.6137, -0.0149, -0.9177]])"]},"metadata":{},"output_type":"display_data"}],"source":["# Creates a new workspace\n","workspace = Workspace() \n","t_agents(workspace, stop_variable=\"env/done\")\n","\n","# We get the transitions: each tensor is transformed so\n","# that: \n","# - we have the value at time step t and t+1 (so all the tensors first dimension have a size of 2)\n","# - there is no distinction between the different environments (here, there is just one environment run in parallel to make it easy)\n","transitions = workspace.get_transitions()\n","\n","# You can see that each pair of actions in the transitions can be found in the workspace\n","display(\"Observations (first 3)\", workspace[\"env/env_obs\"][:3, 0])\n","\n","display(\"Transitions of actions (first 3)\")\n","for t in range(3):\n","    display(f'(s_{t}, s_{t+1})')\n","    display(transitions[\"env/env_obs\"][:, t])\n"]},{"cell_type":"code","execution_count":6,"id":"765a0f4c","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[ 0.0712,  0.2272, -0.0638, -0.4151],\n","         [ 0.0714, -0.5463, -0.0797,  0.6033],\n","         [-0.0342, -1.1272,  0.0289,  1.3822]],\n","\n","        [[ 0.0757,  0.0330, -0.0721, -0.1432],\n","         [ 0.0605, -0.7402, -0.0676,  0.8699],\n","         [-0.0567, -1.3227,  0.0565,  1.6837]]])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Finally, the replay buffer keeps slices [:, i, ...] of the transition workspace\n","# (here at most 100 transitions)\n","rb = ReplayBuffer(max_size=100)\n","\n","# We add the transitions to the buffer....\n","rb.put(transitions)\n","\n","# And sample from them\n","# here we get 3 tuples (s_t, s_{t+1})\n","rb.get_shuffled(3)[\"env/env_obs\"]"]},{"cell_type":"markdown","id":"33519622","metadata":{},"source":["A transition workspace is still a workspace... this is quite handy since each transition can be seen as a mini-episode of two time steps; we can use our agents on it:"]},{"cell_type":"code","execution_count":7,"id":"b6e0528e","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n","         0, 0, 0, 0, 1],\n","        [1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n","         0, 0, 0, 1, 0]])"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["tensor([[0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n","         1, 1, 0, 1, 0],\n","        [1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n","         0, 1, 0, 1, 0]])"]},"metadata":{},"output_type":"display_data"}],"source":["# Just as a reference\n","\n","display(transitions[\"action\"])\n","\n","t_random_agent = TemporalAgent(RandomAgent(action_dim))\n","t_random_agent(transitions, t=0, n_steps=2)\n","\n","# Here, the action tensor will have been overwritten by the new actions\n","display(transitions[\"action\"])"]},{"cell_type":"markdown","id":"dcc389a7","metadata":{"id":"JVvAfhKm9S8p"},"source":["## Definition of agents"]},{"cell_type":"markdown","id":"213e8b6b","metadata":{"id":"RdqzKSLKDtqz"},"source":["The [DQN](https://daiwk.github.io/assets/dqn.pdf) algorithm is a critic only algorithm. Thus we just need a Critic agent (which will also be used to output actions) and an Environment agent. We reuse the `DiscreteQAgent` class that we have already explained in the previous notebook."]},{"cell_type":"code","execution_count":8,"id":"a3abbc3b","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1662009762891,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"r_QIxxHNtBMH"},"outputs":[],"source":["def build_mlp(sizes, activation, output_activation=nn.Identity()):\n","    layers = []\n","    for j in range(len(sizes) - 1):\n","        act = activation if j < len(sizes) - 2 else output_activation\n","        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n","    return nn.Sequential(*layers)"]},{"cell_type":"code","execution_count":9,"id":"ba05e599","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1662009762891,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"ARPW1Mmo7NB-"},"outputs":[],"source":["class DiscreteQAgent(Agent):\n","    def __init__(self, state_dim, hidden_layers, action_dim, device = torch.device(\"cpu\")):\n","        super().__init__()\n","        self.model = build_mlp(\n","            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n","        )\n","        self.model = self.model.to(device)\n","\n","    def forward(self, t, choose_action=True, **kwargs):\n","        obs = self.get((\"env/env_obs\", t))\n","        q_values = self.model(obs)\n","        self.set((\"q_values\", t), q_values)\n","\n","        # Sets the action\n","        if choose_action:\n","            action = q_values.argmax(1)\n","            self.set((\"action\", t), action)"]},{"cell_type":"markdown","id":"96848ae3","metadata":{"id":"yoG1eNBguNTN"},"source":["### Creating an Exploration method"]},{"cell_type":"markdown","id":"24018e44","metadata":{"id":"qvxOy0e180_6"},"source":["As Q-learning, DQN needs some exploration to prevent too early convergence. Here we will use the simple $\\epsilon$-greedy exploration method. The method is implemented as an agent which chooses an action based on the Q-values."]},{"cell_type":"code","execution_count":29,"id":"8bdf4140","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1662009762892,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"Rk0DTVsLtz4a"},"outputs":[],"source":["class EGreedyActionSelector(Agent):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","        self.epsilon = epsilon\n","\n","    def forward(self, t, **kwargs):\n","        q_values = self.get((\"q_values\", t))\n","        nb_actions = q_values.size()[1]\n","        size = q_values.size()[0]\n","        is_random = torch.rand(size).lt(self.epsilon).float().to(device)\n","        \n","        random_action = torch.randint(low=0, high=nb_actions, size=(size,)).to(device)\n","        max_action = q_values.max(1)[1]\n","        action = is_random * random_action + (1 - is_random) * max_action\n","        action = action.long()\n","        self.set((\"action\", t), action)"]},{"cell_type":"markdown","id":"92ed9905","metadata":{"id":"W0AgHYc2ywoS"},"source":["### Training and evaluation environments"]},{"cell_type":"markdown","id":"24d6b00a","metadata":{"id":"1gfbKZMFrghw"},"source":["We build two environments: one for training and another one for evaluation."]},{"cell_type":"markdown","id":"03732345","metadata":{"id":"jDM2Z0THyrtx"},"source":["For training, it is more efficient to use an AutoResetGymAgent, as we do not want to waste time if the task is done in an environment sooner than in the others."]},{"cell_type":"markdown","id":"77c72357","metadata":{"id":"7kN-SniayxRq"},"source":["By contrast, for evaluation, we just need to perform a fixed number of episodes (for statistics), thus it is more convenient to use a NoAutoResetGymAgent with a set of environments and just run one episode in each environment. Thus we can use the `env/done` stop variable and take the average over the cumulated reward of all environments."]},{"cell_type":"markdown","id":"94874944","metadata":{"id":"7_D4Fb4Fz6M1"},"source":["\n","See [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) for explanations about agents and environment agents."]},{"cell_type":"code","execution_count":30,"id":"7c564bd9","metadata":{"executionInfo":{"elapsed":348,"status":"ok","timestamp":1662009763237,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"hT5mr2yGyeUP"},"outputs":[],"source":["def get_env_agents(cfg) -> Tuple[AutoResetGymAgent, NoAutoResetGymAgent]:\n","    # Train environment\n","    train_env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","\n","    # Evaluation environment\n","    eval_env_agent = NoAutoResetGymAgent(\n","    get_class(cfg.gym_env),\n","    get_arguments(cfg.gym_env),\n","    cfg.algorithm.nb_evals,\n","    cfg.algorithm.seed,\n","    )\n","    return train_env_agent, eval_env_agent"]},{"cell_type":"markdown","id":"b023b4f1","metadata":{"id":"EzxoIPtLVJ_i"},"source":["### Create the DQN agent"]},{"cell_type":"markdown","id":"ae5c9838","metadata":{"id":"aaNnZw3bXEYd"},"source":["Interestingly, the loop between the policy and the environment is first defined as a collection of agents, and then embedded into a single TemporalAgent."]},{"cell_type":"code","execution_count":31,"id":"f1accad3","metadata":{"executionInfo":{"elapsed":221,"status":"ok","timestamp":1662009894803,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"G8Uk_RQh8QrO"},"outputs":[],"source":["def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n","    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n","\n","    # Get the two agents (critic and target critic)\n","    critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size, device)\n","    target_critic = copy.deepcopy(critic)\n","\n","    # Builds the train agent that will produce transitions\n","    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n","    tr_agent = Agents(train_env_agent, critic, explorer)\n","    train_agent = TemporalAgent(tr_agent)\n","\n","    # Creates two temporal agents just for \"replaying\" some parts\n","    # of the transition buffer    \n","    q_agent = TemporalAgent(critic)\n","    target_q_agent = TemporalAgent(target_critic)\n","\n","\n","    # Get an agent that is executed on a complete workspace\n","    ev_agent = Agents(eval_env_agent, critic)\n","    eval_agent = TemporalAgent(ev_agent)\n","    train_agent.seed(cfg.algorithm.seed)\n","\n","    return train_agent, eval_agent, q_agent, target_q_agent"]},{"cell_type":"markdown","id":"f71b33f5","metadata":{"id":"lU3cO6znHyDc"},"source":["### The Logger class"]},{"cell_type":"markdown","id":"fe287f72","metadata":{"id":"_1gszmpwhitv"},"source":["Explanations for the logger were already given in [this notebook](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV?usp=sharing)."]},{"cell_type":"code","execution_count":32,"id":"c8b7dfce","metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1662009763240,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"aOkauz_0H2GA"},"outputs":[],"source":["class Logger():\n","\n","  def __init__(self, cfg):\n","    self.logger = instantiate_class(cfg.logger)\n","\n","  def add_log(self, log_string, loss, epoch):\n","    self.logger.add_scalar(log_string, loss.item(), epoch)\n","\n","  # Log losses\n","  def log_losses(self, cfg, epoch, critic_loss, entropy_loss, actor_loss):\n","    self.add_log(\"critic_loss\", critic_loss, epoch)\n","    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n","    self.add_log(\"actor_loss\", actor_loss, epoch)\n"]},{"cell_type":"markdown","id":"5845ee2c","metadata":{"id":"f2vq1OJHWCIE"},"source":["### Setup the optimizers"]},{"cell_type":"markdown","id":"31d7bec2","metadata":{"id":"VzmEKF4J8qjg"},"source":["We use a single optimizer to tune the parameters of the actor (in the prob_agent part) and the critic (in the critic_agent part). It would be possible to have two optimizers which would work separately on the parameters of each component agent, but it would be more complicated because updating the actor requires the gradient of the critic."]},{"cell_type":"code","execution_count":33,"id":"3e10e9ba","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1662009763241,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"YFfzXEu2WFWj"},"outputs":[],"source":["# Configure the optimizer over the q agent\n","def setup_optimizers(cfg, q_agent):\n","    optimizer_args = get_arguments(cfg.optimizer)\n","    parameters = q_agent.parameters()\n","    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n","    return optimizer"]},{"cell_type":"markdown","id":"a5cc3626","metadata":{"id":"YQNvhO_VAJbh"},"source":["### Compute critic loss"]},{"cell_type":"markdown","id":"41800ba4","metadata":{"id":"fxxobbxRaJXO"},"source":["Detailed explanations of the function to compute the critic loss when using a NoAutoResetGymAgent are given in [this notebook](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV)."]},{"cell_type":"markdown","id":"585897eb","metadata":{"id":"fXwrjbueoDw6"},"source":["The case where we use the AutoResetGymAgent is very similar, but we need to specify that we use the first part of the Q-values (`q_values[0]`) for representing $Q(s_t,a_t)$ and the second part (`q_values[1]`) for representing $Q(s_{t+1},a)$, as these values are stored into a transition model."]},{"cell_type":"code","execution_count":34,"id":"0caabac3","metadata":{"executionInfo":{"elapsed":204,"status":"ok","timestamp":1662012575928,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"2sepUK-gAM3u"},"outputs":[],"source":["def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n","    # À compléter... \n","    \"\"\"_summary_\n","\n","    Args:\n","        cfg (_type_): _description_\n","        reward (torch.Tensor): A (T x B) tensor containing the rewards\n","        must_bootstrap (torch.Tensor): a (T x B) tensor containing 0 if the episode is completed at time $t$\n","        q_values (torch.Tensor): a (T x B x A)\n","        action (torch.LongTensor): a (T) long tensor containing the chosen action\n","\n","    Returns:\n","        torch.Scalar: The DQN loss\n","    \"\"\" \n","    max_q = target_q_values.max(1)[0].detach()  # Results in a (T x B) tensor\n","\n","    # To get the max of Q(s_{t+1}, a), we take max_q[1:]\n","    # The same about must_bootstrap. \n","    target = (\n","        reward[:-1] + cfg.algorithm.discount_factor * max_q * must_bootstrap.int()\n","    )\n","\n","    # To get Q(s,a), we use torch.gather along the 3rd dimension (the action)\n","    qvals = q_values[0].gather(1, action[0].unsqueeze(-1)).squeeze(-1)\n","\n","    # Compute the temporal difference (use must_boostrap as to mask out finished episodes)\n","    td = (target - qvals) * must_bootstrap.int()\n","\n","    # Compute critic loss (no need to use must_bootstrap here since we are dealing with \"full\" transitions)\n","    td = target - qvals\n","    td_error = td**2\n","    critic_loss = td_error.mean()\n","    return critic_loss"]},{"cell_type":"markdown","id":"0e64dc25","metadata":{"id":"Jmi91gANWT4z"},"source":["## Main training loop"]},{"cell_type":"markdown","id":"a5774c02","metadata":{"id":"8ixFZeCRN6Y6"},"source":["Note that everything about the shared workspace between all the agents is completely hidden under the hood. This results in a gain of productivity, at the expense of having to dig into the BBRL code if you want to understand the details, change the multiprocessing model, etc."]},{"cell_type":"markdown","id":"0c9807e2","metadata":{"id":"I6SuPOdW_hxl"},"source":["### Agent execution"]},{"cell_type":"markdown","id":"83c79b08","metadata":{"id":"WqlH-8DaVWx2"},"source":["This is the tricky part with BBRL, the one we need to understand in detail. The difficulty lies in the copy of the last step and the way to deal with the n_steps return."]},{"cell_type":"markdown","id":"6ba5ef01","metadata":{"id":"bWAmm0pPotTC"},"source":["The call to `train_agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True)` makes the agent run a number of steps in the workspace. In practice, it calls the [`__call__(...)`](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/agent.py#L54) function which makes a forward pass of the agent network using the workspace data and updates the workspace accordingly."]},{"cell_type":"markdown","id":"b93dc236","metadata":{"id":"Rn3MlNQ3qGPr"},"source":["Now, if we start at the first epoch (`epoch=0`), we start from the first step (`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must not forget to cover the transition at the border between the previous epoch and the current epoch. To avoid this risk, we copy the information from the last time step of the previous epoch into the first time step of the next epoch. This is explained in more details in [a previous colab](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."]},{"cell_type":"markdown","id":"94d41b48","metadata":{"id":"gAnnEjF9L9gk"},"source":["The [previous notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5) explains a lot of these details. In particular, read it to understand the `execute_agents(...)` function, the `transition_workspace = train_workspace.get_transitions()` line and the computation of `must_bootstrap`."]},{"cell_type":"markdown","id":"03835f50","metadata":{"id":"OFB1XFE5YEc6"},"source":["Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` lines. `optimizer.zero_grad()` is necessary to cancel all the gradients computed at the previous iterations\n"]},{"cell_type":"code","execution_count":35,"id":"a61f9066","metadata":{"executionInfo":{"elapsed":214,"status":"ok","timestamp":1662013643487,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"sk85_sRWW-5s"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","def run_dqn(cfg, compute_critic_loss, device):\n","    # 1)  Build the  logger\n","    logger = Logger(cfg)\n","    best_reward = -10e9\n","\n","    # 2) Create the environment agent\n","    train_env_agent, eval_env_agent = get_env_agents(cfg)\n","    \n","\n","    # 3) Create the DQN-like Agent\n","    train_agent, eval_agent, q_agent, target_q_agent = create_dqn_agent(\n","        cfg, train_env_agent, eval_env_agent\n","    )\n","\n","    # 5) Configure the workspace to the right dimension\n","    # Note that no parameter is needed to create the workspace.\n","    # In the training loop, calling the agent() and critic_agent()\n","    # will take the workspace as parameter\n","    train_workspace = Workspace()  # Used for training\n","    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size, device = device)\n","\n","    # 6) Configure the optimizer over the a2c agent\n","    optimizer = setup_optimizers(cfg, q_agent)\n","    nb_steps = 0\n","    last_eval_step = 0\n","    last_critic_update_step = 0\n","\n","    # send to device:\n","    train_agent = train_agent.to(device)\n","    eval_agent = eval_agent.to(device)\n","    q_agent = q_agent.to(device)\n","    target_q_agent = target_q_agent.to(device)\n","    train_workspace = train_workspace.to(device)\n","\n","    # 7) Training loop\n","    for epoch in range(cfg.algorithm.max_epochs):\n","        # Execute the agent in the workspace\n","        if epoch > 0:\n","            train_workspace.zero_grad()\n","            train_workspace.copy_n_last_steps(1)\n","            train_agent(\n","                train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1, stochastic=True\n","            )\n","        else:\n","            train_agent(\n","                train_workspace, t=0, n_steps=cfg.algorithm.n_steps, stochastic=True\n","            )\n","\n","        # Get the transitions\n","        transition_workspace = train_workspace.get_transitions()\n","\n","        action = transition_workspace[\"action\"]\n","        nb_steps += action[0].shape[0]\n","        \n","        # Adds the transitions to the workspace\n","        rb.put(transition_workspace)\n","        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n","\n","        # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace)\n","        q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n","        q_values, done, truncated, reward, action = rb_workspace[\n","            \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n","        ]\n","\n","        with torch.no_grad():\n","            target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n","        target_q_values = rb_workspace[\"q_values\"]\n","\n","        # Determines whether values of the critic should be propagated\n","        # True if the episode reached a time limit or if the task was not done\n","        # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj\n","        must_bootstrap = torch.logical_or(~done[1], truncated[1])\n","\n","        if rb.size() > cfg.algorithm.learning_starts:\n","          # Compute critic loss\n","          critic_loss = compute_critic_loss(\n","              cfg, reward, must_bootstrap, q_values, target_q_values[1], action\n","          )\n","          # Store the loss for tensorboard display\n","          logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n","\n","          optimizer.zero_grad()\n","          critic_loss.backward()\n","          torch.nn.utils.clip_grad_norm_(q_agent.parameters(), cfg.algorithm.max_grad_norm)\n","          optimizer.step()\n","          if nb_steps - last_critic_update_step > cfg.algorithm.target_critic_update:\n","              last_critic_update_step = nb_steps\n","              target_q_agent.agent = copy.deepcopy(q_agent.agent)\n","\n","        if nb_steps - last_eval_step > cfg.algorithm.eval_interval:\n","            last_eval_step = nb_steps\n","            eval_workspace = Workspace()  # Used for evaluation\n","            eval_agent(\n","                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n","            )\n","            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n","            mean = rewards.mean()\n","            logger.add_log(\"reward\", mean, nb_steps)\n","            print(f\"epoch: {epoch}, reward: {mean}\")\n","            if cfg.save_best and mean > best_reward:\n","                best_reward = mean\n","                directory = \"./dqn_critic/\"\n","                if not os.path.exists(directory):\n","                    os.makedirs(directory)\n","                filename = directory + \"dqn0_\" + str(mean.item()) + \".agt\"\n","                eval_agent.save_model(filename)\n"]},{"cell_type":"markdown","id":"935b3ee6","metadata":{"id":"uo6bc3zzKua_"},"source":["## Definition of the parameters"]},{"cell_type":"markdown","id":"8ea08843","metadata":{"id":"36r4PAfvKx-f"},"source":["The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation."]},{"cell_type":"markdown","id":"0b7edc14","metadata":{"id":"jp7jDeGkaoM1"},"source":["### Launching tensorboard to visualize the results"]},{"cell_type":"code","execution_count":36,"id":"f0631b89","metadata":{"executionInfo":{"elapsed":4755,"status":"ok","timestamp":1662012838285,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"-BlLQlwDn5Vh","outputId":"944c72b1-04f5-491e-b059-208525f54d75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Launch tensorboard from the shell:\n","/home/manuel/deepdac/bin/tensorboard --logdir=/home/manuel/RLD/TP3/tmp\n"]}],"source":["# For Colab - otherwise, it is easier and better to launch tensorboard from\n","# the terminal\n","if get_ipython().__class__.__module__ == \"google.colab._shell\":\n","    %load_ext tensorboard\n","    %tensorboard --logdir ./tmp\n","else:\n","    import sys\n","    import os\n","    import os.path as osp\n","    print(f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={os.getcwd()}/tmp\")"]},{"cell_type":"code","execution_count":37,"id":"016e1799","metadata":{"executionInfo":{"elapsed":368972,"status":"ok","timestamp":1662014835613,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"l42OUoGROlSt","outputId":"690161cc-28b2-4ce1-dcff-ed9b41d33dcc"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 8, reward: 9.300000190734863\n","epoch: 17, reward: 9.40000057220459\n","epoch: 26, reward: 8.90000057220459\n","epoch: 35, reward: 9.40000057220459\n","epoch: 44, reward: 9.300000190734863\n","epoch: 53, reward: 9.100000381469727\n","epoch: 62, reward: 9.300000190734863\n","epoch: 71, reward: 8.90000057220459\n","epoch: 80, reward: 8.90000057220459\n","epoch: 89, reward: 9.0\n","epoch: 98, reward: 9.199999809265137\n","epoch: 107, reward: 9.90000057220459\n","epoch: 116, reward: 10.40000057220459\n","epoch: 125, reward: 9.800000190734863\n","epoch: 134, reward: 9.40000057220459\n","epoch: 143, reward: 15.40000057220459\n","epoch: 152, reward: 9.5\n","epoch: 161, reward: 9.600000381469727\n","epoch: 170, reward: 14.300000190734863\n","epoch: 179, reward: 8.90000057220459\n","epoch: 188, reward: 8.90000057220459\n","epoch: 197, reward: 10.5\n","epoch: 206, reward: 9.40000057220459\n","epoch: 215, reward: 11.600000381469727\n","epoch: 224, reward: 12.5\n","epoch: 233, reward: 27.80000114440918\n","epoch: 242, reward: 21.80000114440918\n","epoch: 251, reward: 45.70000076293945\n","epoch: 260, reward: 14.0\n","epoch: 269, reward: 16.600000381469727\n","epoch: 278, reward: 62.10000228881836\n","epoch: 287, reward: 45.5\n","epoch: 296, reward: 30.899999618530273\n","epoch: 305, reward: 68.9000015258789\n","epoch: 314, reward: 59.5\n","epoch: 323, reward: 21.899999618530273\n","epoch: 332, reward: 31.5\n","epoch: 341, reward: 42.900001525878906\n","epoch: 350, reward: 35.400001525878906\n","epoch: 359, reward: 26.30000114440918\n","epoch: 368, reward: 65.20000457763672\n","epoch: 377, reward: 151.3000030517578\n","epoch: 386, reward: 29.100000381469727\n","epoch: 395, reward: 32.60000228881836\n","epoch: 404, reward: 51.70000076293945\n","epoch: 413, reward: 43.10000228881836\n","epoch: 422, reward: 62.10000228881836\n","epoch: 431, reward: 40.400001525878906\n","epoch: 440, reward: 46.79999923706055\n","epoch: 449, reward: 81.30000305175781\n","epoch: 458, reward: 96.0\n","epoch: 467, reward: 70.0999984741211\n","epoch: 476, reward: 45.900001525878906\n","epoch: 485, reward: 48.10000228881836\n","epoch: 494, reward: 195.6999969482422\n","epoch: 503, reward: 74.4000015258789\n","epoch: 512, reward: 115.5999984741211\n","epoch: 521, reward: 85.70000457763672\n","epoch: 530, reward: 97.4000015258789\n","epoch: 539, reward: 97.5\n","epoch: 548, reward: 88.20000457763672\n","epoch: 557, reward: 148.5\n","epoch: 566, reward: 103.5999984741211\n","epoch: 575, reward: 122.0\n","epoch: 584, reward: 134.5\n","epoch: 593, reward: 104.80000305175781\n","epoch: 602, reward: 145.60000610351562\n","epoch: 611, reward: 119.5999984741211\n","epoch: 620, reward: 127.0999984741211\n","epoch: 629, reward: 145.6999969482422\n","epoch: 638, reward: 206.60000610351562\n","epoch: 647, reward: 127.20000457763672\n","epoch: 656, reward: 138.6999969482422\n","epoch: 665, reward: 250.1999969482422\n","epoch: 674, reward: 132.10000610351562\n","epoch: 683, reward: 302.6000061035156\n","epoch: 692, reward: 232.1999969482422\n","epoch: 701, reward: 128.8000030517578\n","epoch: 710, reward: 185.60000610351562\n","epoch: 719, reward: 156.8000030517578\n","epoch: 728, reward: 210.8000030517578\n","epoch: 737, reward: 157.10000610351562\n","epoch: 746, reward: 276.8999938964844\n","epoch: 755, reward: 177.8000030517578\n","epoch: 764, reward: 189.1999969482422\n","epoch: 773, reward: 241.10000610351562\n","epoch: 782, reward: 152.8000030517578\n","epoch: 791, reward: 173.0\n","epoch: 800, reward: 146.90000915527344\n","epoch: 809, reward: 182.60000610351562\n","epoch: 818, reward: 302.3999938964844\n","epoch: 827, reward: 135.90000915527344\n","epoch: 836, reward: 162.10000610351562\n","epoch: 845, reward: 195.0\n","epoch: 854, reward: 262.6000061035156\n","epoch: 863, reward: 156.6999969482422\n","epoch: 872, reward: 222.6999969482422\n","epoch: 881, reward: 197.3000030517578\n","epoch: 890, reward: 211.3000030517578\n","epoch: 899, reward: 302.8000183105469\n","epoch: 908, reward: 203.3000030517578\n","epoch: 917, reward: 201.40000915527344\n","epoch: 926, reward: 208.60000610351562\n","epoch: 935, reward: 157.6999969482422\n","epoch: 944, reward: 177.6999969482422\n","epoch: 953, reward: 208.60000610351562\n","epoch: 962, reward: 267.3000183105469\n","epoch: 971, reward: 166.40000915527344\n","epoch: 980, reward: 184.1999969482422\n","epoch: 989, reward: 198.5\n","epoch: 998, reward: 238.10000610351562\n","epoch: 1007, reward: 229.10000610351562\n","epoch: 1016, reward: 206.6999969482422\n","epoch: 1025, reward: 168.6999969482422\n","epoch: 1034, reward: 171.60000610351562\n","epoch: 1043, reward: 177.10000610351562\n","epoch: 1052, reward: 194.0\n","epoch: 1061, reward: 169.60000610351562\n","epoch: 1070, reward: 174.90000915527344\n","epoch: 1079, reward: 158.40000915527344\n","epoch: 1088, reward: 166.60000610351562\n","epoch: 1097, reward: 183.10000610351562\n","epoch: 1106, reward: 189.0\n","epoch: 1115, reward: 156.1999969482422\n","epoch: 1124, reward: 170.5\n","epoch: 1133, reward: 188.40000915527344\n","epoch: 1142, reward: 202.6999969482422\n","epoch: 1151, reward: 153.0\n","epoch: 1160, reward: 167.3000030517578\n","epoch: 1169, reward: 171.40000915527344\n","epoch: 1178, reward: 169.8000030517578\n","epoch: 1187, reward: 168.40000915527344\n","epoch: 1196, reward: 173.6999969482422\n","epoch: 1205, reward: 171.10000610351562\n","epoch: 1214, reward: 177.6999969482422\n","epoch: 1223, reward: 183.40000915527344\n","epoch: 1232, reward: 162.60000610351562\n","epoch: 1241, reward: 163.1999969482422\n","epoch: 1250, reward: 155.90000915527344\n","epoch: 1259, reward: 181.40000915527344\n","epoch: 1268, reward: 155.40000915527344\n","epoch: 1277, reward: 170.3000030517578\n","epoch: 1286, reward: 158.1999969482422\n","epoch: 1295, reward: 154.40000915527344\n","epoch: 1304, reward: 193.40000915527344\n","epoch: 1313, reward: 149.40000915527344\n","epoch: 1322, reward: 158.1999969482422\n","epoch: 1331, reward: 186.5\n","epoch: 1340, reward: 169.40000915527344\n","epoch: 1349, reward: 172.6999969482422\n","epoch: 1358, reward: 173.5\n","epoch: 1367, reward: 179.90000915527344\n","epoch: 1376, reward: 155.5\n","epoch: 1385, reward: 170.90000915527344\n","epoch: 1394, reward: 172.3000030517578\n","epoch: 1403, reward: 185.3000030517578\n","epoch: 1412, reward: 168.10000610351562\n","epoch: 1421, reward: 182.3000030517578\n","epoch: 1430, reward: 160.90000915527344\n","epoch: 1439, reward: 149.8000030517578\n","epoch: 1448, reward: 172.3000030517578\n","epoch: 1457, reward: 155.8000030517578\n","epoch: 1466, reward: 170.1999969482422\n","epoch: 1475, reward: 149.60000610351562\n","epoch: 1484, reward: 161.60000610351562\n","epoch: 1493, reward: 148.90000915527344\n","epoch: 1502, reward: 167.90000915527344\n","epoch: 1511, reward: 158.3000030517578\n","epoch: 1520, reward: 154.3000030517578\n","epoch: 1529, reward: 158.5\n","epoch: 1538, reward: 145.8000030517578\n","epoch: 1547, reward: 162.6999969482422\n","epoch: 1556, reward: 150.3000030517578\n","epoch: 1565, reward: 155.6999969482422\n","epoch: 1574, reward: 172.6999969482422\n","epoch: 1583, reward: 176.60000610351562\n","epoch: 1592, reward: 157.60000610351562\n","epoch: 1601, reward: 166.90000915527344\n","epoch: 1610, reward: 159.3000030517578\n","epoch: 1619, reward: 175.8000030517578\n","epoch: 1628, reward: 152.8000030517578\n","epoch: 1637, reward: 182.10000610351562\n","epoch: 1646, reward: 209.3000030517578\n","epoch: 1655, reward: 171.40000915527344\n","epoch: 1664, reward: 168.60000610351562\n","epoch: 1673, reward: 172.40000915527344\n","epoch: 1682, reward: 161.5\n","epoch: 1691, reward: 162.3000030517578\n","epoch: 1700, reward: 151.0\n","epoch: 1709, reward: 158.3000030517578\n","epoch: 1718, reward: 154.90000915527344\n","epoch: 1727, reward: 166.6999969482422\n","epoch: 1736, reward: 153.60000610351562\n","epoch: 1745, reward: 168.40000915527344\n","epoch: 1754, reward: 141.0\n","epoch: 1763, reward: 149.5\n","epoch: 1772, reward: 153.0\n","epoch: 1781, reward: 162.40000915527344\n","epoch: 1790, reward: 141.5\n","epoch: 1799, reward: 179.6999969482422\n","epoch: 1808, reward: 163.8000030517578\n","epoch: 1817, reward: 171.6999969482422\n","epoch: 1826, reward: 152.3000030517578\n","epoch: 1835, reward: 168.3000030517578\n","epoch: 1844, reward: 166.0\n","epoch: 1853, reward: 144.0\n","epoch: 1862, reward: 145.10000610351562\n","epoch: 1871, reward: 153.8000030517578\n","epoch: 1880, reward: 160.3000030517578\n","epoch: 1889, reward: 223.10000610351562\n","epoch: 1898, reward: 166.3000030517578\n","epoch: 1907, reward: 163.0\n","epoch: 1916, reward: 138.40000915527344\n","epoch: 1925, reward: 152.1999969482422\n","epoch: 1934, reward: 161.60000610351562\n","epoch: 1943, reward: 161.6999969482422\n","epoch: 1952, reward: 148.6999969482422\n","epoch: 1961, reward: 194.3000030517578\n","epoch: 1970, reward: 176.1999969482422\n","epoch: 1979, reward: 169.1999969482422\n","epoch: 1988, reward: 166.6999969482422\n","epoch: 1997, reward: 184.8000030517578\n","epoch: 2006, reward: 192.10000610351562\n","epoch: 2015, reward: 183.60000610351562\n","epoch: 2024, reward: 183.60000610351562\n","epoch: 2033, reward: 169.1999969482422\n","epoch: 2042, reward: 171.8000030517578\n","epoch: 2051, reward: 208.0\n","epoch: 2060, reward: 195.10000610351562\n","epoch: 2069, reward: 188.40000915527344\n","epoch: 2078, reward: 167.10000610351562\n","epoch: 2087, reward: 175.6999969482422\n","epoch: 2096, reward: 192.60000610351562\n","epoch: 2105, reward: 175.6999969482422\n","epoch: 2114, reward: 165.6999969482422\n","epoch: 2123, reward: 225.1999969482422\n","epoch: 2132, reward: 202.6999969482422\n","epoch: 2141, reward: 227.5\n","epoch: 2150, reward: 168.3000030517578\n","epoch: 2159, reward: 203.40000915527344\n","epoch: 2168, reward: 166.1999969482422\n","epoch: 2177, reward: 236.3000030517578\n","epoch: 2186, reward: 225.40000915527344\n","epoch: 2195, reward: 210.5\n","epoch: 2204, reward: 156.1999969482422\n","epoch: 2213, reward: 188.10000610351562\n","epoch: 2222, reward: 208.8000030517578\n","epoch: 2231, reward: 181.10000610351562\n","epoch: 2240, reward: 187.60000610351562\n","epoch: 2249, reward: 160.40000915527344\n","epoch: 2258, reward: 200.5\n","epoch: 2267, reward: 207.8000030517578\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [37], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m config\u001b[38;5;241m=\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mcreate(params)\n\u001b[1;32m     55\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(config\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mrun_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_critic_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [35], line 43\u001b[0m, in \u001b[0;36mrun_dqn\u001b[0;34m(cfg, compute_critic_loss, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mcopy_n_last_steps(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     train_agent(\n\u001b[1;32m     48\u001b[0m         train_workspace, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_steps\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mn_steps, stochastic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     )\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m _t \u001b[39m=\u001b[39m t\n\u001b[1;32m     78\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent(workspace, t\u001b[39m=\u001b[39;49m_t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m stop_variable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         s \u001b[39m=\u001b[39m workspace\u001b[39m.\u001b[39mget(stop_variable, _t)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, workspace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents:\n\u001b[0;32m---> 31\u001b[0m         a(workspace, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/agent.py:64\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39massert\u001b[39;00m workspace \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39m[Agent.__call__] workspace must not be None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkspace \u001b[39m=\u001b[39m workspace\n\u001b[0;32m---> 64\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkspace \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/gymb.py:368\u001b[0m, in \u001b[0;36mAutoResetGymAgent.forward\u001b[0;34m(self, t, save_render, render, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    367\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_reward(rewards, t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_reward(rewards, t)\n\u001b[1;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_obs(observations, t)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/gymb.py:239\u001b[0m, in \u001b[0;36mGymAgent.set_reward\u001b[0;34m(self, rewards, t)\u001b[0m\n\u001b[1;32m    237\u001b[0m rewards \u001b[39m=\u001b[39m _torch_cat_dict(rewards)\n\u001b[1;32m    238\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m rewards:\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m+\u001b[39m k, t), rewards[k]\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mghost_params\u001b[39m.\u001b[39;49mdevice))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["params={\n","  \"save_best\": True,\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tmp/dqn-buffer-\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,\n","    \"hps\": {\n","      \"seed\": 4,\n","      \"max_grad_norm\": 0.5,\n","      \"epsilon\": 0.02,\n","      \"n_envs\": 8,\n","      \"n_steps\": 32,\n","      \"eval_interval\": 2000,\n","      \"learning_starts\": 2000,\n","      \"nb_evals\": 10,\n","      \"buffer_size\": 1e6,\n","      \"batch_size\": 256,\n","      \"target_critic_update\": 5000,\n","      \"max_epochs\": 3500,\n","      \"discount_factor\": 0.99,\n","      \"architecture\":{\"hidden_size\": [128, 128]},\n","      }  \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 4,\n","    \"max_grad_norm\": 0.5,\n","    \"epsilon\": 0.02,\n","    \"n_envs\": 8,\n","    \"n_steps\": 32,\n","    \"eval_interval\": 2000,\n","    \"learning_starts\": 2000,\n","    \"nb_evals\": 10,\n","    \"buffer_size\": 1e6,\n","    \"batch_size\": 256,\n","    \"target_critic_update\": 5000,\n","    \"max_epochs\": 3500,\n","    \"discount_factor\": 0.99,\n","    \"architecture\":{\"hidden_size\": [128, 128]},\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_env\",\n","    \"env_name\": \"CartPole-v1\",\n","  },\n","  \"optimizer\":\n","  {\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  }\n","}\n","\n","config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_dqn(config, compute_critic_loss, device)"]},{"cell_type":"code","execution_count":50,"id":"3f4b8d6f","metadata":{},"outputs":[],"source":["def setup_params(env_name, algo, optim, epsilon, n_envs, discount_factor, hidden_size, target_critic_update, max_epochs):\n","  hps = {\n","    \"seed\": 4,\n","    \"max_grad_norm\": 0.5,\n","    \"epsilon\": epsilon,\n","    \"n_envs\": n_envs,\n","    \"n_steps\": 32,\n","    \"eval_interval\": 2000,\n","    \"learning_starts\": 2000,\n","    \"nb_evals\": 10,\n","    \"buffer_size\": 1e6,\n","    \"batch_size\": 256,\n","    \"target_critic_update\": target_critic_update,\n","    \"max_epochs\": max_epochs,\n","    \"discount_factor\": discount_factor,\n","    \"architecture\":{\"hidden_size\": [hidden_size, hidden_size]},\n","  }\n","  params={\n","    \"save_best\": True,\n","    \"logger\":{\n","      \"classname\": \"bbrl.utils.logger.TFLogger\",\n","      \"log_dir\": \"./tmp/\"+ str(env_name) +\"/\" +str(algo)+\"/\"+str(algo)+\"-\" + str(time.time()),\n","      \"cache_size\": 10000,\n","      \"every_n_seconds\": 10,\n","      \"verbose\": False,\n","      \"hps\": hps\n","      },\n","\n","    \"algorithm\": hps,\n","    \"gym_env\":{\n","      \"classname\": \"__main__.make_env\",\n","      \"env_name\": env_name,\n","    },\n","    \"optimizer\":\n","    {\n","      \"classname\": optim,\n","      \"lr\": 1e-3,\n","    }\n","  }\n","  return params\n","    "]},{"cell_type":"code","execution_count":51,"id":"d7d19507","metadata":{},"outputs":[{"ename":"TypeError","evalue":"setup_params() missing 1 required positional argument: 'algo'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43msetup_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLunarLander-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch.optim.Adam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_critic_update\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: setup_params() missing 1 required positional argument: 'algo'"]}],"source":["params = setup_params(\n","    env_name = \"LunarLander-v2\",\n","    optim = \"torch.optim.Adam\", \n","    epsilon = 0.02,\n","    n_envs = 5,\n","    discount_factor = 0.99,\n","    hidden_size = 100,\n","    target_critic_update = 5000,\n","    max_epochs = 3000)"]},{"cell_type":"code","execution_count":52,"id":"be8752fa","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 14, reward: 9.300000190734863\n","epoch: 29, reward: 9.40000057220459\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m config\u001b[38;5;241m=\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mcreate(params)\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(config\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_critic_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [35], line 59\u001b[0m, in \u001b[0;36mrun_dqn\u001b[0;34m(cfg, compute_critic_loss, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Adds the transitions to the workspace\u001b[39;00m\n\u001b[1;32m     58\u001b[0m rb\u001b[38;5;241m.\u001b[39mput(transition_workspace)\n\u001b[0;32m---> 59\u001b[0m rb_workspace \u001b[38;5;241m=\u001b[39m \u001b[43mrb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_shuffled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m q_agent(rb_workspace, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, choose_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/utils/replay_buffer.py:114\u001b[0m, in \u001b[0;36mReplayBuffer.get_shuffled\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m workspace \u001b[39m=\u001b[39m Workspace()\n\u001b[1;32m    113\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables:\n\u001b[0;32m--> 114\u001b[0m     workspace\u001b[39m.\u001b[39mset_full(k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariables[k][who]\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m workspace\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_dqn(config, compute_critic_loss, device)"]},{"cell_type":"markdown","id":"4f3e26ce","metadata":{"id":"lou6LTvVK7YY"},"source":["The version used in this colab uses $< s_t, a_t, r_t, s_{t+1}>$ samples. As an exercise, you may switch to $< s_t, a_t, r_{t+1}, s_{t+1}>$ samples, going back to the standard SaLinA notation. For that, replace the import to `bbrl.agents.gyma` instead of `gymb`, and change the temporal difference update rule (in `compute_critic_loss(...)`) accordingly. See [this notebook](https://colab.research.google.com/drive/1Cld72_FBA1aMS2U4EsyV3LGZIlQC_PsC?usp=sharing) for more explanations."]},{"cell_type":"markdown","id":"cbe083fb","metadata":{"id":"v-rvnXyTH_W2"},"source":["## Coding Exercise: Double DQN (DDQN)\n","\n","In DQN, the same network is responsible for selecting and estimating the best next action (in the TD-target) and that may lead to over-estimation: the action which q-value is over-estimated will be chosen more often. As a result, training is slower.\n","\n","To reduce over-estimation, double q-learning (and then DDQN) was proposed. It decouples the action selection from the value estimation.\n","\n","Concretely, in DQN, the target value in the critic loss (used to update the Q critic) for a sample at time $t$ is defined as:\n","\n","$$Y^{DQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right); \\mathbb{\\theta}_{target}\\right)$$\n","\n","where the target network `target_q_agent` with parameters $\\mathbb{\\theta}_{target}$ is used for both action selection and estimation, and can therefore be rewritten:\n","\n","$$Y^{DQN}_{t} = r_{t+1} + \\gamma \\max_{a}{Q}\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right)$$\n","\n","Instead, DDQN uses the online critic `q_agent` with parameters $\\mathbb{\\theta}_{online}$ to select the action, whereas it uses the target network `target_q_agent` to estimate the associated Q-values:\n","\n","$$Y^{DDQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{online}\\right); \\mathbb{\\theta}_{target}\\right)$$\n","\n","\n","The goal in this exercise is for you to write the update method for `DDQN`.\n"]},{"cell_type":"code","execution_count":46,"id":"8162f158","metadata":{"executionInfo":{"elapsed":204,"status":"ok","timestamp":1662014038177,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"},"user_tz":-120},"id":"F3q7rekSfRML"},"outputs":[],"source":["def compute_ddqn_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n","    # À compléter...  \n","    argmax_q = q_values[1].max(1)[1].detach()  # Results in a (T x B) tensor\n","\n","    # To get the max of Q(s_{t+1}, a), we take max_q[1:]\n","    # The same about must_bootstrap. \n","    \n","    \n","    q_target_vals = target_q_values.gather(1, argmax_q.unsqueeze(-1)).squeeze(-1)\n","    \n","    target = (\n","        reward[:-1] + cfg.algorithm.discount_factor * q_target_vals * must_bootstrap.int()\n","    )\n","    # To get Q(s,a), we use torch.gather along the 3rd dimension (the action)\n","    qvals = q_values[0].gather(1, action[0].unsqueeze(-1)).squeeze(-1)\n","\n","    # Compute the temporal difference (use must_boostrap as to mask out finished episodes)\n","    td = (target - qvals) * must_bootstrap.int()\n","\n","    # Compute critic loss\n","    td = target - qvals\n","    td_error = td**2\n","    critic_loss = td_error.mean()\n","\n","    return critic_loss"]},{"cell_type":"code","execution_count":null,"id":"c17cb831","metadata":{},"outputs":[],"source":["params={\n","  \"save_best\": False,\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tmp/ddqn-buffer-\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,    \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 4,\n","    \"max_grad_norm\": 0.5,\n","    \"epsilon\": 0.02,\n","    \"n_envs\": 8,\n","    \"n_steps\": 32,\n","    \"eval_interval\": 2000,\n","    \"learning_starts\": 2000,\n","    \"nb_evals\": 10,\n","    \"buffer_size\": 1e6,\n","    \"batch_size\": 256,\n","    \"target_critic_update\": 5000,\n","    \"max_epochs\": 3500,\n","    \"discount_factor\": 0.99,\n","    \"architecture\":{\"hidden_size\": [128, 128]},\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_env\",\n","    \"env_name\": \"CartPole-v1\",\n","  },\n","  \"optimizer\":\n","  {\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  }\n","}\n"]},{"cell_type":"code","execution_count":54,"id":"993df8bb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 14, reward: 9.300000190734863\n","epoch: 29, reward: 9.40000057220459\n","epoch: 44, reward: 8.90000057220459\n","epoch: 59, reward: 9.40000057220459\n","epoch: 74, reward: 9.300000190734863\n","epoch: 89, reward: 9.100000381469727\n","epoch: 104, reward: 9.300000190734863\n","epoch: 119, reward: 8.90000057220459\n","epoch: 134, reward: 8.90000057220459\n","epoch: 149, reward: 9.0\n","epoch: 164, reward: 9.199999809265137\n","epoch: 179, reward: 9.300000190734863\n","epoch: 194, reward: 9.0\n","epoch: 209, reward: 9.5\n","epoch: 224, reward: 9.300000190734863\n","epoch: 239, reward: 9.699999809265137\n","epoch: 254, reward: 9.5\n","epoch: 269, reward: 10.40000057220459\n","epoch: 284, reward: 9.699999809265137\n","epoch: 299, reward: 9.699999809265137\n","epoch: 314, reward: 11.5\n","epoch: 328, reward: 15.800000190734863\n","epoch: 342, reward: 90.70000457763672\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [54], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m config\u001b[38;5;241m=\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mcreate(params)\n\u001b[1;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(config\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrun_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_ddqn_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [35], line 95\u001b[0m, in \u001b[0;36mrun_dqn\u001b[0;34m(cfg, compute_critic_loss, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m last_eval_step \u001b[38;5;241m=\u001b[39m nb_steps\n\u001b[1;32m     94\u001b[0m eval_workspace \u001b[38;5;241m=\u001b[39m Workspace()  \u001b[38;5;66;03m# Used for evaluation\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[43meval_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_workspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_variable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv/done\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoose_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m rewards \u001b[38;5;241m=\u001b[39m eval_workspace[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/cumulated_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     99\u001b[0m mean \u001b[38;5;241m=\u001b[39m rewards\u001b[38;5;241m.\u001b[39mmean()\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m _t \u001b[39m=\u001b[39m t\n\u001b[1;32m     78\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent(workspace, t\u001b[39m=\u001b[39;49m_t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m stop_variable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         s \u001b[39m=\u001b[39m workspace\u001b[39m.\u001b[39mget(stop_variable, _t)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, workspace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents:\n\u001b[0;32m---> 31\u001b[0m         a(workspace, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/agent.py:64\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39massert\u001b[39;00m workspace \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39m[Agent.__call__] workspace must not be None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkspace \u001b[39m=\u001b[39m workspace\n\u001b[0;32m---> 64\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkspace \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/gymb.py:261\u001b[0m, in \u001b[0;36mGymAgent.forward\u001b[0;34m(self, t, save_render, render, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m rewards \u001b[39m=\u001b[39m []\n\u001b[1;32m    260\u001b[0m \u001b[39mfor\u001b[39;00m k, e \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs):\n\u001b[0;32m--> 261\u001b[0m     obs, reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(k, action[k], save_render, render)\n\u001b[1;32m    262\u001b[0m     observations\u001b[39m.\u001b[39mappend(obs)\n\u001b[1;32m    263\u001b[0m     rewards\u001b[39m.\u001b[39mappend(reward)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/gymb.py:213\u001b[0m, in \u001b[0;36mGymAgent._step\u001b[0;34m(self, k, action, save_render, render)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    201\u001b[0m         {\n\u001b[1;32m    202\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_frame[k],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m         rew,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestep[k] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 213\u001b[0m full_obs, reward, done, truncated, observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_step(\n\u001b[1;32m    214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[k], action, k, save_render, render\n\u001b[1;32m    215\u001b[0m )\n\u001b[1;32m    217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_frame[k] \u001b[39m=\u001b[39m observation\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m done:\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/gymb.py:165\u001b[0m, in \u001b[0;36mGymAgent._make_step\u001b[0;34m(self, env, action, k, save_render, render)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_frame[k] \u001b[39m=\u001b[39m observation\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m full_obs\n\u001b[0;32m--> 165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_step\u001b[39m(\u001b[39mself\u001b[39m, env, action, k, save_render, render):\n\u001b[1;32m    166\u001b[0m     action \u001b[39m=\u001b[39m _convert_action(action)\n\u001b[1;32m    168\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["params = setup_params(\n","    env_name = \"CartPole-v1\",\n","    algo = \"ddqn\",\n","    optim = \"torch.optim.Adam\", \n","    epsilon = 0.02,\n","    n_envs = 5,\n","    discount_factor = 0.99,\n","    hidden_size = 100,\n","    target_critic_update = 5000,\n","    max_epochs = 3000)\n","\n","config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_dqn(config, compute_ddqn_loss, device)"]},{"cell_type":"code","execution_count":null,"id":"d6fcacce","metadata":{},"outputs":[],"source":["hidden_sizes = np.arange(80,210,10)\n","\n","for hd in hidden_sizes:\n","    params = setup_params(\n","        env_name = \"CartPole-v1\",\n","        algo = \"ddqn\",\n","        optim = \"torch.optim.Adam\", \n","        epsilon = 0.02,\n","        n_envs = 5,\n","        discount_factor = 0.99,\n","        hidden_size = hd,\n","        target_critic_update = 5000,\n","        max_epochs = 3000)\n","    config=OmegaConf.create(params)\n","    torch.manual_seed(config.algorithm.seed)\n","    run_dqn(config, compute_ddqn_loss, device)"]}],"metadata":{"jupytext":{"formats":"ipynb,Rmd"},"kernelspec":{"display_name":"Python 3.10.6 ('deepdac')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"b8381a9192b883635ac16797c97396a9adba45cea8be9b52669f729c8ec391d7"}}},"nbformat":4,"nbformat_minor":5}
