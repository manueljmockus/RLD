{"cells":[{"cell_type":"markdown","id":"8c805b53","metadata":{"id":"dYfGJCe52lP4"},"source":["# Outlook"]},{"cell_type":"markdown","id":"3632316b","metadata":{"id":"aZUSf0n_2otG"},"source":["In this notebook, using BBRL, we code the DDPG algorithm. To understand this code, you need [to know more about BBRL](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing). You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1gSdkOBPkIQi_my9TtwJ-qWZQS0b2X7jt?usp=sharing), then [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) and, most importantly, [details about the AutoResetGymAgent](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing)."]},{"cell_type":"markdown","id":"5224201c","metadata":{"id":"HYW4-1lsSwwS"},"source":["The DDPG algorithm is explained in [this video](https://www.youtube.com/watch?v=0D6a0a1HTtc) and you can also read [the corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ddpg.pdf)."]},{"cell_type":"markdown","id":"d1b7acc3","metadata":{"id":"zJZDcDafp7Uf"},"source":["## Installation and Imports"]},{"cell_type":"markdown","id":"707ce97b","metadata":{"id":"aHO1nIdM21Lq"},"source":["### Installation"]},{"cell_type":"markdown","id":"df52173c","metadata":{"id":"Ymc-lbXi9vDE"},"source":["The BBRL library is [here](https://github.com/osigaud/bbrl)."]},{"cell_type":"markdown","id":"01e4b9f0","metadata":{"id":"pDy9yuQH73tJ"},"source":["This is OmegaConf that makes it possible that by just defining the `def run_dqn(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n","\n","More precisely, the code is run by calling\n","\n","`config=OmegaConf.create(params)`\n","\n","`run_dqn(config)`\n","\n","at the very bottom of the colab, after starting tensorboard."]},{"cell_type":"code","execution_count":1,"id":"3eb9573c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"j0MaggiOl4KU","outputId":"7da8e9e4-0352-49ee-dda1-00ac0ac349c6"},"outputs":[],"source":["try:\n","    from easypip import easyimport\n","except:\n","    !pip install easypip\n","    from easypip import easyimport\n","\n","import os\n","import functools\n","import time\n","from typing import Tuple\n","\n","OmegaConf = easyimport(\"omegaconf\").OmegaConf\n","torch = easyimport(\"torch\")\n","bbrl_gym = easyimport(\"bbrl_gym\") \n","bbrl = easyimport(\"bbrl\")"]},{"cell_type":"markdown","id":"6dc67b3c","metadata":{"id":"m4kV9pWV3wRe"},"source":["### Imports"]},{"cell_type":"markdown","id":"70bb67e3","metadata":{"id":"caqhJYbe5YcO"},"source":["Below, we import standard python packages, pytorch packages and gym environments."]},{"cell_type":"markdown","id":"53dd02f4","metadata":{"id":"4l7sTVXbJBE_"},"source":["[OpenAI gym](https://gym.openai.com/) is a collection of benchmark environments to evaluate RL algorithms."]},{"cell_type":"code","execution_count":2,"id":"9a45d991","metadata":{"id":"vktQB-AO5biu"},"outputs":[],"source":["import copy\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import gym"]},{"cell_type":"markdown","id":"50e21ca8","metadata":{"id":"fE1c7ZLf60X_"},"source":["### BBRL imports"]},{"cell_type":"code","execution_count":3,"id":"ec7281a1","metadata":{"id":"RcuqoAvG3zMZ"},"outputs":[],"source":["from bbrl.agents.agent import Agent\n","from bbrl import get_arguments, get_class, instantiate_class\n","\n","# The workspace is the main class in BBRL, this is where all data is collected and stored\n","from bbrl.workspace import Workspace\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n","\n","# AutoResetGymAgent is an agent able to execute a batch of gym environments\n","# with auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n","# Not present in the A2C version...\n","from bbrl.utils.logger import TFLogger\n","from bbrl.utils.replay_buffer import ReplayBuffer"]},{"cell_type":"markdown","id":"b7725320","metadata":{"id":"JVvAfhKm9S8p"},"source":["## Definition of agents"]},{"cell_type":"markdown","id":"2190a1d5","metadata":{"id":"RdqzKSLKDtqz"},"source":["The [DDPG](https://arxiv.org/pdf/1509.02971.pdf) algorithm is an actor critic algorithm. We use the standard function for building the neural networks that will play the role of the actor and the critic."]},{"cell_type":"code","execution_count":4,"id":"7f3c8a3a","metadata":{"id":"r_QIxxHNtBMH"},"outputs":[],"source":["def build_mlp(sizes, activation, output_activation=nn.Identity()):\n","    layers = []\n","    for j in range(len(sizes) - 1):\n","        act = activation if j < len(sizes) - 2 else output_activation\n","        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n","    return nn.Sequential(*layers)"]},{"cell_type":"markdown","id":"c1ed86bc","metadata":{"id":"9YEh57AHeVB4"},"source":["To implement the gym environment, we use the same approach as usual."]},{"cell_type":"code","execution_count":5,"id":"6837f2e2","metadata":{"id":"Fsb5QRzw7V0o"},"outputs":[],"source":["def make_gym_env(env_name):\n","    return gym.make(env_name)"]},{"cell_type":"markdown","id":"21d9cda3","metadata":{"id":"hgE2yEat36XV"},"source":["The critic is a neural network taking the state $s$ and action $a$ as input, and its output layer has a unique neuron whose value is the value of being in that state and performing that action $Q(s,a)$.\n","\n","As usual, the ```forward(...)``` function is used to write Q-values in the workspace from time indexes, whereas the ```predict_value(...)``` function` is used in other contexts, such as plotting a view of the Q function."]},{"cell_type":"code","execution_count":6,"id":"ea2c0a53","metadata":{"id":"-ygl_STM3zKa"},"outputs":[],"source":["class ContinuousQAgent(Agent):\n","    def __init__(self, state_dim, hidden_layers, action_dim):\n","        super().__init__()\n","        self.is_q_function = True\n","        self.model = build_mlp(\n","            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n","        )\n","\n","    def forward(self, t, detach_actions=False):\n","        obs = self.get((\"env/env_obs\", t))\n","        action = self.get((\"action\", t))\n","        if detach_actions:\n","            action = action.detach()\n","        osb_act = torch.cat((obs, action), dim=1)\n","        q_value = self.model(osb_act)\n","        self.set((\"q_value\", t), q_value)\n","\n","    def predict_value(self, obs, action):\n","        osb_act = torch.cat((obs, action), dim=0)\n","        q_value = self.model(osb_act)\n","        return q_value"]},{"cell_type":"markdown","id":"12b08f72","metadata":{"id":"zW0Vrm2438Hz"},"source":["The actor is also a neural network, it takes a state $s$ as input and outputs an action $a$."]},{"cell_type":"code","execution_count":7,"id":"5916b981","metadata":{"id":"C67eALQ63mQm"},"outputs":[],"source":["class ContinuousDeterministicActor(Agent):\n","    def __init__(self, state_dim, hidden_layers, action_dim):\n","        super().__init__()\n","        layers = [state_dim] + list(hidden_layers) + [action_dim]\n","        self.model = build_mlp(\n","            layers, activation=nn.ReLU(), output_activation=nn.Tanh()\n","        )\n","\n","    def forward(self, t):\n","        obs = self.get((\"env/env_obs\", t))\n","        action = self.model(obs)\n","        self.set((\"action\", t), action)\n","\n","    def predict_action(self, obs, stochastic):\n","        assert (\n","            not stochastic\n","        ), \"ContinuousDeterministicActor cannot provide stochastic predictions\"\n","        return self.model(obs)"]},{"cell_type":"markdown","id":"fb2c240f","metadata":{"id":"yoG1eNBguNTN"},"source":["### Creating an Exploration method"]},{"cell_type":"markdown","id":"e5b181cb","metadata":{"id":"qvxOy0e180_6"},"source":["In the continuous action domain, basic exploration differs from the methods used in the discrete action domain. Here we generally add some Gaussian noise to the output of the actor."]},{"cell_type":"code","execution_count":8,"id":"fa8c64c8","metadata":{"id":"ATPprO4B8smL"},"outputs":[],"source":["from torch.distributions import Normal"]},{"cell_type":"code","execution_count":9,"id":"920e7056","metadata":{"id":"IXroJC7D3ZiD"},"outputs":[],"source":["class AddGaussianNoise(Agent):\n","    def __init__(self, sigma):\n","        super().__init__()\n","        self.sigma = sigma\n","\n","    def forward(self, t, **kwargs):\n","        act = self.get((\"action\", t))\n","        dist = Normal(act, self.sigma)\n","        action = dist.sample()\n","        self.set((\"action\", t), action)\n","\n"]},{"cell_type":"markdown","id":"3d86b47a","metadata":{"id":"d-j7mPDc3g9J"},"source":["In [the original DDPG paper](https://arxiv.org/pdf/1509.02971.pdf), the authors rather used the more sophisticated Ornstein-Uhlenbeck noise where noise is correlated between one step and the next."]},{"cell_type":"code","execution_count":10,"id":"6b70b50a","metadata":{"id":"oo3DoKew3dWS"},"outputs":[],"source":["class AddOUNoise(Agent):\n","    \"\"\"\n","    Ornstein Uhlenbeck process noise for actions as suggested by DDPG paper\n","    \"\"\"\n","\n","    def __init__(self, std_dev, theta=0.15, dt=1e-2):\n","        self.theta = theta\n","        self.std_dev = std_dev\n","        self.dt = dt\n","        self.x_prev = 0\n","\n","    def forward(self, t, **kwargs):\n","        act = self.get((\"action\", t))\n","        x = (\n","            self.x_prev\n","            + self.theta * (act - self.x_prev) * self.dt\n","            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)\n","        )\n","        self.x_prev = x\n","        self.set((\"action\", t), x)\n"]},{"cell_type":"markdown","id":"9ef71cc4","metadata":{"id":"W0AgHYc2ywoS"},"source":["### Training and evaluation environments"]},{"cell_type":"markdown","id":"9036701a","metadata":{"id":"1gfbKZMFrghw"},"source":["We build two environments: one for training and another one for evaluation."]},{"cell_type":"markdown","id":"bbe0cb07","metadata":{"id":"jDM2Z0THyrtx"},"source":["For training, it is more efficient to use an AutoResetGymAgent, as we do not want to waste time if the task is done in an environment sooner than in the others."]},{"cell_type":"markdown","id":"b101d7fc","metadata":{"id":"7kN-SniayxRq"},"source":["By contrast, for evaluation, we just need to perform a fixed number of episodes (for statistics), thus it is more convenient to use a NoAutoResetGymAgent with a set of environments and just run one episode in each environment. Thus we can use the `env/done` stop variable and take the average over the cumulated reward of all environments."]},{"cell_type":"markdown","id":"970930b6","metadata":{"id":"7_D4Fb4Fz6M1"},"source":["\n","See [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) for explanations about agents and environment agents."]},{"cell_type":"code","execution_count":11,"id":"cf7c5003","metadata":{"id":"hT5mr2yGyeUP"},"outputs":[],"source":["def get_env_agents(cfg):\n","    train_env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","    eval_env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.nb_evals,\n","        cfg.algorithm.seed,\n","    )\n","    return train_env_agent, eval_env_agent"]},{"cell_type":"markdown","id":"75d89c22","metadata":{"id":"EzxoIPtLVJ_i"},"source":["### Create the DDPG agent"]},{"cell_type":"markdown","id":"0823cbc6","metadata":{"id":"aaNnZw3bXEYd"},"source":["In this function we create the critic and the actor, but also an exploration agent to add noise and a target critic. The version below does not use a target actor as it proved hard to tune, but such a target actor is used in the original paper."]},{"cell_type":"code","execution_count":12,"id":"70befeb0","metadata":{"id":"G8Uk_RQh8QrO"},"outputs":[],"source":["# Create the DDPG Agent\n","def create_ddpg_agent(cfg, train_env_agent, eval_env_agent):\n","    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n","    critic = ContinuousQAgent(\n","        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n","    )\n","    target_critic = copy.deepcopy(critic)\n","    actor = ContinuousDeterministicActor(\n","        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n","    )\n","    # target_actor = copy.deepcopy(actor) # not used in practice, though described in the paper\n","    noise_agent = AddGaussianNoise(cfg.algorithm.action_noise) # alternative : AddOUNoise\n","    tr_agent = Agents(train_env_agent, actor, noise_agent)  \n","    ev_agent = Agents(eval_env_agent, actor)\n","\n","    # Get an agent that is executed on a complete workspace\n","    train_agent = TemporalAgent(tr_agent)\n","    eval_agent = TemporalAgent(ev_agent)\n","    train_agent.seed(cfg.algorithm.seed)\n","    return train_agent, eval_agent, actor, critic, target_critic  # , target_actor"]},{"cell_type":"markdown","id":"4611bf9c","metadata":{"id":"lU3cO6znHyDc"},"source":["### The Logger class"]},{"cell_type":"markdown","id":"cf09cd6e","metadata":{"id":"_1gszmpwhitv"},"source":["Explanations for the logger were already given in [this notebook](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV?usp=sharing)."]},{"cell_type":"code","execution_count":13,"id":"df28725a","metadata":{"id":"aOkauz_0H2GA"},"outputs":[],"source":["class Logger():\n","\n","  def __init__(self, cfg):\n","    self.logger = instantiate_class(cfg.logger)\n","\n","  def add_log(self, log_string, loss, epoch):\n","    self.logger.add_scalar(log_string, loss.item(), epoch)\n","\n","  # Log losses\n","  def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):\n","    self.add_log(\"critic_loss\", critic_loss, epoch)\n","    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n","    self.add_log(\"a2c_loss\", a2c_loss, epoch)\n"]},{"cell_type":"markdown","id":"c083d953","metadata":{"id":"f2vq1OJHWCIE"},"source":["### Setup the optimizers"]},{"cell_type":"markdown","id":"7db83e02","metadata":{"id":"VzmEKF4J8qjg"},"source":["We use two separate optimizers to tune the parameters of the actor and the critic separately. That makes it possible to use a different learning rate for the actor and the critic."]},{"cell_type":"code","execution_count":14,"id":"451a681b","metadata":{"id":"YFfzXEu2WFWj"},"outputs":[],"source":["# Configure the optimizers\n","def setup_optimizers(cfg, actor, critic):\n","    actor_optimizer_args = get_arguments(cfg.actor_optimizer)\n","    parameters = actor.parameters()\n","    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)\n","    critic_optimizer_args = get_arguments(cfg.critic_optimizer)\n","    parameters = critic.parameters()\n","    critic_optimizer = get_class(cfg.critic_optimizer)(\n","        parameters, **critic_optimizer_args\n","    )\n","    return actor_optimizer, critic_optimizer"]},{"cell_type":"markdown","id":"c44090d5","metadata":{"id":"YQNvhO_VAJbh"},"source":["### Compute critic loss"]},{"cell_type":"markdown","id":"c8b9283e","metadata":{"id":"fxxobbxRaJXO"},"source":["Detailed explanations of the function to compute the critic loss when using a NoAutoResetGymAgent are given in [this notebook](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV?usp=sharing)."]},{"cell_type":"markdown","id":"ebbfc746","metadata":{"id":"fXwrjbueoDw6"},"source":["The case where we use the AutoResetGymAgent is very similar, but we need to specify that we use the first part of the Q-values (`q_values[0]`) for representing $Q(s_t,a_t)$ and the second part (`q_values[1]`) for representing $Q(s_{t+1},a)$, as these values are stored into a transition model. Then the values used to compute the target in the critic update are stored into ```target_q_values```."]},{"cell_type":"code","execution_count":15,"id":"5c3c7cbb","metadata":{"id":"2sepUK-gAM3u"},"outputs":[],"source":["def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values):\n","    # Compute temporal difference\n","    q_next = target_q_values\n","    target = (\n","        reward[:-1][0]\n","        + cfg.algorithm.discount_factor * q_next.squeeze(-1) * must_bootstrap.int()\n","    )\n","    td = target - q_values.squeeze(-1)\n","    # Compute critic loss\n","    td_error = td**2\n","    critic_loss = td_error.mean()\n","    return critic_loss"]},{"cell_type":"markdown","id":"6d8f50c8","metadata":{"id":"ljCg7i26XLgb"},"source":["To update the target critic, one uses the following equation:"]},{"cell_type":"markdown","id":"f0e26f48","metadata":{"id":"AO0AShomKBz8"},"source":["$\\theta' \\leftarrow \\tau \\theta + (1- \\tau) \\theta'$\n","\n"]},{"cell_type":"markdown","id":"3f3b915b","metadata":{"id":"EFY9VV1UXSsT"},"source":["where $\\theta$ is the vector of parameters of the critic, and $\\theta'$ is the vector of parameters of the target critic."]},{"cell_type":"markdown","id":"bfde9f46","metadata":{"id":"ty_UYntZXf9Q"},"source":["The `soft_update_params(...)` function is in charge of performing this soft update."]},{"cell_type":"code","execution_count":16,"id":"bb77ea0f","metadata":{"id":"cfq7tKEI8UWQ"},"outputs":[],"source":["def soft_update_params(net, target_net, tau):\n","    for param, target_param in zip(net.parameters(), target_net.parameters()):\n","        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"]},{"cell_type":"markdown","id":"81fb5682","metadata":{"id":"ixiF82r3x1ZT"},"source":["### Compute actor loss"]},{"cell_type":"markdown","id":"f2decaf7","metadata":{"id":"CUlRVqJKx5g9"},"source":["The actor loss is straightforward. We want the actor to maximize Q-values, thus we minimize the mean of negated Q-values."]},{"cell_type":"code","execution_count":17,"id":"1a460700","metadata":{"id":"DTXjXTRi5Ipp"},"outputs":[],"source":["def compute_actor_loss(q_values):\n","    return -q_values.mean()"]},{"cell_type":"markdown","id":"542a4900","metadata":{"id":"Jmi91gANWT4z"},"source":["## Main training loop"]},{"cell_type":"markdown","id":"01792bf0","metadata":{"id":"I6SuPOdW_hxl"},"source":["### Agent execution"]},{"cell_type":"markdown","id":"bf087977","metadata":{"id":"WqlH-8DaVWx2"},"source":["This is the tricky part with BBRL, the one we need to understand in detail. The difficulty lies in the copy of the last step and the way to deal with the n_steps return."]},{"cell_type":"markdown","id":"fdf86f65","metadata":{"id":"bWAmm0pPotTC"},"source":["The call to `train_agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True)` makes the agent run a number of steps in the workspace. In practice, it calls the [`__call__(...)`](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/agent.py#L54) function which makes a forward pass of the agent network using the workspace data and updates the workspace accordingly."]},{"cell_type":"markdown","id":"d39c744a","metadata":{"id":"Rn3MlNQ3qGPr"},"source":["Now, if we start at the first epoch (`epoch=0`), we start from the first step (`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must not forget to cover the transition at the border between the previous epoch and the current epoch. To avoid this risk, we copy the information from the last time step of the previous epoch into the first time step of the next epoch. This is explained in more details in [a previous notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."]},{"cell_type":"markdown","id":"b3dcd230","metadata":{"id":"gAnnEjF9L9gk"},"source":["A [previous notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5) explains a lot of these details. In particular, read it to understand the `execute_agents(...)` function, the `transition_workspace = train_workspace.get_transitions()` line and the computation of `must_bootstrap`."]},{"cell_type":"markdown","id":"647337bb","metadata":{"id":"OFB1XFE5YEc6"},"source":["Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` lines. \n","\n","`optimizer.zero_grad()` is necessary to cancel all the gradients computed at the previous iterations\n"]},{"cell_type":"markdown","id":"90e2cabd","metadata":{"id":"SqFEgJIrhdxE"},"source":["Note the way we count the steps, to properly ignore the steps corresponding to a transition from an episode to the next."]},{"cell_type":"markdown","id":"fbb7f8b2","metadata":{"id":"O_cswkRmhnTX"},"source":["Note also that every ```cfg.algorithm.eval_interval```, we evaluate the current agent and save it and plot it if its performance is the new best performance."]},{"cell_type":"code","execution_count":18,"id":"1170ba2d","metadata":{"id":"-Eu4Pjvp8HWp"},"outputs":[],"source":["from bbrl.visu.visu_policies import plot_policy\n","from bbrl.visu.visu_critics import plot_critic"]},{"cell_type":"code","execution_count":31,"id":"df33b10a","metadata":{"id":"sk85_sRWW-5s"},"outputs":[],"source":["def run_ddpg(cfg):\n","    # 1)  Build the  logger\n","    logger = Logger(cfg)\n","    best_reward = -10e9\n","\n","    # 2) Create the environment agent\n","    train_env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","    eval_env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.nb_evals,\n","        cfg.algorithm.seed,\n","    )\n","\n","    # 3) Create the DDPG Agent\n","    (\n","        train_agent,\n","        eval_agent,\n","        actor,\n","        critic,\n","        # target_actor,\n","        target_critic,\n","    ) = create_ddpg_agent(cfg, train_env_agent, eval_env_agent)\n","    ag_actor = TemporalAgent(actor)\n","    # ag_target_actor = TemporalAgent(target_actor)\n","    q_agent = TemporalAgent(critic)\n","    target_q_agent = TemporalAgent(target_critic)\n","\n","    train_workspace = Workspace()\n","    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n","\n","    # Configure the optimizer\n","    actor_optimizer, critic_optimizer = setup_optimizers(cfg, actor, critic)\n","    nb_steps = 0\n","    tmp_steps = 0\n","    \n","    # Pass to cuda\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(device)\n","    train_workspace = train_workspace.to(device)\n","    ag_actor = ag_actor.to(device)\n","    q_agent = q_agent.to(device)\n","    target_q_agent = target_q_agent.to(device)\n","    train_agent = train_agent.to(device)\n","    \n","    # Training loop\n","    for epoch in range(cfg.algorithm.max_epochs):\n","        # Execute the agent in the workspace\n","        if epoch > 0:\n","            train_workspace.zero_grad()\n","            train_workspace.copy_n_last_steps(1)\n","            train_agent(train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1)\n","        else:\n","            train_agent(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n","\n","        transition_workspace = train_workspace.get_transitions()\n","        action = transition_workspace[\"action\"]\n","        nb_steps += action[0].shape[0]\n","        rb.put(transition_workspace)\n","        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n","        rb_workspace = rb_workspace.to(device)\n","\n","        done, truncated, reward, action = rb_workspace[\n","            \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n","        ]\n","        # print(f\"done {done}, reward {reward}, action {action}\")\n","        if nb_steps > cfg.algorithm.learning_starts:\n","            # Determines whether values of the critic should be propagated\n","            # True if the episode reached a time limit or if the task was not done\n","            # See https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing\n","            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n","\n","            # Critic update\n","            # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB\n","            q_agent(rb_workspace, t=0, n_steps=1)\n","            q_values = rb_workspace[\"q_value\"]\n","            # print(f\"q_values ante : {q_values}\")\n","\n","            with torch.no_grad():\n","                # replace the action at t+1 in the RB with \\pi(s_{t+1}), to compute Q(s_{t+1}, \\pi(s_{t+1}) below\n","                ag_actor(rb_workspace, t=1, n_steps=1)\n","                # compute q_values: at t+1 we have Q(s_{t+1}, \\pi(s_{t+1})\n","                target_q_agent(rb_workspace, t=1, n_steps=1)\n","                # q_agent(rb_workspace, t=1, n_steps=1)\n","            # finally q_values contains the above collection at t=0 and t=1\n","            post_q_values = rb_workspace[\"q_value\"]\n","            # print(f\"q_values post : {post_q_values[1]}\")\n","\n","            # Compute critic loss\n","            critic_loss = compute_critic_loss(\n","                cfg, reward, must_bootstrap, q_values[0], post_q_values[1]\n","            )\n","            logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n","            critic_optimizer.zero_grad()\n","            critic_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                critic.parameters(), cfg.algorithm.max_grad_norm\n","            )\n","            critic_optimizer.step()\n","\n","            # Actor update\n","            # Now we determine the actions the current policy would take in the states from the RB\n","            ag_actor(rb_workspace, t=0, n_steps=1)\n","            # We determine the Q values resulting from actions of the current policy\n","            q_agent(rb_workspace, t=0, n_steps=1)\n","            # and we back-propagate the corresponding loss to maximize the Q values\n","            q_values = rb_workspace[\"q_value\"]\n","            actor_loss = compute_actor_loss(q_values)\n","            logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n","            # if -25 < actor_loss < 0 and nb_steps > 2e5:\n","            actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                actor.parameters(), cfg.algorithm.max_grad_norm\n","            )\n","            actor_optimizer.step()\n","            # Soft update of target q function\n","            tau = cfg.algorithm.tau_target\n","            soft_update_params(critic, target_critic, tau)\n","            # soft_update_params(actor, target_actor, tau)\n","\n","        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n","            tmp_steps = nb_steps\n","            eval_workspace = Workspace()  # Used for evaluation\n","            eval_workspace = eval_workspace.to(device)\n","            eval_agent = eval_agent.to(device)\n","            eval_agent(eval_workspace, t=0, stop_variable=\"env/done\")\n","            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n","            mean = rewards.mean()\n","            logger.add_log(\"reward\", mean, nb_steps)\n","            print(f\"nb_steps: {nb_steps}, reward: {mean}, epoch: {epoch}\")\n","\n","            if mean > best_reward:\n","                best_reward = mean\n","\n","                if cfg.save_best:\n","                    directory = \"./ddpg_agent/\"\n","                    if not os.path.exists(directory):\n","                        os.makedirs(directory)\n","                    filename = directory + \"ddpg_\" + str(mean.item()) + \".agt\"\n","                    eval_agent.save_model(filename)\n","\n","                if cfg.plot_agents:\n","                    actor = actor.to(device),\n","                    eval_env_agent = eval_env_agent.to(device),\n","                    plot_policy(\n","                        actor,\n","                        eval_env_agent,\n","                        \"./ddpg_plots/\",\n","                        cfg.gym_env.env_name,\n","                        best_reward,\n","                        stochastic=False,\n","                    )\n","                    plot_critic(\n","                        q_agent.agent,\n","                        eval_env_agent,\n","                        \"./ddpg_plots/\",\n","                        cfg.gym_env.env_name,\n","                        best_reward,\n","                    )\n"]},{"cell_type":"markdown","id":"7351d9d1","metadata":{"id":"uo6bc3zzKua_"},"source":["## Definition of the parameters"]},{"cell_type":"markdown","id":"8c1ef631","metadata":{"id":"36r4PAfvKx-f"},"source":["The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation."]},{"cell_type":"code","execution_count":35,"id":"09e2d3a3","metadata":{"id":"JB2B8zELNWQd"},"outputs":[],"source":["params={\n","  \"save_best\": True,\n","  # Set to true to have an insight on the learned policy\n","  # (but slows down the evaluation a lot!)\n","  \"plot_agents\": False,\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tblogs/ddpg/ddpg-\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,    \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 1,\n","    \"max_grad_norm\": 0.5,\n","    \"epsilon\": 0.02,\n","    \"n_envs\": 1,\n","    \"n_steps\": 100,\n","    \"eval_interval\": 2000,\n","    \"nb_evals\": 10,\n","    \"gae\": 0.8,\n","    \"max_epochs\": 2100,\n","    \"discount_factor\": 0.98,\n","    \"buffer_size\": 2e5,\n","    \"batch_size\": 64,\n","    \"tau_target\": 0.05,\n","    \"learning_starts\": 10000,\n","    \"action_noise\": 0.1,\n","    \"architecture\":{\n","        \"actor_hidden_size\": [400, 300],\n","        \"critic_hidden_size\": [400, 300],\n","        },\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_gym_env\",\n","    \"env_name\": \"Pendulum-v1\",\n","  },\n","  \"actor_optimizer\":{\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  },\n","  \"critic_optimizer\":{\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  }\n","}"]},{"cell_type":"markdown","id":"f4a888b0","metadata":{"id":"jp7jDeGkaoM1"},"source":["### Launching tensorboard to visualize the results"]},{"cell_type":"code","execution_count":36,"id":"2b1f2c25","metadata":{"id":"-BlLQlwDn5Vh"},"outputs":[{"name":"stdout","output_type":"stream","text":["Launch tensorboard from the shell:\n","/home/manuel/deepdac/bin/tensorboard --logdir=/home/manuel/RLD/TP4/tmp\n"]}],"source":["# For Colab - otherwise, it is easier and better to launch tensorboard from\n","# the terminal\n","if get_ipython().__class__.__module__ == \"google.colab._shell\":\n","    %load_ext tensorboard\n","    %tensorboard --logdir ./tmp\n","else:\n","    import sys\n","    import os\n","    import os.path as osp\n","    print(f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={os.getcwd()}/tmp\")"]},{"cell_type":"code","execution_count":37,"id":"bd3b3d8f","metadata":{"id":"l42OUoGROlSt"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","nb_steps: 2069, reward: -1179.7987060546875, epoch: 20\n","nb_steps: 4138, reward: -1271.1614990234375, epoch: 41\n","nb_steps: 6206, reward: -1277.4393310546875, epoch: 62\n","nb_steps: 8275, reward: -1214.9703369140625, epoch: 83\n","nb_steps: 10344, reward: -1493.2109375, epoch: 104\n","nb_steps: 12412, reward: -1637.2738037109375, epoch: 125\n","nb_steps: 14481, reward: -1679.9764404296875, epoch: 146\n","nb_steps: 16550, reward: -1616.8427734375, epoch: 167\n","nb_steps: 18618, reward: -1621.5955810546875, epoch: 188\n","nb_steps: 20687, reward: -1411.6834716796875, epoch: 209\n","nb_steps: 22756, reward: -1527.14599609375, epoch: 230\n","nb_steps: 24824, reward: -1563.2398681640625, epoch: 251\n","nb_steps: 26893, reward: -1479.7080078125, epoch: 272\n","nb_steps: 28962, reward: -1482.560302734375, epoch: 293\n","nb_steps: 31030, reward: -1587.6307373046875, epoch: 314\n","nb_steps: 33099, reward: -1521.2431640625, epoch: 335\n","nb_steps: 35168, reward: -1655.419189453125, epoch: 356\n","nb_steps: 37236, reward: -1491.2674560546875, epoch: 377\n","nb_steps: 39305, reward: -1556.5057373046875, epoch: 398\n","nb_steps: 41374, reward: -1322.2086181640625, epoch: 419\n","nb_steps: 43442, reward: -1438.6441650390625, epoch: 440\n","nb_steps: 45511, reward: -1243.970703125, epoch: 461\n","nb_steps: 47580, reward: -1093.5582275390625, epoch: 482\n","nb_steps: 49648, reward: -1328.1571044921875, epoch: 503\n","nb_steps: 51717, reward: -1273.165283203125, epoch: 524\n","nb_steps: 53786, reward: -1120.900634765625, epoch: 545\n","nb_steps: 55854, reward: -1178.3023681640625, epoch: 566\n","nb_steps: 57923, reward: -1080.39306640625, epoch: 587\n","nb_steps: 59992, reward: -1183.6162109375, epoch: 608\n","nb_steps: 62060, reward: -1110.650390625, epoch: 629\n","nb_steps: 64129, reward: -1059.2242431640625, epoch: 650\n","nb_steps: 66198, reward: -1046.8553466796875, epoch: 671\n","nb_steps: 68266, reward: -1010.2706909179688, epoch: 692\n","nb_steps: 70335, reward: -1053.5150146484375, epoch: 713\n","nb_steps: 72403, reward: -1105.36181640625, epoch: 734\n","nb_steps: 74472, reward: -1077.094970703125, epoch: 755\n","nb_steps: 76541, reward: -1028.59912109375, epoch: 776\n","nb_steps: 78609, reward: -1051.5748291015625, epoch: 797\n","nb_steps: 80678, reward: -1117.1470947265625, epoch: 818\n","nb_steps: 82747, reward: -1115.7982177734375, epoch: 839\n","nb_steps: 84815, reward: -1100.8125, epoch: 860\n","nb_steps: 86884, reward: -1044.2913818359375, epoch: 881\n","nb_steps: 88953, reward: -1048.6688232421875, epoch: 902\n","nb_steps: 91021, reward: -1107.828125, epoch: 923\n","nb_steps: 93090, reward: -1096.701416015625, epoch: 944\n","nb_steps: 95159, reward: -1029.569580078125, epoch: 965\n","nb_steps: 97227, reward: -995.5888671875, epoch: 986\n","nb_steps: 99296, reward: -1075.7294921875, epoch: 1007\n","nb_steps: 101365, reward: -1028.64404296875, epoch: 1028\n","nb_steps: 103433, reward: -1004.4400634765625, epoch: 1049\n","nb_steps: 105502, reward: -942.7421875, epoch: 1070\n","nb_steps: 107571, reward: -1043.4815673828125, epoch: 1091\n","nb_steps: 109639, reward: -1010.4656372070312, epoch: 1112\n","nb_steps: 111708, reward: -1082.7952880859375, epoch: 1133\n","nb_steps: 113777, reward: -1106.4622802734375, epoch: 1154\n","nb_steps: 115845, reward: -1084.3819580078125, epoch: 1175\n","nb_steps: 117914, reward: -1015.0209350585938, epoch: 1196\n","nb_steps: 119983, reward: -995.4899291992188, epoch: 1217\n","nb_steps: 122051, reward: -1048.2218017578125, epoch: 1238\n","nb_steps: 124120, reward: -1015.9682006835938, epoch: 1259\n","nb_steps: 126189, reward: -1035.2061767578125, epoch: 1280\n","nb_steps: 128257, reward: -1023.3980712890625, epoch: 1301\n","nb_steps: 130326, reward: -975.0310668945312, epoch: 1322\n","nb_steps: 132395, reward: -991.3078002929688, epoch: 1343\n","nb_steps: 134463, reward: -1085.8636474609375, epoch: 1364\n","nb_steps: 136532, reward: -926.6077270507812, epoch: 1385\n","nb_steps: 138600, reward: -896.3825073242188, epoch: 1406\n","nb_steps: 140669, reward: -981.3709106445312, epoch: 1427\n","nb_steps: 142738, reward: -1046.1575927734375, epoch: 1448\n","nb_steps: 144806, reward: -1018.8617553710938, epoch: 1469\n","nb_steps: 146875, reward: -973.5724487304688, epoch: 1490\n","nb_steps: 148944, reward: -1049.6219482421875, epoch: 1511\n","nb_steps: 151012, reward: -968.4193725585938, epoch: 1532\n","nb_steps: 153081, reward: -978.9818725585938, epoch: 1553\n","nb_steps: 155150, reward: -969.4562377929688, epoch: 1574\n","nb_steps: 157218, reward: -928.1376953125, epoch: 1595\n","nb_steps: 159287, reward: -876.16064453125, epoch: 1616\n","nb_steps: 161356, reward: -947.54638671875, epoch: 1637\n","nb_steps: 163424, reward: -997.2965087890625, epoch: 1658\n","nb_steps: 165493, reward: -928.076171875, epoch: 1679\n","nb_steps: 167562, reward: -1018.7810668945312, epoch: 1700\n","nb_steps: 169630, reward: -951.3834838867188, epoch: 1721\n","nb_steps: 171699, reward: -1021.9359741210938, epoch: 1742\n","nb_steps: 173768, reward: -968.16845703125, epoch: 1763\n","nb_steps: 175836, reward: -1038.7584228515625, epoch: 1784\n","nb_steps: 177905, reward: -935.6276245117188, epoch: 1805\n","nb_steps: 179974, reward: -984.0116577148438, epoch: 1826\n","nb_steps: 182042, reward: -967.9650268554688, epoch: 1847\n","nb_steps: 184111, reward: -879.7427978515625, epoch: 1868\n","nb_steps: 186180, reward: -967.2250366210938, epoch: 1889\n","nb_steps: 188248, reward: -1001.1967163085938, epoch: 1910\n","nb_steps: 190317, reward: -917.77392578125, epoch: 1931\n","nb_steps: 192386, reward: -928.6929931640625, epoch: 1952\n","nb_steps: 194454, reward: -953.3997192382812, epoch: 1973\n","nb_steps: 196523, reward: -922.1638793945312, epoch: 1994\n","nb_steps: 198592, reward: -963.7072143554688, epoch: 2015\n","nb_steps: 200660, reward: -870.0244140625, epoch: 2036\n","nb_steps: 202729, reward: -980.4259643554688, epoch: 2057\n","nb_steps: 204798, reward: -865.38232421875, epoch: 2078\n","nb_steps: 206866, reward: -1035.7371826171875, epoch: 2099\n"]}],"source":["os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_ddpg(config)"]},{"cell_type":"markdown","id":"545ec6fb","metadata":{"id":"paHdoNlz9Lpg"},"source":["## What's next?"]},{"cell_type":"code","execution_count":null,"id":"bb07a5b4","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"9677feb7","metadata":{"id":"F2OIv4em9Lpj"},"source":["Starting from the above version , you sould code [the TD3 algorithm](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf).\n","\n","For that, you need to use two critics (and two target critics) and always take the minimum output between the two when you ask for the Q-value of a (state, action) pair.\n","\n","In more detail, you have to do the following:\n","- replace the single critic and corresponding target critic with two critics and target critics (name them ```critic_1, critic_2, target_critic_1, target_critic_2```)\n","- get the q-values and target q-values corresponding to all these critics.\n","- then the target q-values you should consider to update the critic should be the min over the target q-values at each step (use ```torch.min(...)``` to get this min over a sequence of data).\n","- to update the actor, do it with the q-values of an arbitrarily chosen critic, e.g. critic_1.\n","\n"]},{"cell_type":"code","execution_count":31,"id":"28c751f0","metadata":{},"outputs":[],"source":["# Create the DDPG Agent\n","def create_td3_agent(cfg, train_env_agent, eval_env_agent):\n","    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n","    critic_1 = ContinuousQAgent(\n","        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n","    )\n","    target_critic_1 = copy.deepcopy(critic_1)\n","    critic_2 = ContinuousQAgent(\n","        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n","    )\n","    target_critic_2 = copy.deepcopy(critic_2)\n","    actor = ContinuousDeterministicActor(\n","        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n","    )\n","    # target_actor = copy.deepcopy(actor) # not used in practice, though described in the paper\n","    noise_agent = AddGaussianNoise(cfg.algorithm.action_noise) # alternative : AddOUNoise\n","    tr_agent = Agents(train_env_agent, actor, noise_agent)  \n","    ev_agent = Agents(eval_env_agent, actor)\n","\n","    # Get an agent that is executed on a complete workspace\n","    train_agent = TemporalAgent(tr_agent)\n","    eval_agent = TemporalAgent(ev_agent)\n","    train_agent.seed(cfg.algorithm.seed)\n","    return train_agent, eval_agent, actor, critic_1, critic_2, target_critic_1, target_critic_2  # , target_actor"]},{"cell_type":"code","execution_count":32,"id":"1ee1da87","metadata":{},"outputs":[],"source":["# Configure the optimizers\n","def setup_optimizers_td3(cfg, actor, critic_1, critic_2):\n","    actor_optimizer_args = get_arguments(cfg.actor_optimizer)\n","    parameters = actor.parameters()\n","    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)\n","    critic_optimizer_args = get_arguments(cfg.critic_optimizer)\n","    parameters = critic_1.parameters()\n","    critic_optimizer_1 = get_class(cfg.critic_optimizer)(\n","        parameters, **critic_optimizer_args\n","    )\n","    parameters = critic_2.parameters()\n","    critic_optimizer_2 = get_class(cfg.critic_optimizer)(\n","        parameters, **critic_optimizer_args\n","    )\n","    return actor_optimizer, critic_optimizer_1, critic_optimizer_2"]},{"cell_type":"code","execution_count":33,"id":"8969b005","metadata":{},"outputs":[],"source":["def run_td3(cfg):\n","    # 1)  Build the  logger\n","    logger = Logger(cfg)\n","    best_reward = -10e9\n","\n","    # 2) Create the environment agent\n","    train_env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","    eval_env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.nb_evals,\n","        cfg.algorithm.seed,\n","    )\n","\n","    # 3) Create the DDPG Agent\n","    (\n","        train_agent,\n","        eval_agent,\n","        actor,\n","        critic_1,\n","        critic_2,\n","        target_critic_1,\n","        target_critic_2,\n","    ) = create_td3_agent(cfg, train_env_agent, eval_env_agent)\n","    ag_actor = TemporalAgent(actor)\n","    # ag_target_actor = TemporalAgent(target_actor)\n","    q_agent_1 = TemporalAgent(critic_1)\n","    q_agent_2 = TemporalAgent(critic_2)\n","    target_q_agent_1 = TemporalAgent(target_critic_1)\n","    target_q_agent_2 = TemporalAgent(target_critic_2)\n","\n","    train_workspace = Workspace()\n","    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n","\n","    # Configure the optimizer\n","    actor_optimizer, critic_optimizer_1, critic_optimizer_2 = setup_optimizers_td3(cfg, actor, critic_1, critic_2)\n","    nb_steps = 0\n","    tmp_steps = 0\n","    \n","    update_actor = True\n","\n","    # Training loop\n","    for epoch in range(cfg.algorithm.max_epochs):\n","        # Execute the agent in the workspace\n","        if epoch > 0:\n","            train_workspace.zero_grad()\n","            train_workspace.copy_n_last_steps(1)\n","            train_agent(train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1)\n","        else:\n","            train_agent(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n","\n","        transition_workspace = train_workspace.get_transitions()\n","        action = transition_workspace[\"action\"]\n","        nb_steps += action[0].shape[0]\n","        rb.put(transition_workspace)\n","        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n","\n","        done, truncated, reward, action = rb_workspace[\n","            \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n","        ]\n","        # print(f\"done {done}, reward {reward}, action {action}\")\n","        if nb_steps > cfg.algorithm.learning_starts:\n","            # Determines whether values of the critic should be propagated\n","            # True if the episode reached a time limit or if the task was not done\n","            # See https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing\n","            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n","\n","            # Critic update\n","            # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB\n","            q_agent_1(rb_workspace, t=0, n_steps=1)\n","            q_values_1 = rb_workspace[\"q_value\"]\n","            q_agent_2(rb_workspace, t=0, n_steps=1)\n","            q_values_2 = rb_workspace[\"q_value\"]\n","            # print(f\"q_values ante : {q_values}\")\n","\n","            with torch.no_grad():\n","                # replace the action at t+1 in the RB with \\pi(s_{t+1}), to compute Q(s_{t+1}, \\pi(s_{t+1}) below\n","                ag_actor(rb_workspace, t=1, n_steps=1)\n","                # compute q_values: at t+1 we have Q(s_{t+1}, \\pi(s_{t+1})\n","                target_q_agent_1(rb_workspace, t=1, n_steps=1)\n","                \n","                # q_agent(rb_workspace, t=1, n_steps=1)\n","            # finally q_values contains the above collection at t=0 and t=1\n","            post_q_values_1 = rb_workspace[\"q_value\"]\n","            \n","            with torch.no_grad():\n","                # compute q_values: at t+1 we have Q(s_{t+1}, \\pi(s_{t+1})\n","                target_q_agent_2(rb_workspace, t=1, n_steps=1)\n","                \n","                # q_agent(rb_workspace, t=1, n_steps=1)\n","            # finally q_values contains the above collection at t=0 and t=1\n","            post_q_values_2 = rb_workspace[\"q_value\"]\n","            # print(f\"q_values post : {post_q_values[1]}\")\n","\n","\n","            with torch.no_grad():\n","                post_q_values = torch.cat((post_q_values_1, post_q_values_2), dim = -1)\n","                post_q_values = post_q_values.min(dim = -1, keepdim = True)[0]\n","            \n","            # Compute critic loss\n","            critic_loss_1 = compute_critic_loss(\n","                cfg, reward, must_bootstrap, q_values_1[0], post_q_values[1]\n","            )\n","            logger.add_log(\"critic_loss\", critic_loss_1, nb_steps)\n","            critic_optimizer_1.zero_grad()\n","            critic_loss_1.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                critic_1.parameters(), cfg.algorithm.max_grad_norm\n","            )\n","            critic_optimizer_1.step()\n","            \n","            # Compute critic loss 2\n","            critic_loss_2 = compute_critic_loss(\n","                cfg, reward, must_bootstrap, q_values_2[0], post_q_values[1]\n","            )\n","            logger.add_log(\"critic_loss\", critic_loss_2, nb_steps)\n","            critic_optimizer_1.zero_grad()\n","            critic_loss_2.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                critic_2.parameters(), cfg.algorithm.max_grad_norm\n","            )\n","            critic_optimizer_2.step()\n","\n","            if (update_actor): \n","                # Actor update\n","                # Now we determine the actions the current policy would take in the states from the RB\n","                ag_actor(rb_workspace, t=0, n_steps=1)\n","                # We determine the Q values resulting from actions of the current policy\n","                q_agent_1(rb_workspace, t=0, n_steps=1)\n","                # and we back-propagate the corresponding loss to maximize the Q values\n","                q_values = rb_workspace[\"q_value\"]\n","                actor_loss = compute_actor_loss(q_values)\n","                logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n","                # if -25 < actor_loss < 0 and nb_steps > 2e5:\n","                actor_optimizer.zero_grad()\n","                actor_loss.backward()\n","                torch.nn.utils.clip_grad_norm_(\n","                    actor.parameters(), cfg.algorithm.max_grad_norm\n","                )\n","                actor_optimizer.step()\n","                \n","            update_actor = not update_actor \n","            # Soft update of target q function\n","            tau = cfg.algorithm.tau_target\n","            soft_update_params(critic_1, target_critic_1, tau)\n","            soft_update_params(critic_2, target_critic_2, tau)\n","            # soft_update_params(actor, target_actor, tau)\n","\n","        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n","            tmp_steps = nb_steps\n","            eval_workspace = Workspace()  # Used for evaluation\n","            eval_agent(eval_workspace, t=0, stop_variable=\"env/done\")\n","            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n","            mean = rewards.mean()\n","            logger.add_log(\"reward\", mean, nb_steps)\n","            print(f\"nb_steps: {nb_steps}, reward: {mean}, epoch {epoch}\")\n","\n","            if mean > best_reward:\n","                best_reward = mean\n","\n","                if cfg.save_best:\n","                    directory = \"./td3_agent/\"\n","                    if not os.path.exists(directory):\n","                        os.makedirs(directory)\n","                    filename = directory + \"td3_\" + str(mean.item()) + \".agt\"\n","                    eval_agent.save_model(filename)\n","\n","                if cfg.plot_agents:\n","                    plot_policy(\n","                        actor,\n","                        eval_env_agent,\n","                        \"./td3_plots/\",\n","                        cfg.gym_env.env_name,\n","                        best_reward,\n","                        stochastic=False,\n","                    )\n","                    plot_critic(\n","                        q_agent_1.agent,\n","                        eval_env_agent,\n","                        \"./td3_plots/\",\n","                        cfg.gym_env.env_name,\n","                        best_reward,\n","                    )\n"]},{"cell_type":"code","execution_count":34,"id":"e47d6b4c","metadata":{},"outputs":[],"source":["params={\n","  \"save_best\": True,\n","  # Set to true to have an insight on the learned policy\n","  # (but slows down the evaluation a lot!)\n","  \"plot_agents\": False,\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tblogs/td3/td3-\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,    \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 1,\n","    \"max_grad_norm\": 0.5,\n","    \"epsilon\": 0.02,\n","    \"n_envs\": 1,\n","    \"n_steps\": 100,\n","    \"eval_interval\": 2000,\n","    \"nb_evals\": 10,\n","    \"gae\": 0.8,\n","    \"max_epochs\": 2100,\n","    \"discount_factor\": 0.98,\n","    \"buffer_size\": 2e5,\n","    \"batch_size\": 64,\n","    \"tau_target\": 0.05,\n","    \"learning_starts\": 10000,\n","    \"action_noise\": 0.1,\n","    \"architecture\":{\n","        \"actor_hidden_size\": [400, 300],\n","        \"critic_hidden_size\": [400, 300],\n","        },\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_gym_env\",\n","    \"env_name\": \"Pendulum-v1\",\n","  },\n","  \"actor_optimizer\":{\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  },\n","  \"critic_optimizer\":{\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  }\n","}"]},{"cell_type":"code","execution_count":35,"id":"2e1f02ec","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["nb_steps: 2069, reward: -1183.197509765625, epoch 20\n","nb_steps: 4138, reward: -1104.410400390625, epoch 41\n","nb_steps: 6206, reward: -1238.423095703125, epoch 62\n","nb_steps: 8275, reward: -1215.0811767578125, epoch 83\n","nb_steps: 10344, reward: -1659.2777099609375, epoch 104\n","nb_steps: 12412, reward: -1721.273681640625, epoch 125\n","nb_steps: 14481, reward: -1753.0628662109375, epoch 146\n","nb_steps: 16550, reward: -1661.569580078125, epoch 167\n","nb_steps: 18618, reward: -1714.880126953125, epoch 188\n","nb_steps: 20687, reward: -1496.2874755859375, epoch 209\n","nb_steps: 22756, reward: -1526.474853515625, epoch 230\n","nb_steps: 24824, reward: -1583.419189453125, epoch 251\n","nb_steps: 26893, reward: -1571.977783203125, epoch 272\n","nb_steps: 28962, reward: -1595.1905517578125, epoch 293\n","nb_steps: 31030, reward: -1580.5068359375, epoch 314\n","nb_steps: 33099, reward: -1549.844970703125, epoch 335\n","nb_steps: 35168, reward: -1468.183349609375, epoch 356\n","nb_steps: 37236, reward: -1427.9190673828125, epoch 377\n","nb_steps: 39305, reward: -1477.3057861328125, epoch 398\n","nb_steps: 41374, reward: -1387.2940673828125, epoch 419\n","nb_steps: 43442, reward: -1354.3570556640625, epoch 440\n","nb_steps: 45511, reward: -1282.005126953125, epoch 461\n","nb_steps: 47580, reward: -1262.7926025390625, epoch 482\n","nb_steps: 49648, reward: -1334.9273681640625, epoch 503\n","nb_steps: 51717, reward: -1345.3388671875, epoch 524\n","nb_steps: 53786, reward: -1221.6156005859375, epoch 545\n","nb_steps: 55854, reward: -1341.4259033203125, epoch 566\n","nb_steps: 57923, reward: -1193.007080078125, epoch 587\n","nb_steps: 59992, reward: -1284.1695556640625, epoch 608\n","nb_steps: 62060, reward: -1127.408447265625, epoch 629\n","nb_steps: 64129, reward: -1159.1812744140625, epoch 650\n","nb_steps: 66198, reward: -1234.37255859375, epoch 671\n","nb_steps: 68266, reward: -1133.834716796875, epoch 692\n","nb_steps: 70335, reward: -1220.141845703125, epoch 713\n","nb_steps: 72403, reward: -1212.451416015625, epoch 734\n","nb_steps: 74472, reward: -1077.3909912109375, epoch 755\n","nb_steps: 76541, reward: -1183.7294921875, epoch 776\n","nb_steps: 78609, reward: -1160.4573974609375, epoch 797\n","nb_steps: 80678, reward: -1108.8314208984375, epoch 818\n","nb_steps: 82747, reward: -1219.8974609375, epoch 839\n","nb_steps: 84815, reward: -1137.1451416015625, epoch 860\n","nb_steps: 86884, reward: -1016.8054809570312, epoch 881\n","nb_steps: 88953, reward: -1116.5941162109375, epoch 902\n","nb_steps: 91021, reward: -1148.2821044921875, epoch 923\n","nb_steps: 93090, reward: -1164.9266357421875, epoch 944\n","nb_steps: 95159, reward: -1016.4450073242188, epoch 965\n","nb_steps: 97227, reward: -1015.7468872070312, epoch 986\n","nb_steps: 99296, reward: -1172.759033203125, epoch 1007\n","nb_steps: 101365, reward: -990.8660278320312, epoch 1028\n","nb_steps: 103433, reward: -1066.7952880859375, epoch 1049\n","nb_steps: 105502, reward: -1004.5498046875, epoch 1070\n","nb_steps: 107571, reward: -952.671875, epoch 1091\n","nb_steps: 109639, reward: -958.26953125, epoch 1112\n","nb_steps: 111708, reward: -1077.085205078125, epoch 1133\n","nb_steps: 113777, reward: -1100.4195556640625, epoch 1154\n","nb_steps: 115845, reward: -1052.2122802734375, epoch 1175\n","nb_steps: 117914, reward: -1000.0105590820312, epoch 1196\n","nb_steps: 119983, reward: -1035.33349609375, epoch 1217\n","nb_steps: 122051, reward: -1063.3638916015625, epoch 1238\n","nb_steps: 124120, reward: -1080.189208984375, epoch 1259\n","nb_steps: 126189, reward: -1044.2774658203125, epoch 1280\n","nb_steps: 128257, reward: -1037.037109375, epoch 1301\n","nb_steps: 130326, reward: -1048.564697265625, epoch 1322\n","nb_steps: 132395, reward: -1006.8170166015625, epoch 1343\n","nb_steps: 134463, reward: -1061.145751953125, epoch 1364\n","nb_steps: 136532, reward: -1010.296875, epoch 1385\n","nb_steps: 138600, reward: -872.14599609375, epoch 1406\n","nb_steps: 140669, reward: -948.6804809570312, epoch 1427\n","nb_steps: 142738, reward: -1015.64697265625, epoch 1448\n","nb_steps: 144806, reward: -913.19384765625, epoch 1469\n","nb_steps: 146875, reward: -891.9734497070312, epoch 1490\n","nb_steps: 148944, reward: -901.2799072265625, epoch 1511\n","nb_steps: 151012, reward: -948.8331909179688, epoch 1532\n","nb_steps: 153081, reward: -882.4251098632812, epoch 1553\n","nb_steps: 155150, reward: -893.9256591796875, epoch 1574\n","nb_steps: 157218, reward: -954.0076293945312, epoch 1595\n","nb_steps: 159287, reward: -911.0823974609375, epoch 1616\n","nb_steps: 161356, reward: -839.9854736328125, epoch 1637\n","nb_steps: 163424, reward: -925.9494018554688, epoch 1658\n","nb_steps: 165493, reward: -936.2483520507812, epoch 1679\n","nb_steps: 167562, reward: -916.5108642578125, epoch 1700\n","nb_steps: 169630, reward: -862.8072509765625, epoch 1721\n","nb_steps: 171699, reward: -900.46630859375, epoch 1742\n","nb_steps: 173768, reward: -1059.3382568359375, epoch 1763\n","nb_steps: 175836, reward: -869.5611572265625, epoch 1784\n","nb_steps: 177905, reward: -952.8703002929688, epoch 1805\n","nb_steps: 179974, reward: -1063.9061279296875, epoch 1826\n","nb_steps: 182042, reward: -1023.2496337890625, epoch 1847\n","nb_steps: 184111, reward: -952.5279541015625, epoch 1868\n","nb_steps: 186180, reward: -962.4932861328125, epoch 1889\n","nb_steps: 188248, reward: -1038.32275390625, epoch 1910\n","nb_steps: 190317, reward: -1010.58837890625, epoch 1931\n","nb_steps: 192386, reward: -923.7802734375, epoch 1952\n","nb_steps: 194454, reward: -1087.4952392578125, epoch 1973\n","nb_steps: 196523, reward: -1073.490966796875, epoch 1994\n","nb_steps: 198592, reward: -1061.2452392578125, epoch 2015\n","nb_steps: 200660, reward: -921.2017822265625, epoch 2036\n","nb_steps: 202729, reward: -948.3477783203125, epoch 2057\n","nb_steps: 204798, reward: -922.8297119140625, epoch 2078\n","nb_steps: 206866, reward: -1008.5050659179688, epoch 2099\n"]}],"source":["config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_td3(config)"]},{"cell_type":"markdown","id":"ff60fbd7","metadata":{"id":"jkp3YCxcLlfh"},"source":["## Experimental comparison"]},{"cell_type":"markdown","id":"a9b3d416","metadata":{"id":"dD9eCH3w-hdc"},"source":["Take an environment where the over-estimation bias may matter, and compare the performance of DDPG and TD3. Visualize the Q-value long before convergence to see whether indeed DDPG overestimates the Q-values with respect to TD3."]}],"metadata":{"jupytext":{"formats":"ipynb,Rmd"},"kernelspec":{"display_name":"Python 3.10.6 ('deepdac')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"b8381a9192b883635ac16797c97396a9adba45cea8be9b52669f729c8ec391d7"}}},"nbformat":4,"nbformat_minor":5}
