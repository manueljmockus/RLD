{"cells":[{"cell_type":"markdown","id":"dbfb3b22","metadata":{"id":"t5Y434EKaEji"},"source":["# Outlook #"]},{"cell_type":"markdown","id":"de4e510d","metadata":{"id":"7t49ETcZaJg3"},"source":["In this colab we will study basic reinforcement learning algorithms: TD learning, q-learning and sarsa. We will also investigate two basic exploration strategies: $\\epsilon$-greedy and softmax."]},{"cell_type":"markdown","id":"2cd7d5f8","metadata":{"id":"pfP_irDZNa0J"},"source":["# Installation #"]},{"cell_type":"code","execution_count":9,"id":"fa8f4046","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qRaL0jp5zmp","outputId":"fb744b04-cf6d-4e5e-ded7-0902ee61f03a"},"outputs":[],"source":["try:\n","    import my_gym\n","except ModuleNotFoundError as e:\n","    !pip install git+https://github.com/osigaud/my_gym"]},{"cell_type":"code","execution_count":10,"id":"148e45ed","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APW9lTEe59ah","outputId":"67f642ea-ea3b-4181-9bb1-c77f257b115d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7fcb7f17cd70427ab5c49b3d6d73d7a4","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","from typing import Tuple, List\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","\n","from mazemdp.toolbox import egreedy, egreedy_loc, softmax, discreteProb\n","from mazemdp.maze_plotter import show_videos\n","from mazemdp.mdp import Mdp\n","from my_gym.envs.maze_mdp import MazeMDPEnv\n","\n","# For visualization\n","os.environ[\"VIDEO_FPS\"] = \"5\"\n","if not os.path.isdir(\"./videos\"):\n","    os.mkdir(\"./videos\")\n","\n","from IPython.display import Video\n","\n","# Settings\n","NB_EPISODES = 50\n","TIMEOUT = 25\n","\n","import gym\n","import my_gym\n","\n","\n","env = gym.make(\"MazeMDP-v0\", kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2})\n","env.reset()\n","\n","# in dynamic programming, there is no agent moving in the environment\n","env.init_draw(\"The maze\")"]},{"cell_type":"markdown","id":"1fc5e88a","metadata":{"id":"ytERHME1NfYr"},"source":["# Reinforcement Learning #"]},{"cell_type":"markdown","id":"baa6b4d5","metadata":{"id":"ZyT1upohNjL1"},"source":["\n","Reinforcement Learning is about finding the optimal policy in an MDP which is initially unknown to the agent. More precisely, the state and action spaces are known, but the agent does not know the transition and reward functions. Generally speaking, the agent has to explore the MDP to figure out which action in which state leads to which other state and reward. The model-free case is about finding this optimal policy just through very local updates, without storing any information about previous interactions with the environment. Principles of these local updates can already be found in the Temporal Difference (TD) algorithm, which iteratively computes optimal values for all state using local updates.\n","The most widely used model-free RL algorithms are **q-learning**, **sarsa** and **actor-critic** algorithms. Below we focus on the first two."]},{"cell_type":"markdown","id":"dc410ab7","metadata":{"id":"MzBuWYhCtQmF"},"source":["As for dynamic programming, we first create a maze-like MDP. Reinforcement learning is slower than dynamic programming, so we will work with smaller mazes."]},{"cell_type":"code","execution_count":11,"id":"df8cff57","metadata":{"id":"srNM1JqTtYAc"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63bc304ba006476fb48cacd540af75e6","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["import gym\n","import my_gym\n","\n","\n","env = gym.make(\"MazeMDP-v0\", kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2})\n","env.reset()\n","\n","# in dynamic programming, there is no agent moving in the environment\n","env.init_draw(\"The maze\")"]},{"cell_type":"markdown","id":"4f2555af","metadata":{"id":"fSbk6GyAN0-e"},"source":["## Temporal Difference (TD) learning ##"]},{"cell_type":"markdown","id":"1f2ed7a1","metadata":{"id":"JNnNJpTON5aU"},"source":["\n","Given a state and an action spaces as well as a policy, TD(0) computes the state value of this policy based on the following equations:\n","$$\\delta_t = r(s_t,a_t) + \\gamma V^{(t)}(s_{t+1})-V^{(t)}(s_t)$$\n","$$V^{(t+1)}(s_t) = V^{(t)}(s_t) + \\alpha\\delta_t$$\n","\n","where $\\delta$ is the TD error and $\\alpha$ is a parameter called \"learning rate\".\n","\n","The code is provided below, so that you can take inspiration later on. The important part is the computation of $\\delta$, and the update of the values of $V$.\n","\n","To run TD learning, a policy is needed as input. Such a policy can be retreived by using the `policy_iteration_q(mdp)` function defined in the dynamic programming notebook.\n","\n","If you want to run this notebook independently, you can use instead the `random_policy` provided in `mazemdp`. This is what we do here by default, replace it if you want to run TD learning from an optimal policy.\n"]},{"cell_type":"code","execution_count":12,"id":"f1049646","metadata":{"id":"xniYyOwU8C3b"},"outputs":[],"source":["from mazemdp import random_policy"]},{"cell_type":"markdown","id":"39c40197","metadata":{"id":"f_YBjgxOPCn8"},"source":["**Question:** In the code of the *temporal_difference(...)* function below, fill the missing parts with # some code = ... "]},{"cell_type":"code","execution_count":13,"id":"d35eec4c","metadata":{},"outputs":[],"source":["def get_policy_from_v(mdp: MazeMDPEnv, v: np.ndarray) -> np.ndarray:\n","    # Outputs a policy given the state values\n","    policy = np.zeros(mdp.nb_states)  # initial state values are set to 0\n","    for x in range(mdp.nb_states):  # for each state x\n","        if x not in mdp.terminal_states:\n","            # Compute the value of the state x for each action u of the MDP action space\n","            v_temp = []\n","            for u in mdp.action_space.actions:\n","                # Process sum of the values of the neighbouring states\n","                summ = 0\n","                for y in range(mdp.nb_states):\n","                    summ = summ + mdp.P[x, u, y] * v[y]\n","                v_temp.append(mdp.r[x, u] + mdp.gamma * summ)\n","            policy[x] = np.argmax(v_temp)\n","    return policy"]},{"cell_type":"code","execution_count":14,"id":"c31c1cd2","metadata":{"id":"0lL1I8Jx6Bzf"},"outputs":[],"source":["def temporal_difference(\n","    mdp: MazeMDPEnv,\n","    policy: np.ndarray,\n","    nb_episodes: int = 50,\n","    alpha: float = 0.2,\n","    timeout: int = 25,\n","    render: bool = True,\n",") -> np.ndarray:\n","    # alpha: learning rate\n","    # timeout: timeout of an episode (maximum number of timesteps)\n","    v = np.zeros(mdp.nb_states)  # initial state value v\n","    mdp.timeout = timeout\n","\n","    if render:\n","        mdp.init_draw(\"Temporal differences\")\n","\n","    for _ in tqdm(range(nb_episodes)):  # for each episode\n","\n","        # Draw an initial state randomly (if uniform is set to False, the state is drawn according to the P0\n","        #                                 distribution)\n","        x = mdp.reset(uniform=True)\n","        done = False\n","        while not done:  # update episode at each timestep\n","            # Show agent\n","            if render:\n","                mdp.draw_v_pi(v, policy)\n","\n","            # Step forward following the MDP: x=current state,\n","            #                                 pol[i]=agent's action according to policy pol,\n","            #                                 r=reward gained after taking action pol[i],\n","            #                                 done=tells whether the episode ended,\n","            #                                 and info gives some info about the process\n","            [y, r, done, _] = mdp.step(egreedy_loc(policy[x], mdp.action_space.size, epsilon=0.2))\n","\n","            delta = r + mdp.gamma * v[y] - v[x]\n","            v[x] = v[x] + alpha * delta\n","            \n","            # Update agent's position (state)\n","            x = y\n","\n","    if render:\n","        # Show the final policy\n","        mdp.current_state = 0\n","        mdp.draw_v_pi(v, policy, title=\"Temporal Differences\")\n","    return v"]},{"cell_type":"markdown","id":"f5bd79b3","metadata":{"id":"8f0swnV9tlym"},"source":["Once this is done, you can run it."]},{"cell_type":"code","execution_count":8,"id":"d4964cc3","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385,"referenced_widgets":["0aa56bbb0dda43eaa971a9f1197e16e9","901560d738114e2ba21980b76c83a6ca","2ac8aacfe24b4249bc83388825f38746","9bd55dc68c9b43c1833e64a9104668ae","b2fae5467dc5453d8bdc6311d04d70fd","aefc0cd1907646bca7085321328c6c84","0019181536f344c7b2d854e1434c1fe0","b73b20cd1eb74a84972a78ad335af72d","648e8cd2a37542f690a285ed9c8f5704","e60235ebaef143e2b1c9b040bd334123","345e57fffe5c423ebabab02ef7eb53c0"]},"id":"R56l4AT3to_F","outputId":"d76ca926-48dc-4aa2-b17d-f298157d5dd5"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d7c2b96d0bd43658572b76cf8cb2a8b","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"167ca3a0fe254854aa1c9223cbc5eca7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["env = gym.make(\"MazeMDP-v0\", kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2})\n","env.reset()\n","policy = random_policy(env)\n","v = temporal_difference(env, policy, nb_episodes=NB_EPISODES, timeout=TIMEOUT)"]},{"cell_type":"markdown","id":"aa5d9a51","metadata":{},"source":["Unless you were lucky, the generated value function is boring: if the policy does not reach the final state, all values are 0. To avoid this, you can copy-paste a dynamic programming function from the previous notebook, use it to get an optimal policy, and use this policy for TD learning. You should get a much more interesting value function."]},{"cell_type":"code","execution_count":15,"id":"66f2ae00","metadata":{},"outputs":[],"source":["# Your code to obtain an optimal policy here\n","def get_policy_from_q(q: np.ndarray) -> np.ndarray:\n","    policy = np.zeros(len(q), dtype = np.int64)\n","    for i, x in enumerate(q):\n","        policy[i] = np.argmax(x)\n","    return policy\n","\n","def evaluate_one_step_q(mdp: MazeMDPEnv, q: np.ndarray, policy: np.ndarray) -> np.ndarray:\n","    # Outputs the state value function after one step of policy evaluation\n","    qnew = np.zeros((mdp.nb_states, mdp.action_space.size))  # initial action values are set to 0\n","    q_updates = 0\n","    for x in range(mdp.nb_states):  # for each state x\n","        # Compute the value of the state x for each action u of the MDP action space\n","        if x not in mdp.terminal_states:\n","            for u in mdp.action_space.actions:\n","                # Process sum of the values of the neighbouring states\n","                summ = 0\n","                for y in range(mdp.nb_states):\n","                    \n","                    summ += mdp.P[x, u, y] * q[y, policy[y]]\n","                    \n","                qnew[x, u] = mdp.r[x, u] + mdp.gamma * summ\n","                q_updates += 1\n","    return qnew, q_updates\n","def evaluate_q(mdp: MazeMDPEnv, policy: np.ndarray) -> np.ndarray:\n","    # Outputs the state value function of a policy\n","    q = np.zeros((mdp.nb_states, mdp.action_space.size))  # initial action values are set to 0\n","    stop = False\n","    q_updates = 0\n","    while not stop:\n","        qold = q.copy()\n","        q, q_step_updates = evaluate_one_step_q(mdp, q, policy)\n","        q_updates += q_step_updates\n","        # Test if convergence has been reached\n","        if (np.linalg.norm(q - qold)) < 0.01:\n","            stop = True\n","    return q, q_updates\n","# ---------------- Policy Iteration with the Q function -----------------#\n","# Given a MDP, this algorithm simultaneously computes \n","# the optimal action value function Q and the optimal policy\n","\n","def policy_iteration_q(mdp: MazeMDPEnv, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n","    \"\"\"policy iteration over the q function.\"\"\"\n","    q = np.zeros((mdp.nb_states, mdp.action_space.size))  # initial action values are set to 0\n","    q_list = []\n","    policy = random_policy(mdp)\n","\n","    stop = False\n","    q_total_updates = 0\n","    iterations = 0\n","\n","    if render:\n","        mdp.init_draw(\"Policy iteration Q\")\n","\n","    while not stop:\n","        qold = q.copy()\n","\n","        if render:\n","            mdp.draw_v(q, title=\"Policy iteration Q\")\n","\n","        # Step 1 : Policy evaluation\n","        q, q_updates = evaluate_q(mdp, policy)\n","        q_total_updates += q_updates\n","        # Step 2 : Policy improvement\n","        policy = get_policy_from_q(q)\n","        # Check convergence\n","        if (np.linalg.norm(q - qold)) <= 0.01:\n","            stop = True\n","        q_list.append(np.linalg.norm(q))\n","        iterations += 1\n","\n","    if render:\n","        mdp.draw_v_pi(q, get_policy_from_q(q), title=\"Policy iteration Q\")\n","    return q, q_list, q_total_updates, iterations\n","\n","def get_optimal_policy(env):\n","    q, _, _, _ = policy_iteration_q(env, render= False)\n","    return get_policy_from_q(q)"]},{"cell_type":"code","execution_count":16,"id":"30c914df","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"045a038ac5004b2686462532ed7c4b79","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b0f57e4c878429db68375b77e78a789","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["opt_policy  = get_optimal_policy(env)\n","v = temporal_difference(env, opt_policy, nb_episodes=NB_EPISODES, timeout=TIMEOUT)"]},{"cell_type":"markdown","id":"6e598288","metadata":{"id":"wtog6PFsPhCn"},"source":["## Q-learning ##"]},{"cell_type":"markdown","id":"8293f18c","metadata":{"id":"jP5cC07-Pjxl"},"source":["\n","The **q-learning** algorithm accounts for an agent exploring an MDP and updating at each step a model of the state action-value function stored into a Q-table. It is updated as follows:\n","\n","$$\\delta_t = r(s_t,a_t) + \\gamma \\max_{a \\in A} Q^{(t)}(s_{t+1},a)-Q^{(t)}(s_t,a_t)$$\n","\n","$$Q^{(t+1)}(s_t,a_t) = Q^{(t)}(s_t,a_t) + \\alpha \\delta_t.$$\n"]},{"cell_type":"markdown","id":"bacb7916","metadata":{"id":"2gVzVZCLxPh8"},"source":["To visualize the policy, we need the `get_policy_from_q(q)` function that we defined in the dynamic programming notebook. Import it below."]},{"cell_type":"code","execution_count":17,"id":"a21cb9dc","metadata":{"id":"tUlSmESgxhXQ"},"outputs":[],"source":["def get_policy_from_q(q: np.ndarray) -> np.ndarray:\n","    # Outputs a policy given the action values\n","    return np.argmax(q, axis=1)"]},{"cell_type":"markdown","id":"c9f05a39","metadata":{"id":"7pq3cgWNPu_t"},"source":["**Question:**  Fill the code of the `q_learning(...)` function below."]},{"cell_type":"code","execution_count":18,"id":"96048a42","metadata":{"id":"LhM1c2Ct6dKb"},"outputs":[],"source":["# --------------------------- Q-Learning epsilon-greedy version -------------------------------#\n","\n","# Given an exploration rate epsilon, the QLearning algorithm computes the state action-value function\n","# based on an epsilon-greedy policy\n","# alpha is the learning rate\n","\n","\n","def q_learning_eps(\n","    mdp: MazeMDPEnv,\n","    epsilon: float,\n","    nb_episodes: int = 20,\n","    timeout: int = 50,\n","    alpha: float = 0.5,\n","    render: bool = True,\n","    q_init: float = 0,\n",") -> Tuple[np.ndarray, List[float]]:\n","    # Initialize the state-action value function\n","    # alpha is the learning rate\n","    q = np.full((mdp.nb_states, mdp.action_space.size), q_init)\n","    q_min = np.zeros((mdp.nb_states, mdp.action_space.size))\n","    q_list = []\n","\n","    # Run learning cycle\n","    mdp.timeout = timeout  # episode length\n","\n","    if render:\n","        mdp.init_draw(\"Q-learning e-greedy\")\n","\n","    for _ in tqdm(range(nb_episodes)):\n","        # Draw the first state of episode i using a uniform distribution over all the states\n","        x = mdp.reset(uniform=True)\n","        done = False\n","        while not done:\n","            if render:\n","                # Show the agent in the maze\n","                mdp.draw_v_pi(q, q.argmax(axis=1))\n","\n","            # Draw an action using an epsilon-greedy policy\n","            u = egreedy(q, x, epsilon)\n","\n","            # Perform a step of the MDP\n","            \n","            [y, r, done, _] = mdp.step(u)\n","\n","            delta = r + mdp.gamma * np.max(q[y, :]) - q[x,u]\n","            q[x, u] = q[x, u] + alpha * delta\n","\n","            # Update the agent position\n","            x = y\n","        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n","\n","    if render:\n","        # Show the final policy\n","        mdp.current_state = 0\n","        mdp.draw_v_pi(q, get_policy_from_q(q), title=\"Q-learning e-greedy\")\n","    return q, q_list"]},{"cell_type":"markdown","id":"e4c7be8d","metadata":{"id":"ME7SAuggvNfw"},"source":["And run it."]},{"cell_type":"code","execution_count":19,"id":"87860377","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385,"referenced_widgets":["facc58b300be49fcb53979ce31426f28","cf1a8f7e531a48639ab88a7decf79184","423ada19ca3d4a7c86b93d514b518a18","5790a7812de44421b001fe7061bfe92f","150faf6393ff414590a2bb57fa775b9e","6c16381711184e8c8d4fc9c66c186bfb","4d3fff65773b4b9db61cb0c45d04ac71","b8eab22182fe45e884cf4e3fc7074ea1","043278f92b2a4a61ac0f24cb38fa0f13","79b0046b3c2544349badc0c574d06e02","ea2349a4a28b41f79762edfcc0f1e80e"]},"id":"UBpkpkuWvP8i","outputId":"87aa1056-ffd9-4de1-909f-9322308be7e4"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4633ac738620402b8813da5c5a6d5473","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f4a4ce0778a467ab07ce04e1a2b4b3a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["epsilon = 0.2\n","q, q_list = q_learning_eps(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT, q_init=0.1)"]},{"cell_type":"markdown","id":"a9d012bd","metadata":{"id":"MnLbmQsbQEIO"},"source":["### Learning dynamics"]},{"cell_type":"markdown","id":"01430785","metadata":{"id":"M_6fmrxlQHR3"},"source":["\n","By watching carefully the values while the agent is learning, you can see that the agent favors certains paths over others which have a strictly equivalent value. This can be explained easily: as the agent chooses a path for the first time, it updates the values along that path, these values get higher than the surrounding values, and the agent chooses the same path again and again, increasing the phenomenon. Only steps of random exploration can counterbalance this effect, but they do so extremely slowly."]},{"cell_type":"markdown","id":"e55291e7","metadata":{"id":"JsKOa2i1QMcl"},"source":["### Exploration ###"]},{"cell_type":"markdown","id":"96d11d77","metadata":{"id":"PXkEV9-cQQnP"},"source":["\n","In the `q_learning(...)` function above, action selection is based on a `softmax` policy. Instead, it could have relied on *$\\epsilon$-greedy*.\n"]},{"cell_type":"markdown","id":"665ae996","metadata":{"id":"whX-_jP4QsrU"},"source":["**Question:** In the function below, you have to replace the call to the previous *$\\epsilon$-greedy* policy with a `softmax` policy. The `softmax(...)` and `egreedy(...)` functions are available in `mazemdp.toolbox`."]},{"cell_type":"code","execution_count":22,"id":"90441ca3","metadata":{"id":"iu4WiXRm6mdL"},"outputs":[],"source":["# --------------------------- Q-Learning softmax version ----------------------------#\n","\n","# Given a temperature \"tau\", the QLearning algorithm computes the state action-value function\n","# based on a softmax policy\n","# alpha is the learning rate\n","\n","\n","def q_learning_soft(\n","    mdp: MazeMDPEnv,\n","    tau: float,\n","    nb_episodes: int = 20,\n","    timeout: int = 50,\n","    alpha: float = 0.5,\n","    render: bool = True,\n","    q_init: float = 0,\n",") -> Tuple[np.ndarray, List[float]]:\n","    # Initialize the state-action value function\n","    # alpha is the learning rate\n","    q = np.full((mdp.nb_states, mdp.action_space.size),q_init)\n","    q_min = np.zeros((mdp.nb_states, mdp.action_space.size))\n","    q_list = []\n","\n","    # Run learning cycle\n","    mdp.timeout = timeout  # episode length\n","\n","    if render:\n","        mdp.init_draw(\"Q-learning softmax\")\n","\n","    for _ in tqdm(range(nb_episodes)):\n","        # Draw the first state of episode i using a uniform distribution over all the states\n","        x = mdp.reset(uniform=True)\n","        done = False\n","        while not done:\n","            if render:\n","                # Show the agent in the maze\n","                mdp.draw_v_pi(q, q.argmax(axis=1))\n","\n","            # À compléter... \n","            distribution = softmax(q, x, tau)\n","            \n","            u = np.random.choice(mdp.action_space.size, p = distribution)\n","            # Perform a step of the MDP\n","            [y, r, done, _] = mdp.step(u)\n","\n","            delta = r + mdp.gamma * np.max(q[y, :]) - q[x,u]\n","            q[x, u] = q[x, u] + alpha * delta\n","\n","            # Update the agent position\n","            x = y\n","            \n","        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n","\n","    if render:\n","        # Show the final policy\n","        mdp.current_state = 0\n","        mdp.draw_v_pi(q, get_policy_from_q(q), title=\"Q-learning softmax\")\n","    return q, q_list\n"]},{"cell_type":"markdown","id":"932a25e7","metadata":{"id":"Xevh4vMxvm-q"},"source":["Run this new version"]},{"cell_type":"code","execution_count":21,"id":"4441b168","metadata":{"id":"FcAwK52avpur"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71c6f8a257c34ed1992a56e53a7effd1","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8455943c46ab461186c4c5996d4c7805","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.02\u001b[39m\n\u001b[1;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 3\u001b[0m q, q_list \u001b[38;5;241m=\u001b[39m \u001b[43mq_learning_soft\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNB_EPISODES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [20], line 36\u001b[0m, in \u001b[0;36mq_learning_soft\u001b[0;34m(mdp, tau, nb_episodes, timeout, alpha, render, q_init)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# Show the agent in the maze\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m         \u001b[43mmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_v_pi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# À compléter... \u001b[39;00m\n\u001b[1;32m     39\u001b[0m     distribution \u001b[38;5;241m=\u001b[39m softmax(q, x, tau)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl_gym/envs/maze_mdp.py:87\u001b[0m, in \u001b[0;36mMazeMDPEnv.draw_v_pi\u001b[0;34m(self, v, policy, title, mode)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_v_pi\u001b[39m(\u001b[39mself\u001b[39m, v, policy, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMDP studies\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlegacy\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     86\u001b[0m     agent_pos \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmdp\u001b[39m.\u001b[39;49mrender(v, policy, agent_pos, title, mode\u001b[39m=\u001b[39;49mmode)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/mazemdp/mdp.py:128\u001b[0m, in \u001b[0;36mMdp.render\u001b[0;34m(self, v, policy, agent_pos, title, mode)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplotter\u001b[39m.\u001b[39mrender(v\u001b[39m=\u001b[39mv, agent_state\u001b[39m=\u001b[39magent_pos, title\u001b[39m=\u001b[39mtitle, mode\u001b[39m=\u001b[39mmode)\n\u001b[1;32m    127\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplotter\u001b[39m.\u001b[39;49mrender(\n\u001b[1;32m    129\u001b[0m         v\u001b[39m=\u001b[39;49mv, agent_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_state, policy\u001b[39m=\u001b[39;49mpolicy, title\u001b[39m=\u001b[39;49mtitle, mode\u001b[39m=\u001b[39;49mmode\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplotter\u001b[39m.\u001b[39mrender(v\u001b[39m=\u001b[39mv, title\u001b[39m=\u001b[39mtitle, mode\u001b[39m=\u001b[39mmode)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/mazemdp/maze_plotter.py:294\u001b[0m, in \u001b[0;36mMazePlotter.render\u001b[0;34m(self, v, policy, agent_state, stochastic, title, mode)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_stochastic_policy(plot, v, policy, i, j, state)\n\u001b[1;32m    293\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_policy(plot, policy, i, j, state)\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m agent_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze_attr\u001b[39m.\u001b[39mstate_width) \u001b[39m>\u001b[39m agent_state:\n\u001b[1;32m    297\u001b[0m     x, y \u001b[39m=\u001b[39m coords(\n\u001b[1;32m    298\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze_attr\u001b[39m.\u001b[39mwidth,\n\u001b[1;32m    299\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze_attr\u001b[39m.\u001b[39mheight,\n\u001b[1;32m    300\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze_attr\u001b[39m.\u001b[39mstate_width[agent_state],\n\u001b[1;32m    301\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze_attr\u001b[39m.\u001b[39mstate_height[agent_state],\n\u001b[1;32m    302\u001b[0m     )\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/mazemdp/maze_plotter.py:414\u001b[0m, in \u001b[0;36mMazePlotter.render_policy\u001b[0;34m(self, plot, policy, i, j, state)\u001b[0m\n\u001b[1;32m    412\u001b[0m arw_color \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39m0.6\u001b[39m\n\u001b[0;32m--> 414\u001b[0m plot\u001b[39m.\u001b[39;49maxes\u001b[39m.\u001b[39;49marrow(\n\u001b[1;32m    415\u001b[0m     x0,\n\u001b[1;32m    416\u001b[0m     y0,\n\u001b[1;32m    417\u001b[0m     x,\n\u001b[1;32m    418\u001b[0m     y,\n\u001b[1;32m    419\u001b[0m     alpha\u001b[39m=\u001b[39;49malpha,\n\u001b[1;32m    420\u001b[0m     head_width\u001b[39m=\u001b[39;49m\u001b[39m0.12\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaze_attr\u001b[39m.\u001b[39;49mwidth,\n\u001b[1;32m    421\u001b[0m     head_length\u001b[39m=\u001b[39;49m\u001b[39m0.12\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaze_attr\u001b[39m.\u001b[39;49mheight,\n\u001b[1;32m    422\u001b[0m     fc\u001b[39m=\u001b[39;49marw_color,\n\u001b[1;32m    423\u001b[0m     ec\u001b[39m=\u001b[39;49marw_color,\n\u001b[1;32m    424\u001b[0m )\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5046\u001b[0m, in \u001b[0;36mAxes.arrow\u001b[0;34m(self, x, y, dx, dy, **kwargs)\u001b[0m\n\u001b[1;32m   5043\u001b[0m dy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_yunits(dy)\n\u001b[1;32m   5045\u001b[0m a \u001b[39m=\u001b[39m mpatches\u001b[39m.\u001b[39mFancyArrow(x, y, dx, dy, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5046\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_patch(a)\n\u001b[1;32m   5047\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request_autoscale_view()\n\u001b[1;32m   5048\u001b[0m \u001b[39mreturn\u001b[39;00m a\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/axes/_base.py:2414\u001b[0m, in \u001b[0;36m_AxesBase.add_patch\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m   2412\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2413\u001b[0m     p\u001b[39m.\u001b[39mset_clip_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch)\n\u001b[0;32m-> 2414\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_patch_limits(p)\n\u001b[1;32m   2415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children\u001b[39m.\u001b[39mappend(p)\n\u001b[1;32m   2416\u001b[0m p\u001b[39m.\u001b[39m_remove_method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children\u001b[39m.\u001b[39mremove\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/axes/_base.py:2438\u001b[0m, in \u001b[0;36m_AxesBase._update_patch_limits\u001b[0;34m(self, patch)\u001b[0m\n\u001b[1;32m   2435\u001b[0m vertices \u001b[39m=\u001b[39m []\n\u001b[1;32m   2436\u001b[0m \u001b[39mfor\u001b[39;00m curve, code \u001b[39min\u001b[39;00m p\u001b[39m.\u001b[39miter_bezier():\n\u001b[1;32m   2437\u001b[0m     \u001b[39m# Get distance along the curve of any extrema\u001b[39;00m\n\u001b[0;32m-> 2438\u001b[0m     _, dzeros \u001b[39m=\u001b[39m curve\u001b[39m.\u001b[39;49maxis_aligned_extrema()\n\u001b[1;32m   2439\u001b[0m     \u001b[39m# Calculate vertices of start, end and any extrema in between\u001b[39;00m\n\u001b[1;32m   2440\u001b[0m     vertices\u001b[39m.\u001b[39mappend(curve([\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39mdzeros, \u001b[39m1\u001b[39m]))\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/bezier.py:275\u001b[0m, in \u001b[0;36mBezierSegment.axis_aligned_extrema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m     prefactor \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(i \u001b[39m+\u001b[39m j) \u001b[39m*\u001b[39m _comb(j, i)  \u001b[39m# j on axis 0, i on axis 1\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m _comb(n, j) \u001b[39m*\u001b[39m prefactor \u001b[39m@\u001b[39m P  \u001b[39m# j on axis 0, self.dimension on 1\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maxis_aligned_extrema\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    276\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    Return the dimension and location of the curve's interior extrema.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m        0`\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegree\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["epsilon = 0.02\n","env.reset()\n","q, q_list = q_learning_soft(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT, q_init= 0.1)"]},{"cell_type":"code","execution_count":23,"id":"a5dd4bcb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.0863355835099946, 1.0642603945933535, 1.0442440707875722, 1.2206545249765404, 1.3860474944408303, 1.5355256590845465, 1.6943545527687767, 1.8094361341750365, 1.9855605698699341, 2.0711957795323848, 2.1520691216855297, 2.2325832825821212, 2.3018694049056445, 2.3027823985911686, 2.395322368649044, 2.4458161476866973, 2.4691804064462493, 2.5234832795469417, 2.608690280461195, 2.6141783055827124, 2.614184604080833, 2.656633827551302, 2.730975984375623, 2.8723039834451862, 2.889275880325905, 2.970879431864159, 3.0214704957738996, 3.0258755849151457, 3.027251622693212, 3.064938107884913, 3.1069562234312555, 3.1949646198097277, 3.2701817033085288, 3.2701923969233833, 3.30770630949801, 3.329558001294234, 3.3825637812142513, 3.422541615692928, 3.484545765735051, 3.510909396009014, 3.549354585212576, 3.5725401688552787, 3.5967083057711724, 3.6111814615985525, 3.63158348333086, 3.680675844962656, 3.774214021368407, 3.80222334545904, 3.8265318997045616, 3.850495661681696]\n"]}],"source":["print(q_list)"]},{"cell_type":"markdown","id":"dd39290c","metadata":{"id":"PW4wU1xjRT8J"},"source":["## Sarsa ##"]},{"cell_type":"markdown","id":"1ba88c67","metadata":{"id":"bxDs-SywRWS6"},"source":["\n","The **sarsa** algorithm is very similar to **q-learning**. At first glance, the only difference is in the update rule. However, to perform the update in **sarsa**, one needs to know the action the agent will take when it will be at the next state, even if the agent is taking a random action.\n","\n","This implies that the next state action is determined in advance and stored for being played at the next time step.\n"]},{"cell_type":"markdown","id":"dabdb017","metadata":{"id":"CUdQEF0Rv2uc"},"source":["**Question:** Fill the code below"]},{"cell_type":"code","execution_count":25,"id":"a07ee66c","metadata":{"id":"93VUGbwM6wkq"},"outputs":[],"source":["# --------------------------- Sarsa, epsilon-greedy version -------------------------------#\n","\n","# Given an exploration rate epsilon, the SARSA algorithm computes the state action-value function\n","# based on an epsilon-greedy policy\n","# alpha is the learning rate\n","def sarsa_eps(\n","    mdp: MazeMDPEnv,\n","    epsilon: float,\n","    nb_episodes: int = 20,\n","    timeout: int = 50,\n","    alpha: float = 0.5,\n","    render: bool = True,\n","    q_init: float = 0,\n",") -> Tuple[np.ndarray, List[float]]:\n","    # Initialize the state-action value function\n","    # alpha is the learning rate\n","    q = np.full((mdp.nb_states, mdp.action_space.size), q_init)\n","    q_min = np.zeros((mdp.nb_states, mdp.action_space.size))\n","    q_list = []\n","         \n","    \n","    \n","\n","    # Run learning cycle\n","    mdp.timeout = timeout  # episode length\n","\n","    if render:\n","        mdp.init_draw(\"Sarsa e-greedy\")\n","\n","    for _ in tqdm(range(nb_episodes)):\n","        # Draw the first state of episode i using a uniform distribution over all the states\n","        x = mdp.reset(uniform=True)\n","        done = False            # À compléter... \n","        u_t = egreedy(q, x, epsilon)\n","        \n","        while not done:\n","            if render:\n","                # Show the agent in the maze\n","                mdp.draw_v_pi(q, q.argmax(axis=1))\n","                \n","            [y, r, done, _] = mdp.step(u_t)\n","            u_t1 = egreedy(q, y, epsilon)\n","            \n","            delta = r + mdp.gamma * q[y, u_t1] - q[x,u_t]\n","            q[x, u_t] +=  alpha * delta\n","            x = y\n","            u_t = u_t1\n","        \n","        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n","\n","    if render:\n","        # Show the final policy\n","        mdp.current_state = 0\n","        mdp.draw_v_pi(q, get_policy_from_q(q), title=\"Sarsa e-greedy\")\n","    return q, q_list"]},{"cell_type":"markdown","id":"49624024","metadata":{"id":"i8obKDfc0rhK"},"source":["And run it."]},{"cell_type":"code","execution_count":26,"id":"accc8c78","metadata":{"id":"TgY_9mvU0tUr"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e844f3193da047bea23912b876553158","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d74747287b840ffb27020a6f68e0843","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [26], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.02\u001b[39m\n\u001b[0;32m----> 5\u001b[0m q, q_list \u001b[38;5;241m=\u001b[39m \u001b[43msarsa_eps\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNB_EPISODES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [25], line 39\u001b[0m, in \u001b[0;36msarsa_eps\u001b[0;34m(mdp, epsilon, nb_episodes, timeout, alpha, render, q_init)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# Show the agent in the maze\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m         \u001b[43mmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_v_pi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     [y, r, done, _] \u001b[38;5;241m=\u001b[39m mdp\u001b[38;5;241m.\u001b[39mstep(u_t)\n\u001b[1;32m     42\u001b[0m     u_t1 \u001b[38;5;241m=\u001b[39m egreedy(q, y, epsilon)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl_gym/envs/maze_mdp.py:87\u001b[0m, in \u001b[0;36mMazeMDPEnv.draw_v_pi\u001b[0;34m(self, v, policy, title, mode)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_v_pi\u001b[39m(\u001b[39mself\u001b[39m, v, policy, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMDP studies\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlegacy\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     86\u001b[0m     agent_pos \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmdp\u001b[39m.\u001b[39;49mrender(v, policy, agent_pos, title, mode\u001b[39m=\u001b[39;49mmode)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/mazemdp/mdp.py:128\u001b[0m, in \u001b[0;36mMdp.render\u001b[0;34m(self, v, policy, agent_pos, title, mode)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplotter\u001b[39m.\u001b[39mrender(v\u001b[39m=\u001b[39mv, agent_state\u001b[39m=\u001b[39magent_pos, title\u001b[39m=\u001b[39mtitle, mode\u001b[39m=\u001b[39mmode)\n\u001b[1;32m    127\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplotter\u001b[39m.\u001b[39;49mrender(\n\u001b[1;32m    129\u001b[0m         v\u001b[39m=\u001b[39;49mv, agent_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_state, policy\u001b[39m=\u001b[39;49mpolicy, title\u001b[39m=\u001b[39;49mtitle, mode\u001b[39m=\u001b[39;49mmode\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplotter\u001b[39m.\u001b[39mrender(v\u001b[39m=\u001b[39mv, title\u001b[39m=\u001b[39mtitle, mode\u001b[39m=\u001b[39mmode)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/mazemdp/maze_plotter.py:312\u001b[0m, in \u001b[0;36mMazePlotter.render\u001b[0;34m(self, v, policy, agent_state, stochastic, title, mode)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39m# plot.axes.xticks([])\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m# plot.axes.yticks([])\u001b[39;00m\n\u001b[1;32m    310\u001b[0m plot\u001b[39m.\u001b[39mfig\u001b[39m.\u001b[39mtight_layout()\n\u001b[0;32m--> 312\u001b[0m plot\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mdraw()\n\u001b[1;32m    313\u001b[0m plot\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mflush_events()\n\u001b[1;32m    314\u001b[0m rgba \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(plot\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mbuffer_rgba())\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:408\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[1;32m    406\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[1;32m    407\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 408\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[1;32m    409\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 74\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     76\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/figure.py:3069\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3069\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3070\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3072\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   3073\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/axes/_base.py:3106\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3103\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   3104\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 3106\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3107\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3109\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/table.py:412\u001b[0m, in \u001b[0;36mTable.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_positions(renderer)\n\u001b[1;32m    411\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cells):\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cells[key]\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    414\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/table.py:145\u001b[0m, in \u001b[0;36mCell.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m# draw the rectangle\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    146\u001b[0m \u001b[39m# position the text\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_text_position(renderer)\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/artist.py:55\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     renderer\u001b[39m.\u001b[39mstop_filter(artist\u001b[39m.\u001b[39mget_agg_filter())\n\u001b[0;32m---> 55\u001b[0m \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39;49mget_rasterized():\n\u001b[1;32m     56\u001b[0m     renderer\u001b[39m.\u001b[39m_raster_depth \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m (renderer\u001b[39m.\u001b[39m_rasterizing \u001b[39mand\u001b[39;00m artist\u001b[39m.\u001b[39mfigure \u001b[39mand\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         artist\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39msuppressComposite):\n\u001b[1;32m     59\u001b[0m     \u001b[39m# restart rasterizing to prevent merging\u001b[39;00m\n","File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/matplotlib/artist.py:904\u001b[0m, in \u001b[0;36mArtist.get_rasterized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    901\u001b[0m         gc\u001b[39m.\u001b[39mset_clip_rectangle(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    902\u001b[0m         gc\u001b[39m.\u001b[39mset_clip_path(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_rasterized\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    905\u001b[0m     \u001b[39m\"\"\"Return whether the artist is to be rasterized.\"\"\"\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rasterized\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["env = gym.make(\"MazeMDP-v0\", kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2})\n","env.reset()\n","\n","epsilon = 0.02\n","q, q_list = sarsa_eps(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT, q_init= 0.1)"]},{"cell_type":"markdown","id":"3e7bf5bc","metadata":{"id":"hbtNZZTNhRdx"},"source":["As for **q-learning** above, copy-paste the resulting code to get a *sarsa_soft(...)* and a *sarsa_eps(...)* function."]},{"cell_type":"code","execution_count":27,"id":"c78f1d15","metadata":{"id":"UFMuH9Iq6026"},"outputs":[],"source":["# --------------------------- Sarsa, softmax version -------------------------------#\n","\n","# Given a temperature \"tau\", the SARSA algorithm computes the state action-value function\n","# based on a softmax policy\n","# alpha is the learning rate\n","def sarsa_soft(\n","    mdp: MazeMDPEnv,\n","    tau: float,\n","    nb_episodes: int = 20,\n","    timeout: int = 50,\n","    alpha: float = 0.5,\n","    render: bool = True,\n","    q_init: float = 0\n",") -> Tuple[np.ndarray, List[float]]:\n","\n","    # Initialize the state-action value function\n","    # alpha is the learning rate\n","    q = np.full((mdp.nb_states, mdp.action_space.size), q_init)\n","    q_min = np.zeros((mdp.nb_states, mdp.action_space.size))\n","    q_list = []\n","\n","    # Run learning cycle\n","    mdp.timeout = timeout  # episode length\n","\n","    if render:\n","        mdp.init_draw(\"Sarsa softmax\")\n","\n","    for _ in tqdm(range(nb_episodes)):\n","        # Draw the first state of episode i using a uniform distribution over all the states\n","        x = mdp.reset(uniform=True)\n","        done = False            # À compléter... \n","        dist = softmax(q, x, tau)\n","        u_t = np.random.choice(mdp.action_space.size, p = dist)\n","        \n","        while not done:\n","            if render:\n","                # Show the agent in the maze\n","                mdp.draw_v_pi(q, q.argmax(axis=1))\n","                \n","            [y, r, done, _] = mdp.step(u_t)\n","            dist = softmax(q, x, tau)\n","            u_t1 = np.random.choice(mdp.action_space.size, p = dist)\n","            \n","            delta = r + mdp.gamma * q[y, u_t1] - q[x,u_t]\n","            q[x, u_t] = q[x, u_t] + alpha * delta\n","            x = y\n","            u_t = u_t1\n","        \n","        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n","    if render:\n","        # Show the final policy\n","        mdp.current_state = 0\n","        mdp.draw_v_pi(q, get_policy_from_q(q), title=\"Sarsa softmax\")\n","    return q, q_list\n"]},{"cell_type":"markdown","id":"1566fe0c","metadata":{"id":"koHzhn0h02aa"},"source":["And run it."]},{"cell_type":"code","execution_count":67,"id":"8b37a121","metadata":{"id":"SVW4kFli04GB"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9ae9438cc4f4bb98d80f358fc0853bc","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38c1f3a276b947f5ac32778fc73b7ebb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tau = 6\n","q, q_list = sarsa_soft(env, tau, nb_episodes=NB_EPISODES, q_init = 0.1)"]},{"cell_type":"markdown","id":"d5af8c71","metadata":{"id":"G46ht0tRWLj6"},"source":["## Study part"]},{"cell_type":"markdown","id":"56e7e8c6","metadata":{"id":"1BceAWIYYFg5"},"source":["### Impact of `epsilon` and `tau` on q-learning and sarsa"]},{"cell_type":"markdown","id":"aff12e8b","metadata":{"id":"3B8GdWVfX5N2"},"source":["Compare the number of steps needed by **q-learning** and **sarsa** to converge on a given MDP using the *softmax* and *$\\epsilon$-greedy* exploration strategies. To figure out, you can use the provided `plot_ql_sarsa(m, epsilon, tau, nb_episodes, timeout, alpha, render)` function below with various values for $\\epsilon$ (e.g. 0.001, 0.01, 0.1) and $\\tau$ (e.g. 0.1, 5, 10) and comment the obtained curves. Other visualizations are welcome."]},{"cell_type":"markdown","id":"3e56764b","metadata":{},"source":["Note that instead of the temperature `tau`, computational neuroscience researchers, who generally prefer softmax exploration, use a parameter `beta` which behaves as an inverse of the temperature. That way, the three hyper-parameters of basic tabular RL algorithms are `alpha`, `beta`, and `gamma`."]},{"cell_type":"code","execution_count":28,"id":"2f5ea166","metadata":{"id":"P9kycY4D696H"},"outputs":[],"source":["\n","# -------- plot learning curves of Q-Learning and Sarsa using epsilon-greedy and softmax ----------#\n","\n","def plot_ql_sarsa(m, epsilon, tau, nb_episodes, timeout, alpha, render):\n","    q, q_list1 = q_learning_eps(m, epsilon, nb_episodes, timeout, alpha, render)\n","    q, q_list2 = q_learning_soft(m, tau, nb_episodes, timeout, alpha, render)\n","    q, q_list3 = sarsa_eps(m, epsilon, nb_episodes, timeout, alpha, render)\n","    q, q_list4 = sarsa_soft(m, tau, nb_episodes, timeout, alpha, render)\n","\n","    plt.clf()\n","    plt.plot(range(len(q_list1)), q_list1, label='q-learning epsilon')\n","    plt.plot(range(len(q_list2)), q_list2, label='q-learning tau')\n","    plt.plot(range(len(q_list3)), q_list3, label='sarsa epsilon')\n","    plt.plot(range(len(q_list4)), q_list4, label='sarsa tau')\n","\n","    plt.xlabel('Number of episodes')\n","    plt.ylabel('Norm of Q values')\n","    plt.legend(loc='lower right')\n","    # plt.savefig(\"comparison_RL.png\")\n","    plt.title(\"Comparison of convergence rates\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"e3917163","metadata":{"id":"QjSmclM31MkU"},"outputs":[],"source":["# example\n","tau = 6\n","epsilon = 0.02\n","plot_ql_sarsa(env, epsilon, tau, 1000, 50, 0.5, False)"]},{"cell_type":"markdown","id":"d51fb2a0","metadata":{"id":"sDquKHXZiPmA"},"source":["### Effect of hyper-parameters"]},{"cell_type":"markdown","id":"36ed7583","metadata":{"id":"JqnYA-fKiTU3"},"source":["The other two hyper-parameters of **q-learning** and **sarsa** are $\\alpha$, and $\\gamma$. By varying the values of these hyper-parameters and watching the learning process and behavior of the agent, explain their impact on the algorithm. Using additional plotting functions is also welcome."]},{"cell_type":"markdown","id":"a16b72ce","metadata":{},"source":["### Actor Critic"]},{"cell_type":"code","execution_count":32,"id":"291795cf","metadata":{},"outputs":[],"source":["def critic_v(v, initial_state, end_state, reward, gamma, alpha):\n","    delta = reward + gamma * v[end_state] - v[initial_state]\n","    v[initial_state] += alpha * delta\n","    return v, delta\n"]},{"cell_type":"code","execution_count":null,"id":"41817654","metadata":{},"outputs":[],"source":["def critic_q(q, initial_state, end_state, reward, gamma, alpha):\n","    delta = reward + gamma * v[end_state] - v[initial_state]\n","    v[initial_state] += alpha * delta\n","    return v, delta"]},{"cell_type":"code","execution_count":33,"id":"349dc898","metadata":{},"outputs":[],"source":["def get_policy_from_P(P: np.ndarray) -> np.ndarray:\n","    # Outputs a policy given the action values\n","    return np.argmax(q, axis=1)"]},{"cell_type":"code","execution_count":65,"id":"0a4e2458","metadata":{},"outputs":[],"source":["def actor_critic_v(\n","    mdp: MazeMDPEnv,\n","    epsilon: float,\n","    nb_episodes: int = 20,\n","    timeout: int = 50,\n","    alpha1: float = 0.5,\n","    alpha2: float = 0.5,\n","    render: bool = True,\n","    render_final: bool = True,\n","    P_init: float = 0.1,\n","    v_init: float = 0.1,\n",") -> Tuple[np.ndarray, List[float]]:\n","    # Initialize the state-action value function\n","    # alpha is the learning rate\n","    P = np.full((mdp.nb_states, mdp.action_space.size), P_init)\n","    v = np.full(mdp.nb_states, v_init)\n","    P_list = []\n","\n","    # Run learning cycle\n","    mdp.timeout = timeout  # episode length\n","\n","    if render:\n","        mdp.init_draw(\"Q-learning e-greedy\")\n","\n","    for _ in tqdm(range(nb_episodes)):\n","        # Draw the first state of episode i using a uniform distribution over all the states\n","        x = mdp.reset(uniform=True)\n","        done = False\n","        while not done:\n","            if render:\n","                # Show the agent in the maze\n","                mdp.draw_v_pi(P, P.argmax(axis=1))\n","\n","            # Actor :  Draw an action using an epsilon-greedy policy\n","            u = egreedy(P, x, epsilon)\n","\n","            # Perform a step of the MDP\n","            \n","            [y, r, done, _] = mdp.step(u)\n","            \n","            # Critic: compute delta and update v\n","            v, delta = critic_v(v, x, y, r, mdp.gamma, alpha1)\n","\n","            P[x, u] = P[x, u] + alpha2 * delta\n","            \n","            # Proba normalizarion\n","            P[x, :] = [p/np.sum(P[x,:]) if p> 0 else 0 for p in P[x, :]]\n","\n","            # Update the agent position\n","            x = y\n","        P_list.append(np.linalg.norm(P))\n","\n","    if render_final:\n","        # Show the final policy\n","        mdp.current_state = 0\n","        mdp.draw_v_pi(P, get_policy_from_q(P), title=\"Q-learning e-greedy\")\n","    return P, P_list\n","    "]},{"cell_type":"code","execution_count":66,"id":"3a2d57e0","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e93bab994f2443e18c030f5256b55393","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["from mazemdp import chrono\n","\n","env = gym.make(\"MazeMDP-v0\", kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2})\n","env.reset()\n","env.init_draw(\"The maze\")"]},{"cell_type":"code","execution_count":68,"id":"6f5e264e","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed8e3a7ac6ad4d95bba858a72c886b2f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0d5931d907240deadb032eadba785de","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["2.839860200881958\n"]},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAiMAAAGfCAYAAACNytIiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDz0lEQVR4nO3deXxU1d0G8Gcms2SdyUb2AIFAAoSwC0EFlF1qobbWIq8gRSwV32K1VrFaLS6hL9XWpUXUAlbEtFgBlc3IqhL2BBKWsJMQshBIZrJOJjPn/WOSmwxZJyS5mczz/XzyYebec2d+h4nOw7nnnqsQQggQERERyUQpdwFERETk2hhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWqts5ePny5Vi6dCmWLFmCv/3tb422Wbt2LebPn2+3TavVorKystXvY7Vace3aNfj4+EChUNxOyURERNRJhBAoKSlBWFgYlMqmxz/aHEYOHz6MVatWIT4+vsW2Op0OmZmZ0nNHA8W1a9cQGRnpcI1EREQkv+zsbERERDS5v01hpLS0FHPmzMGHH36I1157rcX2CoUCISEhbXkrAICPjw8AW2d0Ol2bX4eIiIg6j9FoRGRkpPQ93pQ2hZHFixdjxowZmDRpUqvCSGlpKXr16gWr1Yrhw4fjjTfewKBBg5psbzKZYDKZpOclJSUAbCMsDCNERETOpaUzIg5PYE1KSsKxY8eQmJjYqvYxMTFYvXo1Nm/ejHXr1sFqtWLs2LG4evVqk8ckJiZCr9dLPzxFQ0RE1H0phBCitY2zs7MxcuRIJCcnS3NFJkyYgKFDhzY5gfVWZrMZAwYMwOzZs/Hqq6822ubWkZHaYR6DwcCRESIiIidhNBqh1+tb/P526DTN0aNHUVBQgOHDh0vbLBYL9u3bh/feew8mkwlubm7NvoZarcawYcNw/vz5JttotVpotVpHSiMiIiIn5VAYmThxItLT0+22zZ8/H7GxsXjuuedaDCKALbykp6fjvvvuc6xSIiIi6pYcCiM+Pj6Ii4uz2+bl5YWAgABp+9y5cxEeHi7NKVm2bBnGjBmD6OhoFBcXY8WKFbhy5Qoee+yxduoCERERObPbWvSsMVlZWXYLmxQVFWHhwoXIy8uDn58fRowYgf3792PgwIHt/dZERETkhByawCqX1k6AISIioq6jtd/fvDcNERERyYphhIiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiFyI1Sqw9odLOJ5dLHcpknZfZ4SIiIi6rq9OXMMrX50CAFxePkPmamw4MkJERORCzuaXyF1CAwwjRERE7WRPZgE+2HcBVdVWuUuxs+VELhZ9chT7LxRCAYXc5TTA0zRERCSr6yUmZBeVY3hPP7lLuW2PrjkMAAj39cSM+NBWHXOj1ITl285g5tBw3NUvsN1qOV9Qgt/++ziKK6qQfbMCAHAy14CZQ8KlNqZqC7Sqlm9y29E4MkJERLIak7gTD/xjP45cvtmq9nLcxcRqbfk9qy11oyHXSyrt9pVUmvHX5LP4eP/lBse9u+s8Nhy9iv/558HbrrO+HSfzkZ5jkIIIAOQZKqGoNzBy5/LdMFvkH8VhGCEiIllZar7ofzh/A4D9l/qtln6RjjGJO5FnqGyyTVO+OZmHFzam41pxBSrNFpzJMzYbbLZn5GL5tjNIOpSFuFd2YFt6bqPtrhaVo9RUjZvlVdK29BwjdpzMQ0aOAeVV1diUmoO3d57Dy1+exOlco93xR640DGEf7ruI5FP5DvUvNasIu88UYHdmAb4/V4j/HMlu0MZsEajf5cJSExZ8fMSh9+kIPE1DRNROqqqtyLhmwOBwPdRujv1bz2IV+P58IVRKBcb2DYBC0fXO67e33ZkF+GvyWem5UgEs33YG6w5cwWcLx2BwhL7BMZ8dygIAfJF6FU9MiEal2YLn/3sCEwcEo6e/JzYczcYzk2Pg56VpcOzjnxyVHpeZqrE57RrmJfTCyWtG/H5aLO6I8gdgGwWprLZg0bpjdsf/+tNjGByuR1W1FWaLFaZqKwpLTTA1Mj/kv8eu4r/Hrjba73+lXEaIzgOmagsEgIycunBys6wKlwpL8frW0wCAfc/eg54Bng1e41pxBRK3nUFqVhGCfLS4cqMcN8qqGrRrzHu7z9s933f2OgpLTQj01rbq+I7AMEJE1E7++u1ZrNxzAXNG98S8sb3x5jeZSD6VDy+tCp8vGovvzl3Hmh8uI+nxMYj0t/+CST6VJ335rX9sNHoFesFqFQ3adQWZeSU4m1+CH8WH3lZoml8zv6KWQgG8v/cCAOCD7y7i3dnDmjx2c+o1xIXp8e3pfGxKu4ZNadekfesOZGHdgtF28y9u1vuizr5Zju/OFQIAPk65AgD4+aoUDAjV4cL10mYnn6bnGFrVN6UCaOrMzmeHGo5Y1FrzwyXEhPhIz8et2I0Z8aF4fVYcfD1tAavMVI2xy3dJba4WVdi9Rly4DuZqAa1aiYGhOnhrVfjo+0vN1vvshuNYM/+OlrrVYRhGiIjaSe0X6acHs5CZV4IjV4oAACWV1Zj6t31Su+Xbz+DvDw+3OzanuO60w6lcIx5dcxgWIXDohYnQeaihUirafbTkUmEZvLUq9PCx/xdxTnEFXvvatg6Fh9oNecZK3CitwodzR8LHXYXpb++DVQA6DzXG9+9hd+zpXCNyDRW4Nza4wfsJIVBptiK7qBxFjfwrfv3BLOlx+tVi/HC+EEXlVSgzVaOwtMruvTLzSzB39aEm+/bomkM49/p0VFsF5v7zEFIu3pD2VVsaTwm3nj6pFRPsg8JSE355VxRign3goXGD2k0JjUoJT40bvLQqeKrd4KFxg1altPucLheW4eQ1I7QqJa4WlSMtuxhuSiW0aiW0KiWEANbWm0eSU1SBfWev273/lhO5KCwxITrIG1vSc1Fcbrbb/38/jYevpxr+XhqM6OXX6O/Jiz8aCGOlGbtOF+Cpf6c12L8783qDbZ1JIeSYCeQgo9EIvV4Pg8EAnU4ndzlE5MJM1RY8tOoAhvX0xcv3D7LbN+Cl7agwW1p8jSkDg/HB3JEoKqtChdmCMF8P/H33eazYkdmgbVy4Dhk5RiT0CcBnj49pc927zuQj3NdT+lf3Nyfz8PgnR6F2U2D/8xPtAskfN2fgXzUjBi15bloslArATalADx8tliSlAQCentwfuzMLYKgwQwjAU+OGrJvlKKmsbnMfHPXR3JFYfygLu84U2G33ULs1+JwGhuqw4K4ojOjlBx93FdzVtmChcvB0W1vc85c9uFRY1ug+T40byqsa/526MzoAqx8d5fDVMNeKK/CXHZn4+kQuqurNz7mUeF+7B97Wfn8zjBARwTZ0b6q2IDrIp9l22zNypdMp9VevLKk0Y/Ar37T6/e6MDpAmbLbWhTfug5uy9V8WQggoFApk5Bjwo3e/B2CrWQiBP2/PlEZyPNRu6BvkhYoqC4rLza2ee9BW7molvLVqFJaapG3PTo3BiavF2HGybtJmvyBv+HlpUGaqxslrdaMW4b4eiA7yxt6zrf/XvLtaiUpzw9MvB5ZORIjevY09aR85xRX44Vwhfv/fE3bbv3hiLKotAi9tyoBFCPh5qhHorcUTE6JhEQJxYbrbCksWq8DoN75FYant8/7mt+PQP7j5339Htfb7m6dpiMglfLz/Mr47dx1vPjgUek+13b7vzxVKl1Uef3kK9B62/WaLtcHpkfoXeiz76hSUClu7j1s5klCruSAyJEIPU7UVZ/LsV8osq6qGj1aFdQeuIKe4EkoFMDTSF1MGhTR4jTe2nsZnh7Lw2cIxdq/zVvJZrDtwxW4ORYXZYjeJsr4+gV6Yf2dv7DtXiLd/MRRale20zR83ZSCnuAK9A7zgpVUhp7gcBy7aXxWiUiqw+tFRKCgx4WaZCcE6d3io3TBxQDDclApUVFlgqrbAS6uSJvzmGirgoXaD3kNt9/eeU1yBPEMFvLQq9PT3hKfG/usr/aoBH353EV8ev4ZbRfh5YM2jo+DrqcEv1x6GocIMPy8N/nDfANmDCGALVz8fFYleAZ7ILqpAVKAXhkX6QlkTPHf8dlyHvK+bUoEjL07GlhO5MFusCJXx74IjI0Tk1Dan5eBCQSl+O7m/3ZfXn746ibTsYnyyYDS8tSr0fn4LAODl+wdi/p1RUrtz+SWY/Ne6+RzbltyNAaE6fH+uEL/8+DAmDwjG3+fUze/YcTIPv6p3Vcat7u4XiDt6+yM1uxhmixVDInyx/WQezheUSm0i/DwwbVAI4sL1iA7yhtliRVW1FUXlVZg0IFj6164QAtVWgX5/2Nbk+ykVwIEXJsJLo8K2jDx8czIPQyJ9pVM+PXy0mDE41G5ewq08NW6IDvLGQ6MiMSzSD2aLFWnZxYgO8kZ8hB4+7uomj63v4vVSXLheBpWbAlEBXgj383D4qqL2UFFlwY0yE8J9PWCqtsJdLf+iXq6KIyNE1O0JIaQ5ChNig6QVPIUQWPPDZQDAlhPXMCEmSDpm15kC6NzV8HFXwU2pQO4t61UcuHgDKqUCa364hKpqK7ak5+Ldmks9q6qtKK035yE2xAdj+gRA7aaAUqHAj4eGYVBYw8tRfzc1RnpstQrpX7wtUSgUULsp0CfQCxcbmVMQ5KNFQYkJd7y+0277N/XWp7heYmoQRCL8PLBtyd3w0qiarGVIpG+raqyvTw9v9Onh7fBx7c1D44YIje0qJAYR58CRESJymBC2hZNa+6XaEcqrqvHcf9PxVb1h+T49bPMeisqrGp0f0J4mDQjCR/NGdeh71DJbrLhcWIZ/fn8JW9JzMTBUhz/ePxBfHc+V5n0AQKS/h91qmwAQpneHn5cGhgozZg4Nw6yh4YgK9OqUiZlEnMBKRG1isQooYAsalwrL4O+pkeZYLP3iBPZkXkel2QJDhRm/mxqDJyZEy1Lnwx8ewP4Ljk0ABYC7ogNhFQI5xRW4cqMcKqUC1fUWhNDVjJgoFYomJ3L6aFX4y8+HYGojczU6k9UqkF1UDrWbEqqaq1ne23Uenx7MwsxhYXhmcgw0KoYOkg9P0xBRqxkrzVDAtv7C1L/tg1atxPv/MwIz3vkeWpUSma9NR6XZ0mCxpuRT+bKEkatF5VIQ+dmICIzq7YcgnTs81W7w1KjgrlaixFQND7Ubgny0OJVrhN5DjfgIX7vXqQ1eldUWrN1/GXf09sfI3v52bQzlZnx2OAtRgV6YGBsEtw5Y76OtlEoFegV42W3734n98L8T+8lUEVHbMIwQubjsm+WY+NZeuCkUeG1WHApKbJdbznjHdilo7VLX9Ze8fnpyf7yVfLbTbpMuhMDp3BLkGSuw72whvjmZJ+37y4NDWjz+7n49Gt1ee5msp0bVZKjSe6qxaHzfNlRNRK3FMELUTQkhcPlGOXoHeDb7L/nTuUYpVDS31HX9O3uO6OXXYFtrrNhxBkmHsmG2WDGspx/WPDrKbt6J2WK710dhSRUKy0yorLIgz1iJj767hJziigavNy+hl0PvT0RdE8MIUTf1zs7z+Ou3Z/Gbe6Px9JSYRttUW6w4cbUugDR1+eenB68g32gbMdG42ZaxBoCz+aXo/fwWPD6uDx4aFQk3hQKGCjMGNbEY0/qDWSiqWcp679nryCmuwI2yKtwoNeHi9TL8efsZu/kbjbmjtz/6BXtjTJ8ATIuTd84GEbUPTmAl6qZq19UA7FcKrZV1oxy/+CAF15q4FXvvAE9cvlHeYHuwTosNvxqLcSt2N/neE2ODsOCuKKz4JhOpWcW4u18gPNRudpecNkfnrkK4nyc81EroPdTw9dRA76HGM1P6t3rNCyKSHyewEpGkdllwwHYq5NkNx+3uclrfq7PicHd0IHoFeGJjag6e/s9xad+8hF748dBw9AzwxN39AqU7n95q55kC7Kx3P5D67XTuKgyJ9JW2KRXAgFAdAry1GBqhx89GRDZ6y3Qi6r4YRoi6EKtVYM/ZAgyN9IOvh7rd1vGIWroVj4/rg4vXS/Ht6bqQEOitQULfwCZv1T5zaDhW7b2IrJvlSP3jZLsFpD54ZCQ+OXAZ4/r3QN8e3igzVcNLq8Kgl3c0mNg6MFSHB0dGoIePFnFhekT6eyLrZjk0KiUCvDRcmIrIxfE0DVEHEEJg7f7LCPJxx4z40Bbbf3fuOr5Mu4Y+Pbzx5+1nAACDw/VYt2A0rELAz0vjcA0xL26zuwLmVtMGheD9R0a0+DoWq4BSgVZfzlpcXoW07GJk3yxHdJAPRvTy41oXRC6Kp2mIZHTiqgF/+uoUAGBCzFR4aev+U9t/oRAnc4zo4aNFDx8t7owOxCP/PNTgNdJzDBiy7BtoVEr88Ny9drd4b8n1ElOTQaR3gCeSnx7f6nuGOHKXWADw9dTYLb9ORNQShhGiJpRXVWPLiVzcGxuEAO/WBwEAOJtfd5fUjak5uCc2CKeuGfHFsavYlpFn17axyaX1VVVbMer1b/HFE2Ole6/UMlaacfjSTew7ex2zhoVjWE8/nC8oxaS39jZ4nelxIXBTKrBsZpwsNy8jImoKT9MQNeEPG9Px6cEsDAzV4S8PDsEnB67g1+P7tmpyZf0rWQDbLcIbWycDsN207GpR4/tudXn5DFy4Xor3dp3H2fwSZOaV2F0KOyhMh5PXGt4K/tWZg/BIQu9WvQcRUXvhvWmIbtPQZd+guGZNjNgQH5zJK4GPuwrpr0xt0DbXUIGNqTkY2zcQ+cbKZm8x35KEPgFIudj4PVd83FUoqXfX2OaM698DH80dyfkaRCQbzhkhuk3KehM2z+TZTruUVFaj9/Nb8MDwcHhrVai2CrgpFPjqxLWa4JLp8PtE+nvgZ8Mj8ejY3vDQuEGjUmLGO981OsJRG0RC9e54dmoMYkN0GBimw1fHr2Fjag5G9vbDsEg/RPh5INKfl8cSkXPgyAhRE2491dLeXvrRQMwaGtbofJQjl29i3upDKKuyYPkDg2G2CvT090SIzh0henfo3FVd5mZtRERN4cgI0W1q7nQJADw8uieCfLT427fnGux75f6BUCoVWPPDZeg81Aj00mBc/x5IzSqCn5cGPx0egbhwfZOvPbK3P9Jfmdpu64wQEXVlDCPk8i5cL0W4rwfclAr86pOjyDVU4p1fDEWF2QIAeG1WHMb2DUBUoFejoxFRgV5YkpQmPVcpFZg1LBy+nhrMvWXS6Lyx9s+bwyBCRK6CYYS6BYtVYPX3l/DfY1fxrwV3IMjHvdF2pmoL/vbtOWTdKEdhqQnHrxaj0mzFz0dG4JExvbGrZgnzxG1nkJZdDAAI83VHnx7eTb73zKHhuCs6EIWlVfju3HWM6RMAX0/HFykjInJVDCPULfxwvhCvbz0NALjj9Z2Nrt3xxtbT+GDfxUaP/8+Rq4j0q5vwWRtKvDRuGBrp1+gx9QV4axHgrUVMiE9byicicmm85o+6hbxb7jxrsQr8/P0UDH5lB26UmgAAnx3Msmvz558OxuJ7+krP30w+2+B1P5w3Ev5tWIqdiIhajyMj5FSul5jwxbGrCPfzwI/iw/DhvotIPp2Pvj287NqNfC0ZRTVrhIx47VskPjAYJaa69Tnu7heIh0b1xPmCUvx99wVpe7ivBwpLTZg4IAj3xARhbN/AzukYEZELYxghp7Jo3VEcvVIEABgdFSCdmjl06aZdu9ogUmvpF+l2zz1q7hIbHeSNzxaOQXF5Fe4dEAStinePJSLqbAwj1GUJIXDkShF6+XsiSGebkJp1s1zav/6W0y71qZQKfPPbcdiTeR3v770AjUppt+T6A8PDpccJfQM6oHoiImothhHqspJP5ePxT46iT6AXdv1uAj4/ehXXS0zS/r9+az/Ho3+wNwaH+2JQmA4z4kMRrLNdBfPLu6IAABk5BpRUVmN0lD8vmyUi6kIYRqjL+u5cIQDgYmEZisur8LsNx+32R/jZ1ga5cqMcL98/EPMSejcbMppbZIyIiOTDMEJdSlFZFTal5eDHQ8IgUHengqHLkqXHXz15FwZHMFgQEXUXDCPUpfwmKRXfnSvEjpN5iAr0arD/0bG9GUSIiLoZhhHqUmpPzRy4eBMHLtqukJmb0AtzRvdChJ8HvLT8lSUi6m646BnJ4tQ1I5Z+cQL5xrrFyrZn5DbadmRvf8SE+DCIEBF1U/y/O3WYrBvleGPraZSbLVg6PRYDQutuHz3j3e8ghO1S3U8fGwNDhRlPfHqswWs8eU80Jg8I7syyiYiokzGMUIdZ/cMlbD+ZBwDYd/Y6/vTjQfjsUBbee3g4RM3c1B/O38Cdy3chp7jC7tjpcSF47O4ojOjl39llExFRJ2MYodtyLr8Evp4a9PDRAgAqzRZUWwW8tSqkXLhh1/blL08CACa9tddue/0g8pt7o/H0lJgOrpqIiLqS25ozsnz5cigUCjz11FPNttuwYQNiY2Ph7u6OwYMHY+vWrbfzttRF5BRXYPJf92HU699CCIFKswV3/XkXhv7pG7y4KR2Z+SWtfq0lE/vhyIuTGESIiFxQm8PI4cOHsWrVKsTHxzfbbv/+/Zg9ezYWLFiA1NRUzJo1C7NmzUJGRkZb35q6iFPXjNLjhf86itiXtqOwtArVVoF1B+yXag/0bvrOt7+b0h+/ndwfgd7aDquViIi6rjaFkdLSUsyZMwcffvgh/Pz8mm379ttvY9q0aXj22WcxYMAAvPrqqxg+fDjee++9NhVM8jqTZ8SqvRdQXlUNL23dTeW+PZ3faPv3Hh6GbUvuxtr5d2DywGAEeGkweWAw+gV5w89TjV/eGYU5o3t1VvlERNQFtWnOyOLFizFjxgxMmjQJr732WrNtU1JS8PTTT9ttmzp1KjZt2tTkMSaTCSZT3T1IjEZjk22pc2TdKMfhyzfx5+1nUFBiwrXiCug9mx7tGNs3ALOGhuNH8WHStg/njuyMUomIyMk4HEaSkpJw7NgxHD58uFXt8/LyEBxsf2lmcHAw8vLymjwmMTERf/rTnxwtjdrZ/guFcFMoMLpPAP73s2M4ftUg7fs45Ypd299O6o8pg4IRG+IDhYI3oSMiotZzKIxkZ2djyZIlSE5Ohru7e0fVhKVLl9qNphiNRkRGRnbY+5G9SrMFC/91RFoN9dunx9kFkVv5eqqxZFK/ziqPiIi6GYfCyNGjR1FQUIDhw4dL2ywWC/bt24f33nsPJpMJbm5udseEhIQgP99+PkF+fj5CQkKafB+tVgutlpMZ5fLP7y9JQQQArhVXNtrus4VjsOFINh4f36ezSiMiom7IoTAyceJEpKen222bP38+YmNj8dxzzzUIIgCQkJCAnTt32l3+m5ycjISEhLZVTB2uqKzK7vnc1YcatPlo7kgk9A1AQt+AziqLiIi6KYfCiI+PD+Li4uy2eXl5ISAgQNo+d+5chIeHIzExEQCwZMkSjB8/Hm+++SZmzJiBpKQkHDlyBB988EE7dYFux/aMPET4eSAuvO5OuOVmS5PtI/w8MGVgCCYN5BLtRETUPtp9BdasrCwolXVXDI8dOxbr16/Hiy++iBdeeAH9+vXDpk2bGoQa6nwZOQYsWncUAPDvx8fgjijb0uvrD9rWCHlgWDjC/Tygc1ejV4AnJg4IhpuSk1OJiKh9KYSovUtI12U0GqHX62EwGKDT6Vo+gFrli2NX8fR/jkvPowK9cKmwTHr+z3kjMZE3qSMiojZq7ff3bS0HT87NYrXPofWDyB1R/gwiRETUKXijPBf15fFrePbzE43uO71sGjw0DScjExERdQSOjLigwlITfvNZaqP7fjOxH4MIERF1KoYRF/SrT442uU/nzsEyIiLqXAwjLmb195dw9EpRo/u8NG64b3BoJ1dERESujv8M7saqqq3YlpGLsX0D0cNHi63puVj29SkAgNpNgbOvTccP52+g0mzBpIHBMFusULsxnxIRUediGOmmzBYrFnx8GN+dK0RPf08s/+lgPPHpMQBAqN4d//lVAhQKBe7qFygdwyBCRERy4LdPN/X6ltPS/WWybpZjW3rdXZLXzB+FSH9PuUojIiKywzDSTa3df9nu+ScHrgAAPnhkBGJDuHAcERF1HQwj3URGjgGvfHkSl+stXHarnv6edqdliIiIugLOGekmfvTu9wCA4vIqPDy6V4P978wehqmDgqFVcQ0RIiLqWhhGuplNadewKe2a9PyNnwzGw6N7ylgRERFR83iaxonlGSrx0XcXcaPU1Oj+NfNHMYgQEVGXx5ERJ/bqllPYciIXr205bbdd567Ciz8aiHtigmSqjIiIqPUYRpzYlhO5DbYdfGEignXuMlRDRETUNgwjTmxIpC+OZxcDsM0NGdc/kEGEiIicDsOIk9qcliMFkS2/uQuDwvTyFkRERNRGnMDqhC5cL8WSpDQAwL2xQRgYykXMiIjIeTGMOKGT14zS4788OAQKhULGaoiIiG4Pw4iTqbZY8fvPjwMAHhwRAX8vjcwVERER3R6GESditQo8s+E4Ks1WAMAEXrpLRETdAMOIE/nwu4vYXLO66qAwHe4bHCJzRURERLePYcRJlFSakbjtjPT8iyfGcq4IERF1CwwjTqDSbMHgV76Rnq9bMJo3vCMiom6DYcQJ/GVHpvT4jih/3NUvUMZqiIiI2hfDiBP46kTdXXjf+cUwGSshIiJqfwwjXVxFlQX5RttdeUdH+SNEz+XeiYioe2EY6eIG/HG79Pgnw8JlrISIiKhjMIw4kQdHRspdAhERUbtjGHEibkpeyktERN0Pw4iT8NLwUl4iIuqeGEacxOpHR8ldAhERUYdgGOnChBCoPTMTFeglbzFEREQdhGGkC6swW2AVtsdeWpW8xRAREXUQhpEu7I2tp6XHHmrOGSEiou6JYaSLeiv5LNYdyAIAhPt6QMkraYiIqJtiGOmCjly+iXd2npOef/2/d8lYDRERUcdiGOmCkk/lS4+THh8DPy+NjNUQERF1LM6K7EIqzRZ8digLq/ZdBAC89KOBGNMnQOaqiIiIOhbDSBfy0qYMbDh6VXo+ti+DCBERdX88TdNFWK3CLoh8OHckBoTqZKyIiIioc3BkpIv4IjVHenzoDxMR5OMuYzVERESdhyMjXcSpa0YAwLRBIQwiRETkUhhGuogrN8oAACN7+8lcCRERUediGOkCTlwtxq7MAgBA3x7eMldDRETUuRhGuoBXvjwJIYDB4XqM799D7nKIiIg6FcOIzKotVhzLKgYA/Gp8Hy77TkRELodhRGb/PpItPb4vLlTGSoiIiOTBMCIji1Xg7W9t96CZf2dvjooQEZFLYhiR0acHr6CgxAQAmD82SuZqiIiI5MEwIhOrVeDlL08CAMb374GeAZ4yV0RERCQPhhGZFJaZIITt8XPTYuUthoiISEYMIzKotliRcuEGAKCHjxYDw3gPGiIicl0OhZGVK1ciPj4eOp0OOp0OCQkJ2LZtW5Pt165dC4VCYffj7s6lzn/yj/1YkpQGAAj39ZC3GCIiIpk5dKO8iIgILF++HP369YMQAh9//DFmzpyJ1NRUDBo0qNFjdDodMjMzpecKhetdMXI614hcQwXujQ1Gqaka6TkGad/ie6JlrIyIiEh+DoWR+++/3+7566+/jpUrV+LAgQNNhhGFQoGQkJC2V9gNPPCP/agwWzBpQBC+PV0gbd///L0I48gIERG5uDbPGbFYLEhKSkJZWRkSEhKabFdaWopevXohMjISM2fOxMmTJ1t8bZPJBKPRaPfjrIQQqDBbAMAuiKz4WTyDCBEREdoQRtLT0+Ht7Q2tVotFixZh48aNGDhwYKNtY2JisHr1amzevBnr1q2D1WrF2LFjcfXq1WbfIzExEXq9XvqJjIx0tMwuw2wRds/vGxyCLb+5Cw+OdN4+ERERtSeFEEK03KxOVVUVsrKyYDAY8Pnnn+Ojjz7C3r17mwwk9ZnNZgwYMACzZ8/Gq6++2mQ7k8kEk8kkPTcajYiMjITBYIBO51xXnpSaqhH38g7peforU+DjrpaxIiIios5hNBqh1+tb/P52aM4IAGg0GkRH2yZdjhgxAocPH8bbb7+NVatWtXisWq3GsGHDcP78+WbbabVaaLVaR0vrkiprTtEAwLYldzOIEBER3eK21xmxWq12oxjNsVgsSE9PR2io69wQLt9YCQAI8NJgQKhzjeoQERF1BodGRpYuXYrp06ejZ8+eKCkpwfr167Fnzx7s2GE7DTF37lyEh4cjMTERALBs2TKMGTMG0dHRKC4uxooVK3DlyhU89thj7d+TLup8QSkAINSX66sQERE1xqEwUlBQgLlz5yI3Nxd6vR7x8fHYsWMHJk+eDADIysqCUlk32FJUVISFCxciLy8Pfn5+GDFiBPbv39+q+SXdQXF5lbS4WaQf7z1DRETUGIcnsMqhtRNgupo/bEzHpwezAABr5o/CPTFBMldERETUeVr7/c1703Sgw5dvArDdlZdBhIiIqHEMIx3ko+8u4my+bb7I7Dt6ylwNERFR18Uw0gEKS014bctpAMDwnr64q1+gzBURERF1XQ6vM0Iteyv5rPR4/cIxcFe7yVgNERFR18aRkXZmqDAj6ZBt0mriA4MZRIiIiFrAMNLOkk/lw1pzfRLnihAREbWMYaQdmaot+N2G4wCAxff0lbkaIiIi58Awchuqqq14Z+c57L9QiJJKM4YvS5b2PTy6l4yVEREROQ9OYL0Nm1Jz8FbyWbirlfjFqJ4oq7LdFG9ibBDCfT1kro6IiMg5cGTkNpzKNQIAKs1WrN1/GQAQqnfHqkdGyFgVERGRc2EYuQ1VFqvd80BvDZKfHg+VG/9aiYiIWovfmrchz1Bp9/yDuSPhreWZLyIiIkcwjLTR9RITdp0pkJ4vGt8Xw3v6yVgRERGRc+I/49voja2npcfHXpoMfy+NjNUQERE5L46MtNGlwjIAwP+M6ckgQkREdBsYRtqodr7Iz0ZEylwJERGRc2MYaYNqixV5RlsYCdO7y1wNERGRc2MYaYPVP1wCAKiUCgR4a2WuhoiIyLkxjLTBG1vPAAB+PCQMbkqFzNUQERE5N4YRB9VfW+Tx8X1krISIiKh7YBhx0LGsIulxbIhOxkqIiIi6B4YRB/1xc4bcJRAREXUrDCMOql3u/ZExvWSuhIiIqHtgGHGAEALGymoAwJwxPWWuhoiIqHtgGHHAztMFuFlWBY2bEuG+HnKXQ0RE1C0wjLTSrjP5eOxfRwAAkwcGw8ddLXNFRERE3QPDSCv9Nfmc9Pjno7gEPBERUXthGGkFq1UgPccAAHjsriiM799D5oqIiIi6D4aRVvj04BXp8W8n95exEiIiou6HYaQVrhZXSI+9ai7tJSIiovbBMNIKJrMVALBofF+ZKyEiIup+GEZaocpiCyMeajeZKyEiIup+GEZa4XSuEQCgVfOvi4iIqL3x27UVisqqAABeGo6MEBERtTeGkVa4UWoLIwl9A2SuhIiIqPthGGlBtcWKEpPtfjR+nhqZqyEiIup+GEZakJZdDABQKgCdB5eAJyIiam8MIy3YmJoDAJg+OBRqN/51ERERtTd+uzYj11CBTw9mAQB+MjRc5mqIiIi6J4aRZnx+5Kr0eHQffxkrISIi6r4YRppgKDdLoyLPTYuFjzvnixAREXUEhpEmPLMhDXnGSmhUSvx8ZITc5RAREXVbDCONqKq24tvTBQCAx+/ugwBvrcwVERERdV8MI43YdSZfevzUpH4yVkJERNT9MYw0oqjcDADw1qqg4uW8REREHYrftI24UWoCANwTGyRzJURERN0fw0gjzuaXAgBUSoXMlRAREXV/DCON0Kpsfy2evEsvERFRh2MYacTFwjIAQHSQt8yVEBERdX8MI424XBNG3HiahoiIqMMxjDTCU2s7PdMrwEvmSoiIiLo/hpFGGCuqAQDhvu4yV0JERNT9MYzcQgiBkkrbOiO8Hw0REVHHYxi5RVmVBVZhe6xjGCEiIupwDoWRlStXIj4+HjqdDjqdDgkJCdi2bVuzx2zYsAGxsbFwd3fH4MGDsXXr1tsquKPVTl5VKRVwVzOrERERdTSHvm0jIiKwfPlyHD16FEeOHMG9996LmTNn4uTJk422379/P2bPno0FCxYgNTUVs2bNwqxZs5CRkdEuxXeEk9cMAIBqq4BCwatpiIiIOppCCCFu5wX8/f2xYsUKLFiwoMG+hx56CGVlZfj666+lbWPGjMHQoUPx/vvvN/maJpMJJpNJem40GhEZGQmDwQCdTnc75bbokwNX8NKmDNzR2x//WZTQoe9FRETUnRmNRuj1+ha/v9t8HsJisSApKQllZWVISGj8SzslJQWTJk2y2zZ16lSkpKQ0+9qJiYnQ6/XST2RkZFvLdJi52goACNJpO+09iYiIXJnDYSQ9PR3e3t7QarVYtGgRNm7ciIEDBzbaNi8vD8HBwXbbgoODkZeX1+x7LF26FAaDQfrJzs52tMw2M1tsYUSj4nwRIiKizqBy9ICYmBikpaXBYDDg888/x7x587B3794mA0lbaLVaaLXyjEyUmmxrjGjcGEaIiIg6g8NhRKPRIDo6GgAwYsQIHD58GG+//TZWrVrVoG1ISAjy8/PttuXn5yMkJKSN5Xa88wW2O/ZGBXL1VSIios5w2//8t1qtdpNN60tISMDOnTvttiUnJzc5x6QruFRzaW//YB+ZKyEiInINDo2MLF26FNOnT0fPnj1RUlKC9evXY8+ePdixYwcAYO7cuQgPD0diYiIAYMmSJRg/fjzefPNNzJgxA0lJSThy5Ag++OCD9u9JO8kpqgAARPp7ylwJERGRa3AojBQUFGDu3LnIzc2FXq9HfHw8duzYgcmTJwMAsrKyoFTWDbaMHTsW69evx4svvogXXngB/fr1w6ZNmxAXF9e+vWgnVqtASc2cEb0HV18lIiLqDLe9zkhnaO11yrerzFSNQS/bRnlOL5sGD41bh70XERFRd9fh64x0R2U1oyJKBbgUPBERUSfhN249tZf1emlUXAqeiIiokzCM1FNmsgAAvLQOX/FMREREbcQwUo80MqLlXBEiIqLOwjBST+2cEW+OjBAREXUahpF6yqpsYcRTwzBCRETUWRhG6uGcESIios7HMFJPnsG2+qo354wQERF1GoaReo5fNQAAFzsjIiLqRAwj9eTWjIyE6j1kroSIiMh1MIzUU3vH3plDw2SuhIiIyHUwjNSotlhhtthu08Ob5BEREXUehpEaldVW6bG7mnNGiIiIOgvDSI2KKttlvQoFoFXxr4WIiKiz8Fu3RqXZFkbcVW68SR4REVEnYhipIYURNf9KiIiIOhO/eWsYK80AAB93Tl4lIiLqTAwjNYrKbGHEz5NhhIiIqDMxjNQoKq8CAPh6amSuhIiIyLUwjNQoLufICBERkRwYRmpwZISIiEgeDCM1impGRnw5MkJERNSpGEZqFNeMjPhxZISIiKhTMYzUqDtNw5ERIiKizsQwUqNuAitHRoiIiDoTw0iNM3klABhGiIiIOhvDCIAyU7X0uIePVsZKiIiIXA/DCICyqrowEqJ3l7ESIiIi18MwAsBiFQAAtRvv1ktERNTZGEZQF0bclAwjREREnY1hBHVhRKXkXwcREVFn47cvgOqaMMKBESIios7HMALAWjsy4sa/DiIios7Gb1/UjYxwzggREVHnYxgBYLZYAQAqhhEiIqJOxzAC4ODFmwAAH3eVzJUQERG5HoYRAJn5tqXgNSr+dRAREXU2fvsCSM0qAgAsGt9X5kqIiIhcD8MI6u7YGx3kLXMlRERErodhBEB5lQUA4KnmnBEiIqLO5vJhxGoVqDDbwoiHxk3maoiIiFyPy4cRU7VVeswwQkRE1PlcPoyUV1VLjz3UDCNERESdzeXDSEmlLYx4ady4AisREZEMXD6MGCpsV9LoPNQyV0JEROSaXD6MGCttYUTPMEJERCQLlw8jtWuMcGSEiIhIHgwj5VUAAD9PhhEiIiI5uHwYKaoZGfHz1MhcCRERkWtiGKkZGfFlGCEiIpKFy4eRYmlkhKdpiIiI5ODyYaRImjPCkREiIiI5uHwYMXKdESIiIlk5FEYSExMxatQo+Pj4ICgoCLNmzUJmZmazx6xduxYKhcLux93d/baKbk/VVgEA0Ki4+ioREZEcHAoje/fuxeLFi3HgwAEkJyfDbDZjypQpKCsra/Y4nU6H3Nxc6efKlSu3VXR7MltsYUSldPlBIiIiIlmoHGm8fft2u+dr165FUFAQjh49inHjxjV5nEKhQEhISKvfx2QywWQySc+NRqMjZTrEYrXdtVfF+9IQERHJ4raGAwwGAwDA39+/2XalpaXo1asXIiMjMXPmTJw8ebLZ9omJidDr9dJPZGTk7ZTZrOrakRE3jowQERHJoc3fwFarFU899RTuvPNOxMXFNdkuJiYGq1evxubNm7Fu3TpYrVaMHTsWV69ebfKYpUuXwmAwSD/Z2dltLbNF5tqRETeOjBAREcnBodM09S1evBgZGRn4/vvvm22XkJCAhIQE6fnYsWMxYMAArFq1Cq+++mqjx2i1Wmi12raW5hCLNGeEYYSIiEgObQojTz75JL7++mvs27cPERERDh2rVqsxbNgwnD9/vi1v3e7MVk5gJSIikpND38BCCDz55JPYuHEjdu3ahaioKIff0GKxID09HaGhoQ4f2xGqLbbTNGqepiEiIpKFQyMjixcvxvr167F582b4+PggLy8PAKDX6+Hh4QEAmDt3LsLDw5GYmAgAWLZsGcaMGYPo6GgUFxdjxYoVuHLlCh577LF27krb1K4z4sbTNERERLJwKIysXLkSADBhwgS77WvWrMGjjz4KAMjKyoKy3imPoqIiLFy4EHl5efDz88OIESOwf/9+DBw48PYqbye1V9OoeTUNERGRLBRCCCF3ES0xGo3Q6/UwGAzQ6XTt+tr9/rAVZotAytJ7Ear3aNfXJiIicmWt/f52+eEAnqYhIiKSl0uHEYtVoHZcSM2raYiIiGTh0t/A1TULngGAG6+mISIikoVrhxFL3XQZjowQERHJw6W/geuHES4HT0REJA/XDiP1TtNwOXgiIiJ5uHgYqbuSRqFgGCEiIpKDS4eRqmouBU9ERCQ3lw4jFWYLAMBT0+abFxMREdFtcu0wUmULIx5qN5krISIicl2uHUZqRkbc1S7910BERCQrl/4Wrg0jHhqOjBAREcnFtcMIT9MQERHJjmEEgDvDCBERkWxcO4yYOTJCREQkN5cOI5WcM0JERCQ7lw4jlnorsBIREZE8XDqM1GQRKMAwQkREJBeXDiMCtjTCgREiIiL5uHYYqRkZUfImeURERLJx6TBirTlPwyxCREQkH5cOIzUDI1AwjRAREcnGpcOIVXDOCBERkdxcPIzY/uTACBERkXxcOoxAGhlhGiEiIpKLS4cRK6+mISIikp2LhxHRciMiIiLqUC4dRmqjCEdGiIiI5OPSYYRX0xAREcnPpcOI4NU0REREsnPxMMKraYiIiOTm0mGkbp0RhhEiIiK5uHgY4b1piIiI5ObSYaTurr3y1kFEROTKXDyMcM4IERGR3Fw7jNT8yShCREQkH5cOI3VzRhhHiIiI5OLiYcT2J0/TEBERycelwwgXPSMiIpKfi4cRLgdPREQkN5cOI5wzQkREJD+XDiM8TUNERCQ/lw4jnMBKREQkP5cOI5wzQkREJD/XDiM1fyq47BkREZFsXDqM8EZ5RERE8nPxMGL7k3NGiIiI5OPSYURwZISIiEh2Lh5GbH9yZISIiEg+Lh1GrLyahoiISHYuHUaEdDkN0wgREZFcXDqMcGSEiIhIfi4eRmx/cs4IERGRfBwKI4mJiRg1ahR8fHwQFBSEWbNmITMzs8XjNmzYgNjYWLi7u2Pw4MHYunVrmwtuXzVX08hcBRERkStzKIzs3bsXixcvxoEDB5CcnAyz2YwpU6agrKysyWP279+P2bNnY8GCBUhNTcWsWbMwa9YsZGRk3Hbxt4sjI0RERPJTCCFN43TY9evXERQUhL1792LcuHGNtnnooYdQVlaGr7/+Wto2ZswYDB06FO+//36r3sdoNEKv18NgMECn07W13AbmrzmE3ZnXseJn8XhwZGS7vS4RERG1/vv7tuaMGAwGAIC/v3+TbVJSUjBp0iS7bVOnTkVKSkqTx5hMJhiNRrufjlA7MqLgyAgREZFs2hxGrFYrnnrqKdx5552Ii4trsl1eXh6Cg4PttgUHByMvL6/JYxITE6HX66WfyMiOGbXg1TRERETya3MYWbx4MTIyMpCUlNSe9QAAli5dCoPBIP1kZ2e3+3vUxzkjRERE8lG15aAnn3wSX3/9Nfbt24eIiIhm24aEhCA/P99uW35+PkJCQpo8RqvVQqvVtqU0h/CuvURERPJzaGRECIEnn3wSGzduxK5duxAVFdXiMQkJCdi5c6fdtuTkZCQkJDhWaQewWm1/cs4IERGRfBwaGVm8eDHWr1+PzZs3w8fHR5r3odfr4eHhAQCYO3cuwsPDkZiYCABYsmQJxo8fjzfffBMzZsxAUlISjhw5gg8++KCdu+I4Ac4ZISIikptDIyMrV66EwWDAhAkTEBoaKv38+9//ltpkZWUhNzdXej527FisX78eH3zwAYYMGYLPP/8cmzZtanbSa2eRrqbhsmdERESycWhkpDVLkuzZs6fBtgcffBAPPvigI2/VKQSvpiEiIpKdS9+bRnCdESIiItm5dBjh1TRERETyc/EwYvuT64wQERHJx6XDSO0MGM4ZISIiko9rhxGepiEiIpKdS4eRujkjTCNERERycekwIjhnhIiISHYuHUbqFj0jIiIiubh0GKlb9IxxhIiISC4uHkZsf/JqGiIiIvm4dBixCp6nISIikptLh5G6dUaYRoiIiOTi0mHEyjkjREREsnPpMFJ3ozx56yAiInJlLh5GakdGZC6EiIjIhbl0GJHWGeHQCBERkWxcPIzULAcvcx1ERESuzKXDCJeDJyIikp+LhxFeTUNERCQ3lw4jVl5NQ0REJDuXDiOiZtkzhhEiIiL5uHQYsXLOCBERkexUchcgp/l39kaZqRoB3hq5SyEiInJZLh1GnpgQLXcJRERELs+lT9MQERGR/BhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRIiIikhXDCBEREcnKKe7aK4QAABiNRpkrISIiotaq/d6u/R5vilOEkZKSEgBAZGSkzJUQERGRo0pKSqDX65vcrxAtxZUuwGq14tq1a/Dx8YFCoWi31zUajYiMjER2djZ0Ol27vW5X0t37yP45v+7eR/bP+XX3PnZk/4QQKCkpQVhYGJTKpmeGOMXIiFKpRERERIe9vk6n65a/YPV19z6yf86vu/eR/XN+3b2PHdW/5kZEanECKxEREcmKYYSIiIhk5dJhRKvV4uWXX4ZWq5W7lA7T3fvI/jm/7t5H9s/5dfc+doX+OcUEViIiIuq+XHpkhIiIiOTHMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWLh1G/v73v6N3795wd3fH6NGjcejQIblLatErr7wChUJh9xMbGyvtr6ysxOLFixEQEABvb2/89Kc/RX5+vt1rZGVlYcaMGfD09ERQUBCeffZZVFdXd3ZXJPv27cP999+PsLAwKBQKbNq0yW6/EAJ//OMfERoaCg8PD0yaNAnnzp2za3Pz5k3MmTMHOp0Ovr6+WLBgAUpLS+3anDhxAnfffTfc3d0RGRmJ//u//+vorgFouX+PPvpog8902rRpdm26cv8SExMxatQo+Pj4ICgoCLNmzUJmZqZdm/b6vdyzZw+GDx8OrVaL6OhorF27tqO7B6B1fZwwYUKDz3HRokV2bbpqH1euXIn4+HhpBc6EhARs27ZN2u/sn19L/XPmz64xy5cvh0KhwFNPPSVt6/KfoXBRSUlJQqPRiNWrV4uTJ0+KhQsXCl9fX5Gfny93ac16+eWXxaBBg0Rubq70c/36dWn/okWLRGRkpNi5c6c4cuSIGDNmjBg7dqy0v7q6WsTFxYlJkyaJ1NRUsXXrVhEYGCiWLl0qR3eEEEJs3bpV/OEPfxBffPGFACA2btxot3/58uVCr9eLTZs2iePHj4sf//jHIioqSlRUVEhtpk2bJoYMGSIOHDggvvvuOxEdHS1mz54t7TcYDCI4OFjMmTNHZGRkiM8++0x4eHiIVatWyd6/efPmiWnTptl9pjdv3rRr05X7N3XqVLFmzRqRkZEh0tLSxH333Sd69uwpSktLpTbt8Xt58eJF4enpKZ5++mlx6tQp8e677wo3Nzexffv2LtHH8ePHi4ULF9p9jgaDwSn6+OWXX4otW7aIs2fPiszMTPHCCy8ItVotMjIyhBDO//m11D9n/uxudejQIdG7d28RHx8vlixZIm3v6p+hy4aRO+64QyxevFh6brFYRFhYmEhMTJSxqpa9/PLLYsiQIY3uKy4uFmq1WmzYsEHadvr0aQFApKSkCCFsX4xKpVLk5eVJbVauXCl0Op0wmUwdWntr3PplbbVaRUhIiFixYoW0rbi4WGi1WvHZZ58JIYQ4deqUACAOHz4stdm2bZtQKBQiJydHCCHEP/7xD+Hn52fXx+eee07ExMR0cI/sNRVGZs6c2eQxztQ/IYQoKCgQAMTevXuFEO33e/n73/9eDBo0yO69HnroITF16tSO7lIDt/ZRCNsXWv3/+d/K2fro5+cnPvroo275+QlR1z8hus9nV1JSIvr16yeSk5Pt+uQMn6FLnqapqqrC0aNHMWnSJGmbUqnEpEmTkJKSImNlrXPu3DmEhYWhT58+mDNnDrKysgAAR48ehdlstutXbGwsevbsKfUrJSUFgwcPRnBwsNRm6tSpMBqNOHnyZOd2pBUuXbqEvLw8uz7p9XqMHj3ark++vr4YOXKk1GbSpElQKpU4ePCg1GbcuHHQaDRSm6lTpyIzMxNFRUWd1Jum7dmzB0FBQYiJicGvf/1r3LhxQ9rnbP0zGAwAAH9/fwDt93uZkpJi9xq1beT4b/bWPtb69NNPERgYiLi4OCxduhTl5eXSPmfpo8ViQVJSEsrKypCQkNDtPr9b+1erO3x2ixcvxowZMxrU4QyfoVPctbe9FRYWwmKx2P2lA0BwcDDOnDkjU1WtM3r0aKxduxYxMTHIzc3Fn/70J9x9993IyMhAXl4eNBoNfH197Y4JDg5GXl4eACAvL6/Rftfu62pqa2qs5vp9CgoKstuvUqng7+9v1yYqKqrBa9Tu8/Pz65D6W2PatGl44IEHEBUVhQsXLuCFF17A9OnTkZKSAjc3N6fqn9VqxVNPPYU777wTcXFx0vu3x+9lU22MRiMqKirg4eHREV1qoLE+AsDDDz+MXr16ISwsDCdOnMBzzz2HzMxMfPHFF83WX7uvuTad0cf09HQkJCSgsrIS3t7e2LhxIwYOHIi0tLRu8fk11T/A+T87AEhKSsKxY8dw+PDhBvuc4b9Blwwjzmz69OnS4/j4eIwePRq9evXCf/7zn077nzG1r1/84hfS48GDByM+Ph59+/bFnj17MHHiRBkrc9zixYuRkZGB77//Xu5SOkxTfXz88celx4MHD0ZoaCgmTpyICxcuoG/fvp1dpsNiYmKQlpYGg8GAzz//HPPmzcPevXvlLqvdNNW/gQMHOv1nl52djSVLliA5ORnu7u5yl9MmLnmaJjAwEG5ubg1mEufn5yMkJESmqtrG19cX/fv3x/nz5xESEoKqqioUFxfbtanfr5CQkEb7Xbuvq6mtqbnPKiQkBAUFBXb7q6urcfPmTafsd58+fRAYGIjz588DcJ7+Pfnkk/j666+xe/duRERESNvb6/eyqTY6na7TgnhTfWzM6NGjAcDuc+zKfdRoNIiOjsaIESOQmJiIIUOG4O233+42n19T/WuMs312R48eRUFBAYYPHw6VSgWVSoW9e/finXfegUqlQnBwcJf/DF0yjGg0GowYMQI7d+6UtlmtVuzcudPuHKIzKC0txYULFxAaGooRI0ZArVbb9SszMxNZWVlSvxISEpCenm735ZacnAydTicNWXYlUVFRCAkJseuT0WjEwYMH7fpUXFyMo0ePSm127doFq9Uq/U8lISEB+/btg9lsltokJycjJiZG1lM0jbl69Spu3LiB0NBQAF2/f0IIPPnkk9i4cSN27drV4HRRe/1eJiQk2L1GbZvO+G+2pT42Ji0tDQDsPseu3MdbWa1WmEymbvH5Naa2f41xts9u4sSJSE9PR1pamvQzcuRIzJkzR3rc5T/D254C66SSkpKEVqsVa9euFadOnRKPP/648PX1tZtJ3BU988wzYs+ePeLSpUvihx9+EJMmTRKBgYGioKBACGG7fKtnz55i165d4siRIyIhIUEkJCRIx9devjVlyhSRlpYmtm/fLnr06CHrpb0lJSUiNTVVpKamCgDirbfeEqmpqeLKlStCCNulvb6+vmLz5s3ixIkTYubMmY1e2jts2DBx8OBB8f3334t+/frZXfpaXFwsgoODxSOPPCIyMjJEUlKS8PT07JRLX5vrX0lJifjd734nUlJSxKVLl8S3334rhg8fLvr16ycqKyudon+//vWvhV6vF3v27LG7NLK8vFxq0x6/l7WXFT777LPi9OnT4u9//3unXTrZUh/Pnz8vli1bJo4cOSIuXbokNm/eLPr06SPGjRvnFH18/vnnxd69e8WlS5fEiRMnxPPPPy8UCoX45ptvhBDO//k11z9n/+yacusVQl39M3TZMCKEEO+++67o2bOn0Gg04o477hAHDhyQu6QWPfTQQyI0NFRoNBoRHh4uHnroIXH+/Hlpf0VFhXjiiSeEn5+f8PT0FD/5yU9Ebm6u3WtcvnxZTJ8+XXh4eIjAwEDxzDPPCLPZ3NldkezevVsAaPAzb948IYTt8t6XXnpJBAcHC61WKyZOnCgyMzPtXuPGjRti9uzZwtvbW+h0OjF//nxRUlJi1+b48ePirrvuElqtVoSHh4vly5fL3r/y8nIxZcoU0aNHD6FWq0WvXr3EwoULG4Tirty/xvoGQKxZs0Zq016/l7t37xZDhw4VGo1G9OnTx+49OlJLfczKyhLjxo0T/v7+QqvViujoaPHss8/arVXRlfv4y1/+UvTq1UtoNBrRo0cPMXHiRCmICOH8n19z/XP2z64pt4aRrv4ZKoQQ4vbHV4iIiIjaxiXnjBAREVHXwTBCREREsmIYISIiIlkxjBAREZGsGEaIiIhIVgwjREREJCuGESIiIpIVwwgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZ/T82cREwg4q+DQAAAABJRU5ErkJggg==","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","\n","t = chrono.time.time()\n","epsilon = 0.02\n","P, P_list = actor_critic_v(\n","    mdp = env,\n","    epsilon = 0.2,\n","    nb_episodes = 4000,\n","    timeout = 500,\n","    alpha1 = 0.5,\n","    alpha2 = 0.5,\n","    render = False,\n","    render_final= True,\n","    P_init = 0.1,\n","    v_init = 0.1,\n",")\n","print((chrono.time.time() - t))\n","plt.plot(np.arange(len(P_list)), P_list)\n","plt.show()"]},{"cell_type":"code","execution_count":64,"id":"bfa117b2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 1.88854802e-01  4.98312545e-01  1.25042212e-01  1.87790441e-01]\n"," [-3.70674267e-01 -2.48130780e-01  1.56646211e+00  5.23429330e-02]\n"," [ 6.55990463e-01  1.26802174e-02  1.90268845e-01  1.41060475e-01]\n"," [ 5.82839912e-01  1.32989301e-01  1.16597301e-01  1.67573486e-01]\n"," [ 4.93920596e-01  1.88149745e-01  1.60241840e-01  1.57687818e-01]\n"," [-8.80232023e-02 -4.75492279e-01  1.85207056e+00 -2.88555076e-01]\n"," [ 7.17652635e-01  1.12928412e-01  1.26211594e-01  4.32073592e-02]\n"," [ 9.72454100e-02  6.23095956e-01  1.18992522e-01  1.60666112e-01]\n"," [-1.40811382e+00 -9.64316126e-02  2.88597184e+00 -3.81426411e-01]\n"," [ 1.36163032e-02  7.12351924e-01  1.44961262e-01  1.29070511e-01]\n"," [-1.87289385e-01  4.67884455e-02  1.22308432e+00 -8.25833803e-02]\n"," [-1.92369326e-01  9.64719651e-02  1.99552433e+00 -8.99626972e-01]\n"," [-1.67156746e-01  9.73479529e-01  1.13539978e-01  8.01372391e-02]\n"," [-1.72432665e-01  2.15235432e-01  1.20544468e+00 -2.48247451e-01]\n"," [-2.56369931e-01 -1.24443638e-01  1.87500489e+00 -4.94191318e-01]\n"," [ 5.98224431e-02  7.85037981e-01  1.59270356e-01 -4.13078041e-03]\n"," [-1.16656800e+00  3.35215336e+00 -5.87769785e-01 -5.97815578e-01]\n"," [-1.00434865e+00  5.12530232e+00 -7.76071897e-01 -2.34488177e+00]\n"," [-2.56175798e+00  7.18897887e+00 -9.05651945e-01 -2.72156894e+00]\n"," [ 7.05982052e-02  7.87963802e-01  7.05982052e-02  7.08397876e-02]\n"," [ 1.00000000e-01  1.00000000e-01  1.00000000e-01  1.00000000e-01]]\n"]}],"source":["print(P)"]},{"cell_type":"code","execution_count":null,"id":"aee612ec","metadata":{},"outputs":[],"source":["def actor_critic_q(\n","    mdp: MazeMDPEnv,\n","    epsilon: float,\n","    nb_episodes: int = 20,\n","    timeout: int = 50,\n","    alpha1: float = 0.5,\n","    alpha2: float = 0.5,\n","    render: bool = True,\n","    render_final: bool = True,\n","    P_init: float = 0.1,\n","    q_init: float = 0.1,\n",") -> Tuple[np.ndarray, List[float]]:\n","    # Initialize the state-action value function\n","    # alpha is the learning rate\n","    P = np.full((mdp.nb_states, mdp.action_space.size), P_init)\n","    v = np.full((mdp.nb_states, mdp.action_space.sizes), q_init)\n","    P_list = []\n","\n","    # Run learning cycle\n","    mdp.timeout = timeout  # episode length\n","\n","    if render:\n","        mdp.init_draw(\"Q-learning e-greedy\")\n","\n","    for _ in tqdm(range(nb_episodes)):\n","        # Draw the first state of episode i using a uniform distribution over all the states\n","        x = mdp.reset(uniform=True)\n","        done = False\n","        while not done:\n","            if render:\n","                # Show the agent in the maze\n","                mdp.draw_v_pi(P, P.argmax(axis=1))\n","\n","            # Actor :  Draw an action using an epsilon-greedy policy\n","            u = egreedy(P, x, epsilon)\n","\n","            # Perform a step of the MDP\n","            \n","            [y, r, done, _] = mdp.step(u)\n","            \n","            # Critic: compute delta and update v\n","            v, delta = critic_v(v, x, y, r, mdp.gamma, alpha1)\n","\n","            P[x, u] = P[x, u] + alpha2 * delta\n","            \n","            # Proba normalizarion\n","            P[x, :] = [p/np.sum(P[x,:]) if p> 0 else 0 for p in P[x, :]]\n","\n","            # Update the agent position\n","            x = y\n","        P_list.append(np.linalg.norm(P))\n","\n","    if render_final:\n","        # Show the final policy\n","        mdp.current_state = 0\n","        mdp.draw_v_pi(P, get_policy_from_q(P), title=\"Q-learning e-greedy\")\n","    return P, P_list\n","    "]}],"metadata":{"jupytext":{"formats":"ipynb,Rmd"},"kernelspec":{"display_name":"Python 3.10.4 ('deepdac')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"b8381a9192b883635ac16797c97396a9adba45cea8be9b52669f729c8ec391d7"}}},"nbformat":4,"nbformat_minor":5}
