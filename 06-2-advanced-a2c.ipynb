{"cells":[{"cell_type":"code","execution_count":1,"id":"fb902433","metadata":{"id":"j0MaggiOl4KU","outputId":"5c9dc6a3-8fcd-4733-91d7-40e7a2bae06f"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["try:\n","    from easypip import easyimport\n","except:\n","    !pip install easypip\n","    from easypip import easyimport\n","\n","import functools\n","import time\n","\n","easyimport(\"importlib_metadata==4.13.0\")\n","OmegaConf = easyimport(\"omegaconf\").OmegaConf\n","bbrl = easyimport(\"bbrl\")\n","import gym\n","\n","import os\n","import copy\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n","\n","import gym\n","\n","from bbrl.agents.agent import Agent\n","from bbrl import get_arguments, get_class, instantiate_class\n","\n","# The workspace is the main class in BBRL, this is where all data is collected and stored\n","from bbrl.workspace import Workspace\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n","\n","# AutoResetGymAgent is an agent able to execute a batch of gym environments\n","# with auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n","# Not present in the A2C version...\n","from bbrl.utils.logger import TFLogger\n","\n","from torch.distributions import Distribution\n","from torch.distributions.normal import Normal\n","from torch.distributions.independent import Independent\n","\n","from bbrl.visu.visu_policies import plot_policy\n","from bbrl.visu.visu_critics import plot_critic\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":2,"id":"162b8dbb","metadata":{"id":"HFLn1t5rmIDb"},"outputs":[],"source":["def build_backbone(sizes, activation):\n","    layers = []\n","    for j in range(len(sizes) - 2):\n","        layers += [nn.Linear(sizes[j], sizes[j + 1]), activation]\n","    return layers\n","\n","\n","def build_mlp(sizes, activation, output_activation=nn.Identity()):\n","    layers = []\n","    for j in range(len(sizes) - 1):\n","        act = activation if j < len(sizes) - 2 else output_activation\n","        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n","    return nn.Sequential(*layers)\n","\n","class DiscreteActor(Agent):\n","    def __init__(self, state_dim, hidden_size, n_actions):\n","        super().__init__()\n","        self.model = build_mlp([state_dim] + list(hidden_size) + [n_actions], activation=nn.ReLU())\n","        self.model = self.model.to(device)\n","\n","    \n","    def forward(self, t, stochastic, replay=False, **kwargs):\n","        \"\"\" \n","        Compute the action given either a time step (looking into the workspace)\n","        or an observation (in kwargs)\n","        \"\"\"\n","        if \"observation\" in kwargs:\n","            observation = kwargs[\"observation\"]\n","        else:\n","            observation = self.get((\"env/env_obs\", t))\n","        scores = self.model(observation)\n","        probs = torch.softmax(scores, dim=-1)\n","\n","        if stochastic:\n","            action = torch.distributions.Categorical(probs).sample()\n","        else:\n","          action = probs.argmax(1)\n","\n","        entropy = torch.distributions.Categorical(probs).entropy()\n","        logprobs = probs[torch.arange(probs.size()[0]), action].log()\n","\n","        if not replay:\n","            self.set((\"action\", t), action)\n","        self.set((\"action_logprobs\", t), logprobs)\n","        self.set((\"entropy\", t), entropy)\n","\n","    \n","    def predict_action(self, obs, stochastic):\n","        obs = obs.to(device)\n","        scores = self.model(obs)\n","\n","        if stochastic:\n","            probs = torch.softmax(scores, dim=-1)\n","            action = torch.distributions.Categorical(probs).sample()\n","        else:\n","            action = scores.argmax(0)\n","        return action\n","    \n","class ContinuousAgent(Agent):\n","    def dist(self, obs: torch.Tensor) -> Distribution:\n","        \"\"\"Returns the distributions for the given observations\"\"\"\n","        assert False, \"to implement in subclass\"\n","            \n","    def forward(self, t, stochastic, **kwargs):\n","        obs = self.get((\"env/env_obs\", t))\n","        dist = self.dist(obs)\n","\n","        action = dist.sample() if stochastic else dist.mean\n","\n","        logp_pi = dist.log_prob(action)\n","        \n","        self.set((\"entropy\", t), dist.entropy())\n","\n","        self.set((\"action\", t), action)\n","        self.set((\"action_logprobs\", t), logp_pi)\n","\n","\n","    def predict_action(self, obs, stochastic):\n","        \"\"\"Predict just one action (without using the workspace)\"\"\"\n","        obs = obs.to(device)\n","        dist = self.dist(obs)\n","        action = dist.sample() if stochastic else dist.mean\n","        return action\n","    \n","class ConstantVarianceContinuousActor(ContinuousAgent):\n","    def __init__(self, state_dim, hidden_layers, action_dim, **kwargs):\n","        super().__init__()\n","        layers = [state_dim] + list(hidden_layers) + [action_dim]\n","        self.model = build_mlp(layers, activation=nn.ReLU())\n","        self.model = self.model.to(device)\n","        self.std_param = 2\n","\n","    def dist(self, obs: torch.Tensor):\n","        mean = self.model(obs)    \n","        return Independent(Normal(mean, self.std_param), 1)  # std must be positive\n","    \n","class TunableVarianceContinuousActor(ContinuousAgent):\n","    def __init__(self, state_dim, hidden_layers, action_dim):\n","        super().__init__()\n","        layers = [state_dim] + list(hidden_layers) + [action_dim]\n","        self.model = build_mlp(layers, activation=nn.ReLU())\n","        self.model = self.model.to(device)\n","\n","        # The standard deviation associated with each dimension\n","        self.std_param = nn.parameter.Parameter(torch.randn(action_dim, 1))\n","        \n","        # We use the softplus function to compute the variance for the normal\n","        # The base version computes exp(1+log(x)) component-wise\n","        # https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html\n","        self.soft_plus = torch.nn.Softplus()\n","\n","    def dist(self, obs: torch.Tensor):\n","        mean = self.model(obs)\n","        return Independent(Normal(mean, self.soft_plus(self.std_param)), 1)\n","\n","class StateDependentVarianceContinuousActor(ContinuousAgent):\n","    def __init__(self, state_dim, hidden_layers, action_dim):\n","        super().__init__()\n","\n","        # Buils the \"backbone\" neural network\n","        backbone_dim = [state_dim] + list(hidden_layers)\n","        self.layers = build_backbone(backbone_dim, activation=nn.ReLU())\n","        self.backbone = nn.Sequential(*self.layers)\n","        self.backbone = self.backbone.to(device)\n","        \n","        self.mean = nn.Sequential(self.backbone, nn.Linear(backbone_dim[-1], action_dim)).to(device)\n","        self.std = nn.Sequential(self.backbone, nn.Linear(backbone_dim[-1], action_dim)).to(device)\n","        self.mean = self.mean.to(device)\n","        self.std = self.std.to(device)\n","\n","    \n","    def dist(self, obs: torch.Tensor) -> Distribution:        \n","        mean = self.mean(obs)\n","        std = self.std(obs)\n","        return Independent(Normal(mean, nn.functional.softplus(std)), 1)\n","\n","def make_env(env_name):\n","    return gym.make(env_name)\n","\n","\n","def get_env_agents(cfg):\n","    train_env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","    eval_env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.nb_evals,\n","        cfg.algorithm.seed,\n","    )\n","    return train_env_agent, eval_env_agent\n","\n","\n","class VAgent(Agent):\n","    def __init__(self, state_dim, hidden_layers):\n","        super().__init__()\n","        self.is_q_function = False\n","        self.model = build_mlp(\n","            [state_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n","        )\n","        self.model = self.model.to(device)\n","\n","    def forward(self, t, **kwargs):\n","        observation = self.get((\"env/env_obs\", t))\n","        critic = self.model(observation).squeeze(-1)\n","        self.set((\"v_value\", t), critic)\n","        \n","# Create the A2C Agent\n","def make_agents(cfg, train_env_agent, eval_env_agent):\n","    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n","    if train_env_agent.is_continuous_action():\n","        action_agent = globals()[cfg.algorithm.action_agent](\n","            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n","        )\n","    else:\n","        action_agent = DiscreteActor(obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size)\n","\n","    tr_agent = TemporalAgent(Agents(train_env_agent, action_agent))\n","    ev_agent = TemporalAgent(Agents(eval_env_agent, action_agent))\n","\n","    critic_agent = TemporalAgent(VAgent(obs_size, cfg.algorithm.architecture.critic_hidden_size))\n","    return tr_agent, ev_agent, critic_agent\n","\n","class Logger():\n","\n","  def __init__(self, cfg):\n","    self.logger = instantiate_class(cfg.logger)\n","\n","  def add_log(self, log_string, loss, epoch):\n","    self.logger.add_scalar(log_string, loss.item(), epoch)\n","\n","  # Log losses\n","  def log_losses(self, epoch, critic_loss, entropy_loss, a2c_loss):\n","    self.add_log(\"critic_loss\", critic_loss, epoch)\n","    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n","    self.add_log(\"a2c_loss\", a2c_loss, epoch)\n","\n","\n","# Configure the optimizer over the a2c agent\n","def setup_optimizer(cfg, action_agent, critic_agent):\n","    optimizer_args = get_arguments(cfg.optimizer)\n","    parameters = nn.Sequential(action_agent, critic_agent).parameters()\n","    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n","    return optimizer\n","\n","def execute_agent(cfg, epoch, workspace, agent):\n","    if epoch > 0:\n","        workspace.zero_grad()\n","        workspace.copy_n_last_steps(1)\n","        agent(\n","            workspace, t=1, n_steps=cfg.algorithm.n_steps - 1, stochastic=True\n","        )\n","    else:\n","        agent(workspace, t=0, n_steps=cfg.algorithm.n_steps, stochastic=True)\n","        \n","from bbrl.utils.functionalb import gae\n","\n","def compute_critic_loss(cfg, reward, must_bootstrap, v_value):\n","    # Compute temporal difference\n","    # target = reward[:-1] + cfg.algorithm.discount_factor * v_value[1:].detach() * must_bootstrap.int()\n","    target = gae(v_value, reward, must_bootstrap, cfg.algorithm.discount_factor, cfg.algorithm.gae_coef)\n","    td = target - v_value[:-1]\n","    td_error = td**2\n","    critic_loss = td_error.mean()\n","    return critic_loss, td\n","\n","def compute_actor_loss(action_logp, td):\n","    a2c_loss = action_logp[:-1] * td.detach()\n","    return a2c_loss.mean()"]},{"cell_type":"code","execution_count":3,"id":"a6e8984a","metadata":{"id":"sk85_sRWW-5s"},"outputs":[],"source":["def run_a2c(cfg):\n","    logger = Logger(cfg)\n","    best_reward = float('-inf')\n","\n","    # 2) Create the environment agent\n","    train_env_agent, eval_env_agent = get_env_agents(cfg)\n","    \n","    tr_agent, eval_agent, critic_agent = make_agents(cfg, train_env_agent, eval_env_agent)\n","\n","    # 5) Configure the workspace to the right dimension\n","    # Note that no parameter is needed to create the workspace.\n","    # In the training loop, calling the agent() and critic_agent()\n","    # will take the workspace as parameter\n","    train_workspace = Workspace()  # Used for training\n","    \n","    # send to device:\n","    train_env_agent = train_env_agent.to(device)\n","    eval_env_agent = eval_env_agent.to(device)\n","    tr_agent = tr_agent.to(device)\n","    eval_agent = eval_agent.to(device)\n","    critic_agent = critic_agent.to(device)\n","    train_workspace = train_workspace.to(device)\n","\n","    # 6) Configure the optimizer over the a2c agent\n","    optimizer = setup_optimizer(cfg, tr_agent, critic_agent)\n","    nb_steps = 0\n","    tmp_steps = 0\n","\n","    # 7) Training loop\n","    for epoch in (pbar := tqdm(range(cfg.algorithm.max_epochs))):\n","        # Execute the agent in the workspace\n","        if epoch > 0:\n","            train_workspace.zero_grad()\n","            train_workspace.copy_n_last_steps(1)\n","            tr_agent(\n","                train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1, stochastic=True\n","            )\n","        else:\n","            tr_agent(\n","                train_workspace, t=0, n_steps=cfg.algorithm.n_steps, stochastic=True\n","            )\n","\n","        # Compute the critic value over the whole workspace\n","        critic_agent(train_workspace, n_steps=cfg.algorithm.n_steps)\n","\n","        transition_workspace = train_workspace.get_transitions()\n","\n","        v_value, done, truncated, reward, action, action_logp = transition_workspace[\n","            \"v_value\",\n","            \"env/done\",\n","            \"env/truncated\",\n","            \"env/reward\",\n","            \"action\",\n","            \"action_logprobs\",\n","        ]\n","        nb_steps += action[0].shape[0]\n","        # Determines whether values of the critic should be propagated\n","        # True if the episode reached a time limit or if the task was not done\n","        # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj\n","        must_bootstrap = torch.logical_or(~done[1], truncated[1])\n","\n","        # Compute critic loss\n","        critic_loss, td = compute_critic_loss(cfg, reward, must_bootstrap, v_value)\n","        a2c_loss = compute_actor_loss(action_logp, td)\n","\n","        # Compute entropy loss\n","        entropy_loss = torch.mean(train_workspace[\"entropy\"])\n","\n","        # Store the losses for tensorboard display\n","        logger.log_losses(nb_steps, critic_loss, entropy_loss, a2c_loss)\n","\n","        # Compute the total loss\n","        loss = (\n","            -cfg.algorithm.entropy_coef * entropy_loss\n","            + cfg.algorithm.critic_coef * critic_loss\n","            - cfg.algorithm.a2c_coef * a2c_loss\n","        )\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(tr_agent.parameters(), cfg.algorithm.max_grad_norm)\n","        optimizer.step()\n","\n","        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n","            tmp_steps = nb_steps\n","            eval_workspace = Workspace()  # Used for evaluation\n","            eval_workspace = eval_workspace.to(device)\n","            eval_agent(eval_workspace, t=0, stop_variable=\"env/done\", stochastic=False)\n","            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n","            mean = rewards.mean()\n","            logger.add_log(\"reward\", mean, nb_steps)\n","            pbar.set_description(f\"epoch: {epoch}, best_reward: {best_reward}, reward: {mean}\")\n","            if cfg.save_best and mean > best_reward:\n","                best_reward = mean\n","                directory = \"./a2c_policies/\"\n","                if not os.path.exists(directory):\n","                    os.makedirs(directory)\n","                filename = directory + \"a2c_\" + str(mean.item()) + \".agt\"\n","                policy = eval_agent.agent.agents[1]\n","                policy.save_model(filename)\n","                critic = critic_agent.agent\n","                if cfg.plot_policy:\n","                    plot_policy(\n","                        policy,\n","                        eval_env_agent,\n","                        \"./a2c_advanced_plots/\",\n","                        cfg.gym_env.env_name,\n","                        best_reward,\n","                        stochastic=False,\n","                    )\n","                    \"\"\"\n","                    plot_critic(\n","                        critic,\n","                        eval_env_agent,\n","                        \"./a2c_advanced_plots/\",\n","                        cfg.gym_env.env_name,\n","                        best_reward,\n","                    )\"\"\"\n","                    \n","    return mean.item()\n"]},{"cell_type":"code","execution_count":4,"id":"c657cb24","metadata":{"id":"JB2B8zELNWQd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matplotlib backend: module://matplotlib_inline.backend_inline\n","Launch tensorboard from the shell:\n","/home/manuel/deepdac/bin/tensorboard --logdir=\"/home/manuel/RLD/TP6/tblogs-tp6-advanced\"\n"]}],"source":["import my_gym\n","\n","params={\n","  \"save_best\": True,\n","  \"plot_policy\": True,\n","\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tblogs-tp6-advanced/constant-var-actor/\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,    \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 4,\n","    \"n_envs\": 8,\n","    \"nb_evals\":10,\n","    \"n_steps\": 16,\n","    \"eval_interval\": 1000,\n","    \"max_epochs\": 1000,\n","    \"discount_factor\": 0.95,\n","    \"entropy_coef\": 0.001,\n","    \"critic_coef\": 1.0,\n","    \"a2c_coef\": 0.1,\n","    \"gae_coef\": 0.8,\n","    \"max_grad_norm\": 0.5,\n","    \n","    # You can change the chosen action agent here\n","    #\"action_agent\": \"ConstantVarianceContinuousActor\",\n","    #\"action_agent\": \"StateDependentVarianceContinuousActor\",\n","    \"action_agent\": \"TunableVarianceContinuousActor\",  \n","    \"architecture\":{\n","      \"actor_hidden_size\": [25, 25],\n","      \"critic_hidden_size\": [24, 36],\n","    },\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_env\",\n","    \"env_name\": \"CartPoleContinuous-v1\",\n","    },\n","  \"optimizer\":{\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 0.01,\n","  }\n","}\n","\n","\n","# For Colab - otherwise, it is easier and better to launch tensorboard from\n","# the terminal\n","if get_ipython().__class__.__module__ == \"google.colab._shell\":\n","    %load_ext tensorboard\n","    %tensorboard --logdir ./tmp\n","else:\n","    import sys\n","    import os\n","    import os.path as osp\n","    print(f'''Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir=\"{os.getcwd()}/tblogs-tp6-advanced\"''')"]},{"cell_type":"code","execution_count":null,"id":"a0c3f63b","metadata":{"id":"l42OUoGROlSt","outputId":"c06b3639-4c23-4177-8965-447fc9af2c1e"},"outputs":[],"source":["print(device)\n","config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_a2c(config)\n"]},{"cell_type":"code","execution_count":7,"id":"f2626491","metadata":{},"outputs":[],"source":["import optuna\n","\n","  \n","def setup_params(sample_params, base_params, env):\n","    params = {\n","        \"save_best\": True,\n","        \"plot_policy\": True,\n","        \"logger\":{\n","            \"classname\": \"bbrl.utils.logger.TFLogger\",\n","            \"log_dir\": \"./tblogs-tp6-advanced/\"+env+\"/\"+sample_params[\"actor\"]+\"/\" + str(time.time()),\n","            \"cache_size\": 10000,\n","            \"every_n_seconds\": 10,\n","            \"verbose\": False,    \n","            },\n","        \"algorithm\":{\n","            \"seed\": 4,\n","            \"n_envs\": 8,\n","            \"nb_evals\":10,\n","            \"n_steps\": sample_params['n_steps'],\n","            \"eval_interval\": 1000,\n","            \"max_epochs\": 2000,\n","            \"discount_factor\": sample_params['discount_factor'],\n","            \"entropy_coef\": sample_params['entropy_coef'],\n","            \"critic_coef\": sample_params['critic_coef'],\n","            \"a2c_coef\": sample_params['a2c_coef'],\n","            \"gae_coef\": sample_params['gae_coef'],\n","            \"max_grad_norm\": sample_params['max_grad_norm'],\n","            \"action_agent\": sample_params[\"actor\"],  \n","            \"architecture\":{\n","                \"actor_hidden_size\": sample_params['actor_hidden_size'],\n","                \"critic_hidden_size\": sample_params['critic_hidden_size'],\n","            },\n","        },\n","        \"gym_env\":{\n","            \"classname\": \"__main__.make_env\",\n","            \"env_name\": env,\n","            },\n","        \"optimizer\":{\n","            \"classname\": \"torch.optim.Adam\",\n","            \"lr\": sample_params['learning_rate'],\n","        }\n","    }\n","    return params\n","    \n","\n","def sample_a2c_params(trial: optuna.Trial):\n","    \"\"\"\n","    Sampler for A2C hyperparams.\n","\n","    :param trial:\n","    :return:\n","    \"\"\"\n","    normalize: True\n","  n_envs: 8\n","  n_timesteps: !!float 1e6\n","  policy: 'MlpPolicy'\n","  ent_coef: 0.0\n","  max_grad_norm: 0.5\n","  n_steps: 8\n","  gae_lambda: 0.9\n","  vf_coef: 0.4\n","  policy_kwargs: \"dict(log_std_init=-2, ortho_init=False)\"\n","    discount_factor = trial.suggest_categorical(\"discount_factor\", [0.9, 0.95, 0.98])\n","    max_grad_norm = trial.suggest_categorical(\"max_grad_norm\", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5])\n","    gae_coef = trial.suggest_categorical(\"gae_coef\", [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0])\n","    #n_steps = trial.suggest_categorical(\"n_steps\", [8, 16, 32, 64, 128, 256, 512, 1024, 2048])\n","    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-7, 1)\n","    entropy_coef = trial.suggest_loguniform(\"entropy_coef\", 0.00000001, 1)\n","    critic_coef = trial.suggest_loguniform(\"critic_coef\", 0.00000001, 1)\n","    a2c_coef = trial.suggest_loguniform(\"a2c_coef\", 0.00000001, 1)\n","    \n","    net_arch = trial.suggest_categorical(\"net_arch\", [64, 256])\n","    actor = trial.suggest_categorical(\"actor\", [\"TunableVarianceContinuousActor\", \"ConstantVarianceContinuousActor\", \"StateDependentVarianceContinuousActor\"])\n","    return {\n","        \"learning_rate\": learning_rate,\n","        \"n_steps\": 16,\n","        \"discount_factor\": discount_factor,\n","        \"entropy_coef\": entropy_coef,\n","        \"critic_coef\": critic_coef,\n","        \"a2c_coef\": a2c_coef,\n","        \"gae_coef\": gae_coef,\n","        \"max_grad_norm\": max_grad_norm,\n","        \"actor\": actor,\n","        \"actor_hidden_size\": [net_arch, net_arch],\n","        \"critic_hidden_size\": [net_arch, net_arch],\n","        \n","    }"]},{"cell_type":"code","execution_count":8,"id":"53496d07","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-01 14:17:23,404]\u001b[0m A new study created in memory with name: no-name-2e8573a6-2af8-42b4-881b-d856701bdb37\u001b[0m\n","/tmp/ipykernel_2604/2932638803.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n","/tmp/ipykernel_2604/2932638803.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  entropy_coef = trial.suggest_loguniform(\"entropy_coef\", 0.00000001, 0.1)\n","/tmp/ipykernel_2604/2932638803.py:57: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  critic_coef = trial.suggest_loguniform(\"critic_coef\", 0.00000001, 0.1)\n","/tmp/ipykernel_2604/2932638803.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  a2c_coef = trial.suggest_loguniform(\"a2c_coef\", 0.00000001, 0.1)\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.00791960856009742, 'n_steps': 16, 'discount_factor': 0.999, 'entropy_coef': 8.410300998010625e-05, 'critic_coef': 0.00035755850932532807, 'a2c_coef': 0.00011479799330093288, 'gae_coef': 0.92, 'max_grad_norm': 0.9, 'actor': 'StateDependentVarianceContinuousActor', 'actor_hidden_size': [64, 64], 'critic_hidden_size': [64, 64]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -994.9615478515625, reward: -1492.1990966796875: 100%|██████████| 2000/2000 [05:18<00:00,  6.28it/s]\n","\u001b[32m[I 2022-12-01 14:22:41,902]\u001b[0m Trial 0 finished with value: -1492.1990966796875 and parameters: {'discount_factor': 0.999, 'max_grad_norm': 0.9, 'gae_coef': 0.92, 'learning_rate': 0.00791960856009742, 'entropy_coef': 8.410300998010625e-05, 'critic_coef': 0.00035755850932532807, 'a2c_coef': 0.00011479799330093288, 'net_arch': 64, 'actor': 'StateDependentVarianceContinuousActor'}. Best is trial 0 with value: -1492.1990966796875.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.5008165177244058, 'n_steps': 16, 'discount_factor': 0.95, 'entropy_coef': 4.52149880233622e-08, 'critic_coef': 1.3350914092531017e-05, 'a2c_coef': 0.022038986256550926, 'gae_coef': 0.99, 'max_grad_norm': 0.8, 'actor': 'ConstantVarianceContinuousActor', 'actor_hidden_size': [64, 64], 'critic_hidden_size': [64, 64]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -1129.4071044921875, reward: -1567.734619140625: 100%|██████████| 2000/2000 [04:54<00:00,  6.79it/s] \n","\u001b[32m[I 2022-12-01 14:27:36,570]\u001b[0m Trial 1 finished with value: -1567.734619140625 and parameters: {'discount_factor': 0.95, 'max_grad_norm': 0.8, 'gae_coef': 0.99, 'learning_rate': 0.5008165177244058, 'entropy_coef': 4.52149880233622e-08, 'critic_coef': 1.3350914092531017e-05, 'a2c_coef': 0.022038986256550926, 'net_arch': 64, 'actor': 'ConstantVarianceContinuousActor'}. Best is trial 0 with value: -1492.1990966796875.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.010879478454815817, 'n_steps': 16, 'discount_factor': 0.9999, 'entropy_coef': 0.013617771055471581, 'critic_coef': 0.0018278715310616662, 'a2c_coef': 0.0012355182187374942, 'gae_coef': 0.99, 'max_grad_norm': 0.3, 'actor': 'TunableVarianceContinuousActor', 'actor_hidden_size': [256, 256], 'critic_hidden_size': [256, 256]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -874.0810546875, reward: -1401.4061279296875: 100%|██████████| 2000/2000 [05:29<00:00,  6.07it/s]    \n","\u001b[32m[I 2022-12-01 14:33:05,915]\u001b[0m Trial 2 finished with value: -1401.4061279296875 and parameters: {'discount_factor': 0.9999, 'max_grad_norm': 0.3, 'gae_coef': 0.99, 'learning_rate': 0.010879478454815817, 'entropy_coef': 0.013617771055471581, 'critic_coef': 0.0018278715310616662, 'a2c_coef': 0.0012355182187374942, 'net_arch': 256, 'actor': 'TunableVarianceContinuousActor'}. Best is trial 2 with value: -1401.4061279296875.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.001050139425019364, 'n_steps': 16, 'discount_factor': 0.99, 'entropy_coef': 2.1158350291600146e-05, 'critic_coef': 3.4123895268260562e-06, 'a2c_coef': 2.1277866724887434e-05, 'gae_coef': 0.92, 'max_grad_norm': 0.3, 'actor': 'TunableVarianceContinuousActor', 'actor_hidden_size': [256, 256], 'critic_hidden_size': [256, 256]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -843.44970703125, reward: -1279.4151611328125: 100%|██████████| 2000/2000 [05:18<00:00,  6.27it/s]   \n","\u001b[32m[I 2022-12-01 14:38:24,814]\u001b[0m Trial 3 finished with value: -1279.4151611328125 and parameters: {'discount_factor': 0.99, 'max_grad_norm': 0.3, 'gae_coef': 0.92, 'learning_rate': 0.001050139425019364, 'entropy_coef': 2.1158350291600146e-05, 'critic_coef': 3.4123895268260562e-06, 'a2c_coef': 2.1277866724887434e-05, 'net_arch': 256, 'actor': 'TunableVarianceContinuousActor'}. Best is trial 3 with value: -1279.4151611328125.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.0009066540159199412, 'n_steps': 16, 'discount_factor': 0.995, 'entropy_coef': 0.011026416819215262, 'critic_coef': 0.0018178440671131248, 'a2c_coef': 1.2598164464476649e-06, 'gae_coef': 0.92, 'max_grad_norm': 0.8, 'actor': 'StateDependentVarianceContinuousActor', 'actor_hidden_size': [64, 64], 'critic_hidden_size': [64, 64]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -1166.8460693359375, reward: -1601.8599853515625: 100%|██████████| 2000/2000 [05:00<00:00,  6.66it/s]\n","\u001b[32m[I 2022-12-01 14:43:25,149]\u001b[0m Trial 4 finished with value: -1601.8599853515625 and parameters: {'discount_factor': 0.995, 'max_grad_norm': 0.8, 'gae_coef': 0.92, 'learning_rate': 0.0009066540159199412, 'entropy_coef': 0.011026416819215262, 'critic_coef': 0.0018178440671131248, 'a2c_coef': 1.2598164464476649e-06, 'net_arch': 64, 'actor': 'StateDependentVarianceContinuousActor'}. Best is trial 3 with value: -1279.4151611328125.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.033357951095507196, 'n_steps': 16, 'discount_factor': 0.9999, 'entropy_coef': 5.0768982499848466e-05, 'critic_coef': 2.2705416215806046e-05, 'a2c_coef': 1.2897644599515345e-08, 'gae_coef': 0.95, 'max_grad_norm': 1, 'actor': 'StateDependentVarianceContinuousActor', 'actor_hidden_size': [64, 64], 'critic_hidden_size': [64, 64]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -1257.1170654296875, reward: -1481.1353759765625: 100%|██████████| 2000/2000 [04:14<00:00,  7.87it/s]\n","\u001b[32m[I 2022-12-01 14:47:39,359]\u001b[0m Trial 5 finished with value: -1481.1353759765625 and parameters: {'discount_factor': 0.9999, 'max_grad_norm': 1, 'gae_coef': 0.95, 'learning_rate': 0.033357951095507196, 'entropy_coef': 5.0768982499848466e-05, 'critic_coef': 2.2705416215806046e-05, 'a2c_coef': 1.2897644599515345e-08, 'net_arch': 64, 'actor': 'StateDependentVarianceContinuousActor'}. Best is trial 3 with value: -1279.4151611328125.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.007172166197672235, 'n_steps': 16, 'discount_factor': 0.9999, 'entropy_coef': 3.739600752564219e-07, 'critic_coef': 1.9224578879716248e-06, 'a2c_coef': 7.200578995322678e-07, 'gae_coef': 0.99, 'max_grad_norm': 0.3, 'actor': 'TunableVarianceContinuousActor', 'actor_hidden_size': [256, 256], 'critic_hidden_size': [256, 256]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -1257.1170654296875, reward: -1481.1353759765625: 100%|██████████| 2000/2000 [05:26<00:00,  6.12it/s]\n","\u001b[32m[I 2022-12-01 14:53:06,383]\u001b[0m Trial 6 finished with value: -1481.1353759765625 and parameters: {'discount_factor': 0.9999, 'max_grad_norm': 0.3, 'gae_coef': 0.99, 'learning_rate': 0.007172166197672235, 'entropy_coef': 3.739600752564219e-07, 'critic_coef': 1.9224578879716248e-06, 'a2c_coef': 7.200578995322678e-07, 'net_arch': 256, 'actor': 'TunableVarianceContinuousActor'}. Best is trial 3 with value: -1279.4151611328125.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.0001466272889532353, 'n_steps': 16, 'discount_factor': 0.999, 'entropy_coef': 1.2397766137156187e-06, 'critic_coef': 4.889890462677407e-07, 'a2c_coef': 0.0007196057691934382, 'gae_coef': 0.98, 'max_grad_norm': 0.3, 'actor': 'StateDependentVarianceContinuousActor', 'actor_hidden_size': [64, 64], 'critic_hidden_size': [64, 64]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -1116.211181640625, reward: -1640.2789306640625: 100%|██████████| 2000/2000 [06:14<00:00,  5.34it/s]\n","\u001b[32m[I 2022-12-01 14:59:21,106]\u001b[0m Trial 7 finished with value: -1640.2789306640625 and parameters: {'discount_factor': 0.999, 'max_grad_norm': 0.3, 'gae_coef': 0.98, 'learning_rate': 0.0001466272889532353, 'entropy_coef': 1.2397766137156187e-06, 'critic_coef': 4.889890462677407e-07, 'a2c_coef': 0.0007196057691934382, 'net_arch': 64, 'actor': 'StateDependentVarianceContinuousActor'}. Best is trial 3 with value: -1279.4151611328125.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.05304988597148725, 'n_steps': 16, 'discount_factor': 0.9, 'entropy_coef': 1.064294660371012e-07, 'critic_coef': 0.005509667806274398, 'a2c_coef': 4.168109175052286e-07, 'gae_coef': 0.92, 'max_grad_norm': 0.3, 'actor': 'ConstantVarianceContinuousActor', 'actor_hidden_size': [64, 64], 'critic_hidden_size': [64, 64]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -861.0369262695312, reward: -1433.6409912109375: 100%|██████████| 2000/2000 [05:40<00:00,  5.87it/s]\n","\u001b[32m[I 2022-12-01 15:05:01,715]\u001b[0m Trial 8 finished with value: -1433.6409912109375 and parameters: {'discount_factor': 0.9, 'max_grad_norm': 0.3, 'gae_coef': 0.92, 'learning_rate': 0.05304988597148725, 'entropy_coef': 1.064294660371012e-07, 'critic_coef': 0.005509667806274398, 'a2c_coef': 4.168109175052286e-07, 'net_arch': 64, 'actor': 'ConstantVarianceContinuousActor'}. Best is trial 3 with value: -1279.4151611328125.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.03814224227269237, 'n_steps': 16, 'discount_factor': 0.99, 'entropy_coef': 0.0006591931989783329, 'critic_coef': 0.011255869815500367, 'a2c_coef': 4.568113471843917e-07, 'gae_coef': 0.95, 'max_grad_norm': 0.9, 'actor': 'TunableVarianceContinuousActor', 'actor_hidden_size': [256, 256], 'critic_hidden_size': [256, 256]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1997, best_reward: -915.5299682617188, reward: -1291.9088134765625: 100%|██████████| 2000/2000 [05:40<00:00,  5.88it/s]\n","\u001b[32m[I 2022-12-01 15:10:41,845]\u001b[0m Trial 9 finished with value: -1291.9088134765625 and parameters: {'discount_factor': 0.99, 'max_grad_norm': 0.9, 'gae_coef': 0.95, 'learning_rate': 0.03814224227269237, 'entropy_coef': 0.0006591931989783329, 'critic_coef': 0.011255869815500367, 'a2c_coef': 4.568113471843917e-07, 'net_arch': 256, 'actor': 'TunableVarianceContinuousActor'}. Best is trial 3 with value: -1279.4151611328125.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["{'learning_rate': 2.191825231064838e-05, 'n_steps': 16, 'discount_factor': 0.99, 'entropy_coef': 2.977249462045399e-06, 'critic_coef': 6.07404519245161e-08, 'a2c_coef': 2.6959789919387764e-05, 'gae_coef': 1.0, 'max_grad_norm': 0.7, 'actor': 'TunableVarianceContinuousActor', 'actor_hidden_size': [256, 256], 'critic_hidden_size': [256, 256]}\n"]},{"name":"stderr","output_type":"stream","text":["epoch: 1754, best_reward: -1051.1533203125, reward: -1818.632080078125:  88%|████████▊ | 1763/2000 [04:43<00:23,  9.99it/s]  "]}],"source":["import torch\n","\n","import optuna\n","\n","# 1. Define an objective function to be maximized.\n","def objective(trial):\n","\n","    # 2. Suggest values of the hyperparameters using a trial object.\n","    sampled_hyperparams = sample_a2c_params(trial)\n","    trial_params = setup_params(sampled_hyperparams, params, \"Pendulum-v1\")\n","    config=OmegaConf.create(trial_params)\n","    torch.manual_seed(config.algorithm.seed)\n","    print(sampled_hyperparams)\n","    reward = run_a2c(config)\n","    return reward\n","\n","# 3. Create a study object and optimize the objective function.\n","study = optuna.create_study(direction='maximize')\n","study.optimize(objective, n_trials=50)"]},{"cell_type":"code","execution_count":null,"id":"6b44200d","metadata":{},"outputs":[],"source":[]}],"metadata":{"jupytext":{"formats":"ipynb,Rmd"},"kernelspec":{"display_name":"Python 3.10.6 ('deepdac')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"b8381a9192b883635ac16797c97396a9adba45cea8be9b52669f729c8ec391d7"}}},"nbformat":4,"nbformat_minor":5}
